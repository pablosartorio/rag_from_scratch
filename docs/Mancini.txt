 Bienvenidos, bienvenidas a una nueva charla aquí. El tema de Inteligencia Artificial es un tema que convoca bastante, ya hemos tenido otra charla de Inteligencia Artificial, pero hoy Andrés nos viene a hablar de la traspienda. De la Inteligencia Artificial. Ustedes se preguntarán quién es. Antes Mancino y Aler brevemente se preguntarán o no. Él es ingeniero electrónico con una diplomatura en ciencia de datos e ingeniería en Inteligencia Artificial. En su primera etapa en INVAP se dedicó al procesamiento de señales y datos en el grupo de modelística, participando de manera activa en la ingeniería de sistemas de los radares primarios y aerotransportados. En 2021 se unió a VURU, una health tech que busca optimizar el flujo de las operaciones en quirófanos y equipos de diagnóstico por imágenes a través de Inteligencia Artificial. En 2023 regresó a INVAP. Muchas personas. Y se sumó a la gerencia de gobierno en la división de Inteligencia Artificial. Sí que bueno, Andrés. Gracias Daniela por la presentación. Bienvenidos a todos y muchas gracias también por hacerte el tiempo de poder participar. Sea porque pudieron hacerse presente acá, o sea porque se han logrado sumar de manera virtual. Soy consciente de que el tema despierta muchísimo interés. Espero que la charla esté a la altura de la expectativa. Así que... Arrancamos con... Este no escuchó, ¿eh? Arranqué con un título originalmente, porque la idea era darle un enfoque. Y a medida que... que la firmando la charla me fui dando cuenta de que en realidad está piola o quizás este espacio, porque básicamente uno acá puede contar en qué está laburando desde una perspectiva diferente. Y me pareció interesante poder acercarles una charla que pueda llegar a tener un impacto en el futuro cercano cuando tomemos la decisión de en algún proyecto tratar de desarrollar sistemas inteligentes escalables y sostenibles en el tiempo. Así que... Andrés, antes de comentar, si puedes ubicarte entre las banderas para que la gente que está conectada te vea. Ok. Eso me da la huenza. De frente o de espalda. Bueno, nada, mejor así. El interés en inteligencia artificial es algo que ya existía, digamos, pero está claro que con la aparición de CharGPT allá por noviembre del 2022, la inteligencia artificial generativa, aquellos sistemas inteligentes capaces de generar nuevo contenido, como texto, video, imágenes, audio, potenciaron un poco ese interés y básicamente porque redefinieron la forma en la que interactuamos con la tecnología. Y eso desencadenó, por un lado, debates cruciales acerca del impacto que tiene esta tecnología en los ámbitos personales, profesionales, corporativos, y como sociedad, como por ejemplo la mejora de la productividad, el desplazamiento de los puestos de trabajo, la reconversión laboral, la erosión de la privacidad, incluso cuestiones que tienen que ver con la diseminación de desinformación. Y a nivel gubernamental, obviamente, empezó a pasar también el desafío de tener que, en algún punto, regular la necesidad de las regulaciones, de la redundancia, porque en algún punto se tiene que promover la protección a los ciudadanos y promover la ética y por el otro, no tratar de frenar el proceso del progreso y la innovación. Pero por otro, empezó a pasar que la mayoría ha querido empezar a utilizar inteligencia artificial por el simple hecho de que es una tecnología en tendencia, tópico en tendencia. Y en ese contexto, yo no estoy en total desacuerdo con lo que dice el jefe de WALL-E acá en el Comité de Dilbert, pero me parece que es fundamental entender qué implica verdaderamente el detrás de bambalinas, de tener que desarrollar sistemas inteligentes escalables y sostenibles como para cuando uno realmente tome la decisión de embarcarse en proyectos que tengan que ver con un sistema inteligente, tenga en cuenta cuál es el impacto de lo que implica tener que desarrollar una tecnología de este tipo. Entonces, la charla está estructurada como para recorrer desde el ciclo de vida en los proyectos de inteligencia artificial, magileno en particular, pasando por los roles, la infraestructura y las herramientas que en algún punto soportan ese ciclo de vida para después poder tratar de circunscribirlo en un marco metodológico de cómo sería piola llevar adelante este tipo de proyectos. Y finalmente, dedicar un ratito a charlar de por qué fracasan este tipo de proyectos. Y finalmente, dejar algunos puntos clave que tienen que ver con lo que estaría bueno que haga la organización y lo que estaría bueno que tengan en cuenta los proyectos que tengan intenciones de subirse a tablas. Bien, si arrancamos con el ciclo de vida de los proyectos de Machine Learning y un poco acollándome en esta explosión de la inteligencia artificial generativa, empezó a suceder mucho de que las empresas se encontraban en la situación de que si no adoptaban esta tecnología, podían eventualmente perder el terreno ganado en el área en la que se encontraban y en el peor de los casos, directamente quedarse afuera, completamente afuera de un espectro de negocio particular. Entonces empezó a sobrevolar la idea de que uno a partir de una idea, proveyendo datos, puede crear un sistema inteligente o instanciar un sistema inteligente entrenado por un tercero, obtener predicciones asociadas a esos datos para responder a la idea. Y lo cierto es que cuando uno trata de llevar eso a la práctica, la realidad es bastante más compleja, tiene múltiples aristas y si uno las quiere llevar adelante de una manera correcta, bueno, nada, tiene que prestarle atención. Entonces la idea es enfocarnos en la realidad, ir tratando de recorrer las distintas etapas, haciendo énfasis en las características claves y las preguntas incómodas que nos tenemos que hacer en cada una de las fases, como para entender lo que significa tener que desarrollar un sistema inteligente escalable y sospechoso. Entonces todo comienza con tener que describir correctamente la problemática que queremos resolver. Básicamente en esta etapa uno lo que intenta hacer es tratar de recolectar la mayor cantidad de información posible que nos permita describir de una manera integral la problemática que uno quiere resolver. Yo ahí dejé lista algunas series de preguntas. La lista no es exhaustiva, pero intenta tratar de generar algunos alineamientos y remarqué algunas que me parece que son importantes. Una de ellas es cuáles son los indicadores que reflejan la existencia del problema. Porque básicamente si uno conoce cuáles son los indicadores que reflejan la existencia del problema, va a poder cuantificar el impacto que tiene ese problema en el contexto actual. Y ahí está bueno hacer el ejercicio de tratar de llevar esos indicadores a variables sensibles, a variables en las cuales se sienta el golpe. Y básicamente esas son guita y tiempo. Entonces cuando uno presenta el problema de cuánta plata estoy perdiendo, o por ahí si lo quiere ver de una manera más optimista es cuánta plata me puedo ahorrar, o cuánto tiempo estoy perdiendo, el impacto se hace más tangible y probablemente uno pueda llegar a tener un criterio de éxito mayor. Y obviamente esos indicadores básicamente nos van a permitir en el futuro poder tener un baseline contra el cual comparar la solución que estamos proponiendo desarrollar. Porque en algún punto nos va a permitir ver qué tan bueno es lo que estamos haciendo o no. Y finalmente, no menor digamos, hay que tener en cuenta qué datos están disponibles al momento de abordar el problema. Porque básicamente eso nos va a permitir entender con qué datos contamos para poder desarrollar una solución que de alguna forma sea escalable en el tiempo. Luego de eso viene la etapa de recolección de datos. Y acá uno tiene que poder identificar la fuente de datos. Generalmente sucede que cuando uno arranca con un proyecto de este estilo, en una fase temprana aparece alguien con un set de datos y dice, yo quiero poder resolver determinada problemática y lo que tengo es este set de datos. Bueno, cuando a uno le acercan ese problema automáticamente empieza a preguntarse es una única fuente de datos, son múltiples fuentes de datos, cómo accedo a esos datos. No es lo mismo tener que acceder a un sistema de archivos que tener que acceder a través de una consulta o una base de datos, dependiendo del motor, base de datos que tenga detrás. ¿Los tengo que almacenar a esos datos para poder después utilizarlos? ¿Los tengo que trasear para poder identificar de dónde provienen y dónde terminan? Entonces empieza a pasar mucho de que cuando se empiezan a hacer esas preguntas empieza fundamentalmente a pensar por debajo en qué tecnologías tengo que usar para poder hacer la extracción de datos. No es lo mismo tener que consultar un sistema de archivos que tener que hacer una consulta o una base de datos. Obviamente si los datos necesitan estar etiquetados, porque básicamente necesitamos tener la verdad contra la cual comparar por ejemplo un proyecto de Machine Learning que sea supervisado. El etiquetado de datos es una etapa fundamental, hay que poder instanciar tecnologías que permitan hacer eso. Es un mundo en sí mismo. Luego tenemos la necesidad de empezar a almacenar los datos para básicamente poder encontrar el lugar en el cual almacenarlos para como dije antes, tratar de trasearlos, usarlos, reutilizarlos, poder compartirlos. Hay un montón de cosas que tienen que ver mucho con la infraestructura alrededor de todo eso que vamos a tocar después. Y ahí, más tarde que temprano, uno incluso se tiene que poder preguntar con qué frecuencia vamos a recolectar los datos. Porque de repente alguien viene con el problema y te dice yo tengo esta cuenta de datos y esa cuenta de datos representa un instante de tiempo particular del problema que quiero resolver. Ahora, los datos varían en el tiempo. Si varían en el tiempo. Tengo en el universo de datos que me diste originalmente esa posibilidad. Tienen estacionalidad. Bueno, básicamente uno se tiene que hacer esas preguntas. Obviamente empiezan a entrar cuestiones que tienen que ver. Tengo que acceder a datos que son privados. Tengo que tener ciertas reflexiones con lo que tiene que ver con la privacidad y la protección de los datos. Son cuestiones que uno se tiene que ir preguntando. Yo voy a pasar por un proceso lineal. Pero después de esto básicamente va a tener un montón de iteraciones. Luego pasamos a la preparación de datos. Donde uno dice, bueno, básicamente tengo mi primer set de datos. Hago un análisis exploratorio. Puedo hacer findings. Y generar insights para el negocio. Obviamente lo voy a tener que limpiar porque muchas veces lo que nos pasa es que guardamos muchísimos datos de más. Entonces eso también empieza a plantear la necesidad de tener que preguntarse qué datos tenemos que guardar cuando tenemos un sistema de este estilo. Uno muchas veces guarda por guardar. Y después utiliza el 5 o 10% de los datos que almacenó. Puede tener que transformarlos. Porque básicamente no están en las dimensiones en las que le interesa. Lo puede tener que aumentar. Básicamente hacer agregaciones con distintas fuentes de datos. Y donde precisamente se tiene que apoyar en una única fuente de datos para resolver el problema. Y ahí empieza toda la parte en la cual uno empieza a seleccionar aquellas características claves. Que básicamente le permiten entender cuáles son las variables más importantes para el problema que quiere resolver. Obviamente eso tiene que estar, lo vamos a ver después, apoyado a lo largo de todo el siglo. Con expertos de dominio que en algún punto nos ayuden a entender qué problemática tienen los datos. Y obviamente todo ese proceso de transformación y limpieza, bueno nada. Entra en juego también cómo almaceno esos datos preprocesados. Porque yo ya hice el ejercicio de masajearlos, por decirlo de alguna forma, y tener que guardarlos. Entonces, si yo no tengo un lugar donde almacenarlos, significa que voy a tener que masajearlos cada vez que me llegue un set de datos nuevos. O el mismo que me pasaron originalmente lo tengo que volver a masajear. Entonces, nada, se hablan un montón de interrogantes que por ahí tienen que ver de vuelta. Con cuestiones que tienen que ver con cosas más aledañas a lo que es el proceso. Pero que está bueno empezar a imaginarse. Y obviamente, luego entramos en la etapa del modelado y la evaluación. Ahí tenemos la posibilidad de seleccionar los algoritmos de Machine Learning. Contornamente para el tipo de problema que queremos resolver. Puede ser aprendizaje supervisado, no supervisado, puede ser read learning. Hay ahí una infinidad, no necesariamente tiene que ser un algoritmo, pueden ser muchos. Uno puede evaluar más de uno. Empieza a tener que seccionar los datos. En entrenamiento, validación y test. Porque básicamente sobre esos set de datos va a terminar entrenando el modelo y validándolo. Para entender cómo se desempeña lo que ha desarrollado. Y después entra toda la etapa del entrenamiento del modelo y el reajuste. Y la optimización para poder dar con métrica de desempeño. Que en algún punto traten de estar alineadas a lo que el negocio espera. Y acá es fundamental entender que las métricas del modelo. No necesariamente tienen que ser las mismas que los indicadores que consume el negocio. Y uno claramente tiene que empezar a pensar si el set de datos con el cual entrenó. Tiene una distribución representativa. Si hay que recolectar más datos. Si el etiquetado que hice en una primera instancia es escaso y lo tengo que volver a repetir. Todo esto termina siendo un proceso continuo. Y una vez que logré entrenar el modelo y desplegarlo. Entrenar el modelo y tenerlo listo para desplegar. Viene la etapa de despliegue. Y en la etapa de despliegue uno tiene que hacer un montón de cosas asociadas a dónde lo voy a desplegar. Porque por ahí lo entreno en una máquina personal. O lo entreno en un data center. O lo entreno en la nube. Pero después lo tengo que ir a desplegar on the edge. En un dispositivo lo tengo que desplegar dentro de un radar, dentro de un satélite. En un hardware que por ahí tiene restricciones de costo computacional muy chico. Entonces uno empieza a preparar el entorno de producción. Lo logra desplegar. Lo integra con sistemas existentes. Básicamente el modelo de Magic Learning te va a terminar devolviendo alguna métrica puntual. Y sobre eso va a haber que construir una capa aplicación del negocio. Que te termine llevando de lo que es el modelo a lo que es el problema del negocio en sí. Y obviamente una vez que está desplegado es como toda cuestión todo producto. Necesita estar monitoreado para mantener no solo la performance del modelo. Sino también para poder entender en qué momento hay desviaciones y desgradaciones. Que nos implique a nosotros tener que reentrenar el modelo con datos nuevos. Esto devuelta atándolo con el tema de que eventualmente los datos varían con el tiempo. Entonces es importante tener presente de que eventualmente uno tiene que llegar para ese lado. Y obviamente todo eso nos lleva a pensar de que hay que definir políticas de reentrenamiento. En qué momento tengo que reentrenar y en qué momento no. Por qué tengo que reentrenar y por qué no. Entonces, nada. Eso es solamente un flujo lineal. Y lo que termina pasando es que no todo es color de rosas. Y lo que hay son muchas desinteracciones a lo largo de todo esto. Y acá quiero hacer bastante énfasis porque la recolección de datos es un problema en sí mismo. Uno tiene que pensar en la infraestructura, tiene que pensar en políticas de privacidad de datos. Tiene que pensar en la seguridad de los datos. Tiene que pensar en dónde los va a almacenar, cómo los va a almacenar, por qué los va a almacenar. Quién va a acceder, quién no va a acceder. Si uno piensa en tratar de brindar una solución que escale en el tiempo, indefectiblemente va a tener que pasar por ese lado. Y obviamente los datos hay que ordenarlos, arreglarlos para poder contar una historia. Porque si no, no dejan de ser un montón de cuestiones digitales almacenadas en algún lugar que por ahí no terminan teniendo el valor que realmente prefieren. Y es fundamental entender que si nosotros entregamos nuestro modelo con datos que son una cagada, lo que vamos a tener teniendo es otra cagada. Entonces, es entender que es súper importante, perdón por el léxico, es importante entender que hay mucho de esto que se sustenta en los datos. Fundamentalmente en los datos. Y obviamente una vez que el modelo terminó funcionando en nuestra especie personal y lo queremos desplegar, ahí hay otro quilombo igual o más grande que el asociado a la recolección de los datos. Porque básicamente uno tiene que pensar, desplegar un modelo, monitorearlo, tratar de entender en qué momento y satiar políticas de reentrenamiento, de monitoreo. ¿Qué pasa si se me cae? ¿Quién lo levanta? ¿Cuál es el mantenimiento? Y empieza ya a ser una cosa que escala exponencialmente. Obviamente eso es lo que tiene que ver con el ciclo de vida. Y ahora la idea es pasar muy brevemente sobre los roles, la forma de estructurarse, la infraestructura y las herramientas que en algún punto van soporte a ese ciclo de vida de Machine Learning. Entonces, está bueno identificar que generalmente uno tiene las actividades por los proyectos y esto sucede a cross-proyectos. Entonces, los equipos y la infraestructura junto con las herramientas generalmente son a cross-proyectos. Después vamos a ver, existe la posibilidad de poder alocar equipos a determinados proyectos en forma de estructurarse que fomentan, facilitan que esto se pueda hacer de una manera un poco más flexible. Básicamente, los roles estándar que están en la industria, hay un montón. Esto es una tecnología que está en constante evolución. Y como tal, hay un montón, una línea muy grande asociada a cómo se segmentan las habilidades de los roles asociados a estas cosas. Entonces aparece una infinidad de roles y estos simplemente son los cuatro más importantes. El ingeniero de datos, el generalista de datos, el científico de datos y el ingeniero de Machine Learning. Y lo que está peor a resaltar acá es que estos roles básicamente cubren todas las habilidades con algún grado de superposición. Y está bueno entender que tanto el ingeniero de datos como el ingeniero de aprendizaje automático de Machine Learning están bastante más orientados a todo lo que tiene que ver la disponibilización de la operación de este tipo de tecnologías. Mientras que el analista de datos y el científico de datos, en algún punto lo que hacen es estar más ligado a la creación de los modelos, a entender por qué el modelo performa como se preforma, a tratar de sentarse con el individente para poder entender cuáles son sus problemáticas y cómo trasladarlas a un dominio por ahí un poco más técnico. Y esto básicamente es más o menos un primer video gordo, cómo los roles se ajustan a las actividades a lo largo del ciclo de vida. Donde básicamente el científico de datos suele estar a lo largo de todo el ciclo de vida del proyecto de Machine Learning. El data engineer está bastante más enfocado en todo lo que tiene que ver con la recolección y la preparación de datos. Obviamente el científico de datos siempre está presente. El analista de datos está orientado en la parte de lo que es la preparación de datos con el objetivo de encontrar findings, insights al negocio, poder generar métricas, reportes. Insisto, cada uno de estos roles tiene un set de habilidades particular porque todo lo que es presentación de métricas, reportes, visualización de datos, eso es una temática en sí misma. No solo para lo que es la ciencia de datos y la inteligencia artificial, sino en general. Y el Machine Learning Engineer es básicamente el homónimo al data engineer, pero para todo lo que tiene que ver con el despliegue del modelo, la monitorización del mismo a lo largo del ciclo de vida del producto. Obviamente, mira, pasando de que siempre tenemos un sombrero, nos sacamos y nos ponemos el otro. Entonces empieza a pasar de que muchas personas tienen más de un rol, incluso muchas veces son los encargados de tener que fesionar a ese equipo técnico e incluso muchas veces encargados de ser el de setear la estrategia asociada a la temática. El CAIO es el Chief Artificial Intelligence Officer. Entonces, está bueno pensar en cómo estructurarse. Básicamente, la estructuración depende exclusivamente del nivel de experiencia que tenga la compañía o la empresa en el manejo de la tecnología, obviamente del volumen del proyecto que tenga y el alcance de esos proyectos y de la colaboración multifuncional, entre áreas. Básicamente hay tres enfoques, centralizado, federado y bebido. Voy a pasarlo bastante rápido, si quieren ustedes internamente pueden hacer una analogía con lo que sucede en la empresa. Uno es donde básicamente hay un equipo de inteligencia artificial a cross-empresa en el cual básicamente se agrupan los roles anteriores. Obviamente acá aparecen otros roles que tienen que ver con la gestión y demás, pero acuérdense del hombre araña señalándose entre todos. Con lo cual puede suceder que muchos de estos terminen poniéndose un sombrero que no sé si quieren, pero que eventualmente lo van a tener que usar. Donde básicamente está todo agrupado y este grupo brinda servicio a lo largo de toda la empresa. Entonces acá está bueno entender que esto es el enfoque que generalmente terminan utilizando las start-ups en una parte temprana de conceptualización de un proyecto, porque le da rapidez, dinamismo. Uno no depende de estar interactuando con las otras áreas. Básicamente tiene el control absoluto de lo que quiere hacer y cómo lo quiere hacer. Entonces, de vuelta, va a depender de la madurez y el manejo de la tecnología. Después existe el federado, donde básicamente uno lo que hace es mueve a los roles más orientados al modelado y asociado a la expertiz de dominio hacia, digamos, los equipos satelitales. Y básicamente deja centralizado todo lo que tiene que ver con infraestructura para lo que es recolección de datos y disponibilización del modelo. Eso está bueno porque en algún punto uno termina teniendo los expertos de dominio en cada uno de esos equipos. Tiene lo malo de que pisa por la ficción y tener que priorizar básicamente cómo este equipo de data y más IT termina respondiendo a las necesidades de cada uno de los proyectos alrededor de los sectores. Y el otro es el embebido, donde claramente cada sector termina embebiendo estos perfiles para tratar de agilizar todo lo que tiene que ver con la temática puntual de su dominio. Obviamente acá no están puestas las relaciones, mucho menos la idea no era hacer nada sobre eso, sino simplemente hacer referencia de que en un esquema como este uno va a tener que de todas formas tratar de sincronizar y setear criterios entre los distintos equipos a lo largo de la organización. Con la infraestructura pasa lo mismo, digamos. Así como hay un montón de cuestiones que tienen que ver con cómo nos organizamos, con la infraestructura pasa lo mismo. Y sobre todo está el dilema de qué hago de manera local y qué hago de manera remota o en la nube. No lo quiero plantear como que es o local o en la nube, para mí es un esquema híbrido. Hay situaciones en las cuales hay que hacer cuestiones locales, on-premise, y hay cuestiones que se pueden hacer en la nube. Lo que hay que entender acá, la analogía o la ilustración del IVERS son los costos visibles, los costos escondidos. Y básicamente hay que centrarse en la escalabilidad, la disponibilidad, el mantenimiento y la seguridad. Entonces uno tiene que pensar si su proyecto tiene que escalar, qué implica eso localmente, qué implica eso en la nube. La nube es contrasencillo de escalar en la nube, pero básicamente lo que a uno le termina sucediendo es que tiene muchas restricciones de qué datos dejo disponible en la nube y qué no. Entonces empieza a entrar toda una cuestión de sensibilidad de datos asociada a eso. Lo mismo pasa con la flexibilidad y lo mismo pasa con las otras dos variables. Y acá yo lo que hice fue un ejercicio de tratar de traerles un ejemplo de lo que implicaría tener que desplegar una aplicación RAN. Básicamente es Retrieval Aumented Generation. Para el inglés es Generación Aumentada por Recuperación. Y básicamente consiste en tratar de usar uno de estos LLM, como el ChartGPD, en este caso voy a usar el ejemplo de Llama370VitaminaParámetro. Usar la capacidad de generación de texto de esos modelos combinada con técnicas de recuperación de información para, por ejemplo, hacer consultas a documentación técnica. Una aplicación que puede estar bastante en agujes por estos momentos. Entonces si uno agarra y dice que quiere tomar el modelo 70B, que es el más grande de la familia de Llama, al menos de los públicos hasta el momento. Tienen intenciones de largar uno de 400 billones de parámetros y quiere desplegarlo en una precisión de 2 bytes. Esto termina generando un requisito de memoria de 260 GB de memoria. Si uno quiere poder desplegar ese modelo en hardware local o en la nube, bueno, ahí puse un ejemplo de un server con dos placas NVIDIA 30 de 24 GB. Es básicamente el DRQ que tuvimos en ese momento. Un server por 2 a 30 son 89 mil dólares masivos. Eso multiplicado por 6. ¿Por qué 6? Porque básicamente se estimaba que este modelo se podía alocar en 12 placas. Uno lo que hace es agarrar la capacidad de memoria que necesita y lo divide por la capacidad de cada una de las placas. La cuenta nos da 12. 10.8 creo. Básicamente lo que hice fue redondearlo para arriba, pero básicamente estaría necesitando una inversión del orden de medio palo verde para poder alocar un modelo de ese estilo. Y no solo eso, sino que tendría ese hardware de medio palo verde ocupado pura y exclusivamente para tratar de desplegar ese modelo on-premise. Por el contrario, si hacemos eso mismo en la nube, nosotros tenemos cuenta en AWS. Eso es para inferir nada más o entrenamiento también? Eso es para inferir. O sea, ¿de dónde salen los 260 GB de memoria? Gracias por la pregunta. Básicamente son los 70 billones de parámetros, el tamaño del modelo multiplicado por 2 bytes. Y la regla de Edo Gordo establece que uno necesita dos veces el tamaño del modelo para inferencia. Para entrenamiento es 4. Así que aparte tenés que sumarle lo que vas a gastar en el cloud. Exactamente. Acá viene lo del cloud. Si uno solamente lo quiere desplegar en placas Nvidia 140 GB, estas placas son de mejor desempeño que estas placas. No voy a entrar en los detalles técnicos, pero son mejores las placas en cloud que las placas que teníamos en SRFQ. Sí, básicamente necesita 6.5 placas. Básicamente los rounde a 8 porque en la nube generalmente te dan 4 o 8. Te lo dan con potencia 2. Y esa es la instancia de AWS, la instancia de AWS de Active Compute, que básicamente está del orden de $33 dólares la hora. Por lo cual si uno piensa en, ok, quiero tener este modelo, Shama 3, desplegado 8 horas por día, los 365 días del año, necesito $98.000 dólares. Obviamente con toda una logística asociada a llegar a las 8 horas, lo tenemos que apagar, llegar al otro día no tenemos que prender. Y si lo quiere tener prendido los 365 días del año, las 24 horas del día, está hablando de... Presenta Lucas. Obviamente acá entra de juego el costo de la privacidad de los datos y que es lo que uno quiere hacer. Y no me quiero poner filósofo, pero en algún punto este es el esquema de desplegar los modelos en la máxima precisión disponible actualmente. Uno puede pasar a esquemas donde los modelos estos están cuantizados y a expensa de cuantizarlo y bajarle la precisión puede, hay algunas técnicas nuevas que no, perder precisión en las respuestas, pero baja los requerimientos de Farquhar que necesita. Entonces hay que tener en claro que por ahí si uno quiere desplegar una aplicación de este estilo, bueno nada, tiene que invertir mucho para desplegar algo que sirve para consultar la documentación. Eventualmente no necesariamente tiene que estar únicamente utilizado para desplegar este tipo de cosas. Uno tendría la capacidad de tener 12 placas on-premise para poder hacer entrenamiento. Perdona Andrés, una pregunta. El ejemplo de Amazon Web Services es simplemente para mostrar un cloud o da alguna característica técnica especial? Básicamente que estas placas son mejores que estas placas y que hoy como INVAP nosotros tenemos acceso a este tipo de distancias. Entonces la idea era poder hacer una comparativa de lo que implica tener que invertir localmente versus lo que implica tener que tener un costo computacional similar para desplegar algo parecido en un ambiente diferente, con la salvedad de la privacidad de los datos. Si esto lo haces con Azure, por ejemplo, es otra cuenta? Es otro precio, pero será otra instancia el nombre, será otro precio. Este precio es pay as you go, con lo cual es a demanda. Eventualmente hay otros planes que son más baratos, incluso donde vos de alguna forma te comprometes a tener esas instancias prendidas durante cierto tiempo y eso te abarata el costo en la nube. Pero la idea simplemente era poner AWS porque es el cloud que INVAP tiene disponible. En términos de herramientas, nada, este FirstMark es un capital de riesgo que está orientado a inteligencia artificial en las etapas tempranas de las start-ups y básicamente termina mostrando las últimas tendencias y los avances y esto es lo que tiene que ver con el panorama de infraestructura, herramientas y demás en el ámbito de inteligencia artificial. Este era el panorama en 2020, este es el panorama en 2024. No voy a hacer énfasis en el detalle, lo que quiero mostrar con esto es que el abanico de herramientas alrededor de lo que es el código de Machine Learning es infernal, crece a un ritmo deliberadamente rápido y un poco tratar de seguir este ritmo no es moco de pavo, hay que estar atento a que sí y que no. Y obviamente termina pasando de que cuando uno termina viendo qué hacer un proyecto de inteligencia artificial se empieza a encontrar con que en términos relativos el costo del código de Machine Learning termina siendo totalmente bajo comparado con toda la infraestructura que tiene que tener alrededor de estas cosas. Y no por eso le estoy bajando el precio a lo que implica tener que entrenar un modelo y demás cuestiones. Pero este paper que está acá es de Google, se los recomiendo, porque en algún punto me entiendo un poco esto, es decir, el costo de la deuda técnica que uno generalmente arrastra a lo largo de los proyectos y el termina llegando a costos continuos de mantenimiento cuando están en producción. Esto aplica para Machine Learning, pero en realidad lo podemos trasladar a cualquier tecnología de la que estamos acostumbrados. Cuando hablamos del marco metodológico, lo que se suele usar acá es el lema de fallar rápido literal, con el objetivo de rápidamente evaluar qué funciona, qué no, y si no funciona tratar de mejorarlo. Y en ese contexto se suele enmarcar todo en el contexto de prueba de concepto, prototipo y mínimo producto viable. Estas básicamente son las distintas etapas y en qué están orientadas cada una de estas. Y está bueno tratar acá de que tanto la prueba de concepto como el prototipo, esto podemos discutir infinito, no están orientadas al cliente, sino que están orientadas a minimizar la fase y el aprendizaje sobre estas cosas para tratar de poder de manera integral brindar una solución robusta en un MVP. Y eso está bueno que nos hagamos esa idea, porque si no después terminamos convirtiendo nuestros prototipos en productos. Y mantenernos es un dolor de juego. Y finalmente, no tan finalmente porque ya está casi cerca del final, charlar un poquitito sobre los proyectos de Machine Learning que fracasan. En el 85% de los proyectos de Machine Learning fracasan, hay una fuente. Y acá lo voy a evitar a ustedes a que me cuenten por qué se imaginan estos proyectos fracasan. Pueden escanear el QR y básicamente esto es un vago cuor para empezar a aparecer las palabras. Y la idea es que entre todos tratemos de adelantarnos a lo que viene en la edilidad que sigue. Hay una racha que no muere. La racha anentada, digamos. Por eso está el código también, ¿no? ¿Qué es lo que está pasando? No me haces por la pachamada. Sí, por eso. Por el cual no. Me han quedado en la pachamada. Me han quedado en la pachamada. No, no. No, no. No, no. No, no. No, no. No, no. A ver si... No, no, no. Sí, no, sí. Sí, no es lo mismo que está ahí. Seguro, seguro. Sí, el código está bueno. Sí, no es lo mismo que está ahí. El código está bueno. Pueden escribir más de una vez, eh. Igual les como un ratito y... crawling salmon習ktor Aremako Bueno, buenísimo. Bueno, podemos hacer una frase con eso. Bueno, efectivamente no entender el problema es una de las grandes razones. Ahora lo vamos a ver. Pero la calidad y la falta de arroz es fundamental. Entonces se lo saco. Obviamente acá hay algunas razones de esa referencia que después todo el 85% de los proyectos de Recuerdenes Learnes fracasan. En algunos campos todavía es una investigación, eso es cierto, digamos. No hay que desmentirlo. Pero después se empieza a pasar mucho de que los que están condenados al fracaso son aquellos que técnicamente son inviables porque no entendimos el problema, porque no contábamos con los recursos, la infraestructura y demás cuestiones aledañas. Y otra cosa que suele pasar mucho es no pegan el salto a producción. Entonces uno cree que porque de repente agarró un set de datos, hizo un modelo, lo entrenó, lo probó en su computadora y funciona. Y ya está, se terminó. Hay todo un mundo alrededor de desplegar y mantener esto y tratar de recolectar el feedback y los datos para poder entrenarlos y hacer que esto sea sostenible en el tiempo. Y eso claramente está atado con la creación de los datos. Si yo no tengo toda la infraestructura necesaria para poder seguir adquiriendo datos de ese mismo problema, no voy a poder mejorar esta solución. Entonces yo resalté esas dos porque para mí terminan siendo, digamos, de las más importantes. Obviamente el alcance eficiente y los criterios de éxito poco claros, básicamente son dos que la mayoría de ustedes resaltó como uno de los procesos de las razones más importantes. Y un poco la imagen es alusiva, pero básicamente uno tiene que tener una estrategia, tiene que tener infraestructura, tiene que asegurarse un gobierno, sobre todo eso de bajo, para poder pegar el salto y efectivamente convertirse en una empresa que quiere hacer AI e Inteligencia Artificial, aplicando Machine Learning. Y para terminar, los puntos clave para el lado de la organización, obviamente definir el norte. Básicamente queremos aplicar Inteligencia Artificial como soporte a nuestras operaciones, si lo queremos aplicar para potenciar nuestros productos, si queremos hacer nuevos productos o brindar servicios apoyados en este tipo de tecnología, alguno, todos, ninguno, esa decisión hay que tomarla porque en algún punto va a terminar delineando si cuál va a ser la inversión en infraestructura, recursos, cómo estructurarnos y ordenarnos. Obviamente política de datos, nosotros somos una empresa que genera datos, básicamente toda nuestra área de negocio, entonces por qué no pensar en contrato de datos, acuerdo, licencia, suscripciones, tener un catálogo internamente, algo que sea fácil, digamos, de poder consultar una plataforma de datos que podamos consultar de manera segura para poder construir aplicaciones arriba de eso. Obviamente todo eso está asociado con la política de seguridad y privacidad de los datos, vamos a tener que auditarlos y traciarlos, entender desde dónde salen y hasta dónde llegan y dónde se lo mantenan y quién tiene acceso a eso. Obviamente eso tiene que ser lineamientos a cross proyecto, no puede venir un proyecto y decir yo quiero manejarme de esta forma y del otro lado manejarse de otra forma, no digo que no se pueda, no es que no lo haga directamente, digamos, sino que lo que trato de plantear es que está bueno tener lineamientos generales que nos permitan a todos movernos sobre una misma base. Y obviamente esto que mencionaba antes de poder traciarlos, de dónde salen, qué calidad tienen, cómo evolucionan en el tiempo. Y obviamente necesitamos tener una política de recursos, sobre todo infraestructura para entrenamiento, esto a nivel organización, está bueno que la organización tenga infraestructura para entrenamiento y que los proyectos luego se encarguen de la infraestructura para el despliegue. Sobremos nosotros en los proyectos generalmente comprar el mismo hardware que va a terminar teniendo el producto para luego tener un gemelo digital y entrenar ahí. Está bueno, pero está bueno que en realidad esto se administre de otra forma, lo mismo que los roles y los equipos. Está bueno tener una política de cómo pensamos estructurarnos si es que queremos embarcarnos en esto, porque en algún punto va a terminar definiendo cómo van a terminar siendo las interacciones para los distintos proyectos. Y a nivel proyectos, bueno, hacer la referencia de que no es sólo la idea generativa y el boom de chart que tenéis y querer utilizar un LLM para consultar documentación técnica, está buenísimo, básicamente eso es lo que consumimos todos nosotros a diario, pero por ahí está bueno pensar un poquito más allá y tener en claro que es un proceso iterativo esto. Y en ese proceso tenemos que tener súper claro cuál es el problema que queremos resolver, cuáles son los indicadores que vamos a tratar de monitorear y por qué. Porque si nosotros no tenemos claro eso, el criterio de éxito claramente no va a generar el impacto que uno espera. Obviamente hacerse las preguntas típicas de qué datos necesito, los tengo, cómo los extraigo, qué restricciones tengo. Las restricciones están puestas más pensando en esto de privacidad de datos. Che, ¿los datos son míos o los datos son del cliente? Cuando nosotros hacemos un SET, como el ejemplo de un RAR, los datos del RAR son nuestros o son del cliente? Lo mismo sucede con los datos de los productos que generan los satélites. Está bueno pensar un poquito más allá y quizá haya un universo de soluciones alrededor de eso. Y obviamente, lo que mencionaba antes, los resultados dependen de los datos. Y en ese contexto, les dejo una referencia que no es tan nueva, digamos, de Economist de mayo del 2017, donde básicamente los datos son el nuevo petróleo, no tan nuevo porque es del 2017, pero básicamente la analogía que subyace acá es entender que los datos sin respirar tienen valor. Lo mismo que el petróleo. Y en una era digital, en la cual esto cada vez empieza a crecer cada vez más, es súper importante entender que detrás de esto hay un activo pijo insubstantial. Así que bueno, eso era un poco lo que tenía para contarles. Perdón por la velocidad. Teniendo en cuenta que en general somos proveedores de One of a Kind. Y ahí es donde no sé cómo podemos aprovechar esos datos porque en definitiva tenemos una línea de productos sumamente complejos. O sea, son productos y en algunos casos son proyectos, ¿no? Sumamente complejos y además único tipo. Los radares por ahí podrían llegar a cuando justamente yo estaba pensando en esto y vos nombraste radares, dije bueno, radares, verdad, por ahí tenemos más de uno. Pero digamos, ¿cómo se puede llegar a aprovechar ahí? Porque nuestra base de datos, siendo que son proyectos únicos. ¿Sólo son productos o son proyectos? No, productos. ¿Y directamente generan datos? Sí, está bien, pero no tienen repetitividad esos productos. No tenés una fabricación en serio. Claro, y además ese producto que aporta un proyecto es único de su tipo. Entonces, no sé. Ahí no existe mayor problema, digamos, porque vos podés no tener datos si tenés una distancia en la cual vos podés generar datos sintéticos a partir de modelos de inteligencia artificial. Lo cierto es que vos finalmente estás entregando un producto, ¿no? Sí. ¿Vos podés potenciar ese producto con inteligencia artificial? ¿Y podés pensar una línea de productos asociada a esa? Sí, bien. Ahora, ¿a qué datos tengo que recurrir? Porque digamos, vuelvo a repetir, ese producto, capaz que es único. Sí, claro. ¿Cuál es el producto que... ¿A qué área de negocio te da referencia? No, estoy viendo, o sea, en general, el área de negocios de Inback por ahí. Te lo pongo como ejemplo. Por ejemplo, los radares generan datos astélics, del control de vigilancia aérea. Esos datos son sensibles, son de la fuerza aérea. Sí. No se generan. Sí, sí. Y cada producto, tal cual lo describís vos, es único. Hay una fabricación en serio, pero hay ligera diferencia entre los distintos productos que hacen a la configuración, a la calibración y demás cuestiones. Vos tenés un volumen de datos infernal ahí. Eso sí. No sé, me muevo ahora de repente al área satelital y de repente tenés el saucón que genera imágenes SAT. Tenés un producto ahí generado. ¡Qué huela! ¡Qué huela! Que generan datos de la órbita y demás cuestiones. ¿No sé, vos tenés? Sí, siempre igual. Son generadores de datos con lo que saca dentro. Sí. A nivel reactor nuclear, vos estabas haciendo experimentos en los reactores de investigación, donde básicamente vos estabas viniendo un experimento para tratar de optimizar un determinado proceso. Y en ese proceso generás datos que vos puedes optimizar ese proceso utilizando inteligencia. En realidad esos datos son propiedad del cliente. Bueno, ahí va la delineación. Va a depender del contrato que firme. Si es que querés ir para ahí, porque para mí primero hay que responder cuál es el orden. Claro, sí, sí, también. Si vos no querés ir para ahí, pues yo te doy la derecha. No, no, no, yo no es que no quiera. Quisiera visualizarlo, o sea, porque no alcanzo a visualizarlo. Está bien, pero básicamente los sensores que generamos nosotros generan productos, que son datos. Sí, sí, eso corre. Entonces, sobre esos datos vos podés generar otros productos que aporten valor. También tenés telemetría, ¿no? Todos nuestros sistemas generan telemetría y puedes hacer un experimento preventivo. Definitivamente sí. Digamos, lo que yo estoy viendo es que casi toda esa información es sumamente sensible, con lo cual vos tenés que acordar con el que va a operar eso. Sí, pero por ahí es una línea a investigar, ¿no? Que hoy, claramente por ahí, por alguna razón, no se está activando. No, o sea, digamos, en Satellital no me animo a decir. Yo que estoy en el área nuclear, digamos, me parece bastante difícil acordar con el operador que comparte esa información. Yo en febrero participé de un evento de inteligencia artificial en Viena, para la área nuclear, y un sector super conservador, digamos. Y es más o menos una analogía entre decir inteligencia artificial en el sector nuclear, es como decir tengo una bomba arriba de un avión, ¿no? Esa es la tentación un poco que me llevé. Y hay todo un mundo alrededor de la vigilancia, la vigilancia de computer vision, o la vigilancia alrededor del control de acceso a una instalación nuclear. Entonces, y ahí de vuelta, si vos como empresa provees el sistema completo o el reactor completo, tenés datos, siempre tenés datos. Y obviamente va a depender de si la industria es más o menos flexible para poder ingresar, pero me parece que hay que empezar a pensar un poco en esto, ¿no? ¿Vamos a querer construir productos? ¿Apoyamos los datos? Para mí la pregunta de fondo es esa. O sea, una cosa es querer y otra cosa es poder. Pero no porque no pueda, no voy a querer, o no voy a intentar. No, pero digamos, interpretás la sensibilidad de la área nuclear. Lo que quiero decir es que no sea una limitante, ¿no? Una cosa es que quieras y otra que no puedas, pero que no sea una limitante. No, no, está bien. Vos pueden decir por no haber, digamos, cómo puedo orientar el intentar acceder a eso. Y ahí está bueno. Porque obviamente ha pasado y está perdido. No sé quién, yo modero a quien respondo. No sé si te lo tengo que preguntar a vos, pero lo planteaste y no lo veo ni a Santy ni a Nacho por ahí están online. ¿Hay alguien que esté pensando en esta estrategia? Digo, en esas preguntitas que hiciste al final. ¿Sos consciente vos o tenés alguna información si hay alguien que esté pensando en...? Sí, hay una intención del lado de estrategia por parte de Santy. No, no hay atampaneales, digamos, pero hay una intención de tratar de entender cuál es la mejor forma de poder implementar esto al largo de los años. No, espera, primero estaba acá. Perdón. Quería preguntar si en la empresa hay algún proyecto de aplicar esto para sistema de calidad, para generación de documentos, para revisión de documentos. Nosotros el año pasado implementábamos un sistema RAC, de generación aumentada por recuperación, para la ingeniería del sistema de Saviamar. Donde básicamente nos entregamos el modelo en la máxima precisión, pero básicamente lo hicimos en un modelo cuantizado. Y lo hicimos sobre un set de documentos acotados, como para entender cuál era la viabilidad de esto y que no. Obviamente lo hicimos en un hardware local. El hardware nosotros también lo usábamos para entrenar modelos para otra cosa, con lo cual la disponibilización de ese hardware para que lo usen. Nada, tenía sus bemoles. Nosotros logramos hacer una aplicación web, posteada en un servidor, para que se pueda acceder a través de ahí. Y también lo integramos con Zulip. Cualquiera de la ingeniería del sistema de proyectos de Saviamar puede consultar documentación, el set de documentos que no se habían pasado a través del chat de Zulip. O un bot, que básicamente era la interfaz para poner el modelo, que era el YAMA-7B. Y eso básicamente quedó ahí. No pasó a producción. ¿Por qué? Porque empezó a pasar de que hay una cuestión bastante puntual con ese tipo de aplicación, que tiene que ver con cómo vos extraes la información de los documentos. La información importante de los documentos. Entonces te encontrás con que, si bien nosotros tenemos ciertos templates para crear documentos, yo genero los documentos de una forma, vos genera de otra, él genera de otra. Y si bien hay un template, dentro de eso hay una variabilidad. De repente hay tablas que no son tablas, sino que son imágenes. Entonces te encontrás con una particularidad en lo que es el preprocesamiento de los datos para poder responder preguntas que escapa a lo que es el modelo. Y vuelvo al ejemplo de que el código de Machine Learning termina siendo un pedacito muy muy chiquito de todo este proceso. Y bueno, nada, en ese momento generaba respuestas. En algunos casos eran muy buenas, en otros casos eran bastante malas. Y no le pudimos dar contilidad como para poder tener una segunda instancia de iteración. Pero había una intención de hacerlo sobre documentación técnica. En algunas de las preguntas que se han hecho, aprovecho para darles la oportunidad de hacer una charla. Y está bueno el foco más terrenal del automágico de la unidad de la universidad potencial. Ya atrás hay un montón de cosas y eso está... Mi pregunta es algo que no me quedó claro. Cuando mencionaste el ecosistema enorme que crece de manera exponencial, de herramientas a través de todo esto, lo que no me quedó claro de eso es si no estar al abancar de las herramientas te deja fuera, te bloquea de alguna manera, o es un tema de... una vez que van creciendo esas herramientas y te quedas con un centro de herramientas particulares, te queda deprecado y no puedes seguir laburando, tenés que sí o sí estar. Puede quedar deprecado, puede que no, pero lo que te puede pasar en realidad es que de repente vos no sepas que hay una herramienta mejor o distinta para poder abordar ese problema. Entonces en algún punto vos tenés que estar a la vanguardia tratando de entender cuáles son las herramientas disponibles, evaluarlas, someterlas a revisión como para entender cuál es el pro y el contra de determinada herramienta por sobre otra, si es paga, si no es paga, si es libre, si no es libre, si es web, si la tengo que escostear localmente. Entonces, ¿podés usar un set de herramientas acotado? Sí, totalmente. Podemos usar Windows 95 si queremos acá. Yo no sé si es un dato demasiado específico, pero vos ahí mostrabas los dos modelos en el caso de la opción de tu propia plataforma. Bueno, ahí una aproximación de inversión. En términos de demanda de energía, ¿de qué orden? Bueno, la pregunta digamos, porque no es menor, las placas están del orden de, a ver, que haces la cuenta, de cuánto consume el server completo, pero estaban alrededor de 1200W el server. Así que hay que hacer la cuenta del público de eso cuando está entrenando para tener realmente dimensión de lo que implica el término de energía. De todas formas, aprovecho y sobre eso digo de que cuando vos lo tenés localmente, vos tenés que asegurarte de que eso quede disponible. Si se cae, ¿quién lo levanta? ¿Cómo lo levanta? ¿Implica tenerlo prendido? ¿Lo puedo tener prendido ocioso? Y eso hace hacer un montón de preguntas que tienen que ver con el mantenimiento, la disponibilidad de eso, que no son menores. Y ahí entre el juego, ¿qué es más importante para vos? ¿La sensibilidad o la privacidad de los datos o la disponibilidad de tener un herramienta? Para mí no es una por sobre la otra, es una relación de compromiso que dependerá del tipo de proyecto que se marque. Y después, otra pregunta era, las dos opciones están basadas en tecnología de NVIDIA. Entiendo que es medio el dueño del Market Share. ¿Hay alguna otra tecnología o si uno se decidiera por una tecnología de ese tipo, qué proyección tiene? Buenísimo la pregunta y me das pie para otra cosa. Hoy todo el stack tecnológico y de software alrededor de estos generalmente termina utilizando NVIDIA como backend. Hay obviamente intenciones para poder hacer lo mismo con AMD, pero los desempeños, la performance del hardware debajo no es la misma. Así que todo el stack tecnológico está apoyado en las placas de NVIDIA. Y ahí hago una referencia de que está Clementina, que está en el Servicio Métro Lógico Nacional. Y Clementina tomó la opción de ir por placa GPV de Intel. Con lo cual, todo el stack tecnológico todavía se está tratando de adaptar a ver si puede soportar ese backend de compra. Entonces ahí también al momento de invertir está bueno la pregunta porque tenés que pensar en qué track de software voy a querer construir. ¿Por qué eso va a tener un impacto directo en el cierre que vas a terminar de comprar? Así que está buenísimo la pregunta. Cuando hablamos de entrenar, en un proceso de apertura, un ratito, estamos hablando de un 11. Por ejemplo, el 17 de documentos. El 17 de documentos nosotros, está buena la pregunta también, no entrenamos el modelo, usamos un modelo entrenado. Básicamente la técnica está de recuperación, generación aumentada por recuperación. Lo que haces es recuperar información en función de la consulta que haces. Entonces de repente es, ¿A dónde se conecta el pin L14? Con esa consulta, vos accedes a una base de datos vectorial, donde básicamente por distancia se termina determinando cuáles son las referencias de la documentación que tiene en el pin L14 y con qué se conecta. Y eso se pone en el prompt de la consulta, donde básicamente vos ponés la consulta y toda la información que le diste de contexto. Entonces no tenés que reentrenarlo. La más que está en cómo vos extraes esta información es la agregás al prompt. Y esto termina siendo prompt engineering. Hay toda una tendencia alrededor de eso, donde vos tranquilamente podés decirle o mejorar la forma en la que consultas. Obviamente si lo haces en la nube, la privacidad de los datos y te expusiste lo que se dice. Eso porque te afana en el prompt. Eso porque, no sé si suelen usar una charla que te diga, pero si. Por defecto, ChatGPT te dice que tus datos van a ser utilizados en la próxima ronda de entrenamiento. Y te dice, in your data for everyone. ¿Vos podés destilar eso? Sí, por eso destilás eso y no sabés si no le vendiste el arma al diablo igual. Tenía una pregunta un poco menos técnica. Yo he estado escuchando de inteligencia artificial hace muchísimos años, hace 20 años o más. Hay una película hace 15 años o algo así. Pero de repente, hace uno o dos años explotó. Y ahora mi abuela sabe lo que es inteligencia artificial. ¿A qué se vio eso? Yo lo único que veo es que apareció ChatGPT y de alguna forma bajó al mundo y todos podemos acceder a eso. ¿Es eso lo que pasó que hizo? ¿Explote o hay otras cosas atrás? Era un poco el slide en comienzo, ¿no? La aparición de ChatGPT te cambió la forma en la que vos interactuas con la tecnología. ¿Pero es sólo eso? Es eso más el avance en el hardware de abajo. Porque de repente empezaron a salir placas muchísimo más ojudas, digamos, que soportan todas las operaciones matemáticas que esos grandes modelos de lenguaje terminan teniendo. Entonces es un conjunto de que apareció esta tecnología, pero en realidad el hardware la viene acompañando. Y los transformers. Y obviamente, sí, totalmente. Los transformers que básicamente... Las nuevas arquitecturas. Había evolucionado las arquitecturas de redes neuronales. En 2017 salió la arquitectura Transformer que básicamente la utilizan los LLM, donde básicamente han optimizado la forma en la cual se puede computar este tipo de cosas. Que básicamente por debajo no terminan siendo más que multiplicaciones matriciales, ¿no? Pero optimizadas incluso para el tipo de hardware que se les deja. Ahora, porque se me ocurrió otra cosita. No, en... Siendo... Se me cruzó una aplicación. En... A ver si lo me echamos. ¿Cuánto... Se puede asegurar, digamos, lo cerrado de... Quiero decir, como producto. O sea, aprovechar estas herramientas. ¿Se puede asegurar que son cerradas y que...? Totalmente. O sea, ¿puedes decir para hacer un producto...? No, cerrado en el sentido de que no puede ser... Chupado. Bueno, ahí hay todo uno. Y eso digamos, justamente. Sí, hay como dos líneas. Por ejemplo, OpenAI, que es ChatKPT. Vos no tenés abierta la arquitectura del modelo. Que por debajo responden las consultas que vos le haces al ChatKPT. Ahora, lo que se llama 3.70B. Que es el que yo puse como ejemplo, que es el de Facebook. Ese es abierto. Vos podés ver cuál es la arquitectura. Tenés los modelos abiertos. Son dos líneas totalmente diferentes, ¿no? Ahora, cuando vos te orientás más a cuestiones que tienen que ver con... productos que brilla una empresa, probablemente sean más del estilo de OpenAI. La propiedad intelectual que quede a vos, digamos. Entonces vos terminás entregando un binario que tiene determinadas entradas y determinadas áreas. Tendrás que tener una caja en el que lo haces. Así que sí, lo podés hacer por la misma. Sí, gracias. Bueno, no tengo nada muy bueno a la charla, gracias. Una consulta. Hoy hablaste de los L. O sea, de la inteligencia artificial, bueno, basada en LL. Modelo estos largos, no grandes. Hay proyectos que se manejan con redes neuronales que no implican tanta infraestructura, ¿no? Que son... Es otra escala, ¿no es cierto? Es un tema que reconoce el escáner, digamos, de letras manuales. Totalmente. No necesitaste esa infraestructura. Puse el ejemplo de... Otra diferencia, claro, así. Exactamente. Puse el ejemplo del LLM porque básicamente es lo que todos consumimos diarios, digamos. Entonces automáticamente uno piensa, cheque, quiero consultar documentación técnica. Bueno, ¿quierés hacerlo? Porque no me mojo de pavo, digamos, ¿no? No es que porque tenés charla. Que pete esto acá adentro es igual de transparente. Básicamente venía por ese lado el ejefe, ¿no? Pero está claro de que el tamaño y la dimensión de los modelos dependen de la problemática, ¿sí? La cantidad de inputs, de datos, de asociación. El tamaño de un modelo y el hardware que necesitás es específico de la problemática incluso que quieras reservar. ¿Sí, Chuchu? Entiendo, perdón. También se está nada ahí. Leía también que se están desarrollando chips, o sea, están tratando de meter en las computadoras personales, digamos, soporte de hardware. Hace poco, por la semana pasada, salió un Basic que tiene una performance superior a las últimas placas que Nvidia presentó en el GTC en 2004. Para un modelo, el llamado 270B que es de última generación. Y vos decís, bueno, ¿y cuánto? Lo que no hay carajo en términos de performance y de, como se llama, capacidad de computación. Con lo cual, sí, de vuelta, también hay un mundo que está tratando de jugarle Nvidia, ¿no? No cualquiera le puede jugar Nvidia. Entonces, nada. Los Intel Core nuevos vienen con una NPU integrada. Sí. Y los procesadores de celular también. No, al nivel de esto. La memoria, capacidad de almacenamiento, eso ya excede cualquier cosa. No, pero ya, pero hay muchas iniciativas de poder inferir on the Edge. O sea, que vos en el procesador que tenés en tu celular o en tu computadora, no vas a hacer un entrenamiento de una red. Pero en el LME, vos sí, vas a hacer un montón de, van a correr un montón de redes chicas. Y una vez que vos entrenaste el modelo y lo querés desplegar on the Edge, tenés otro proceso de tratar de llevar eso que entrenaste. Que por ahí no necesariamente tiene que estar ajustado al hardware con el cual vos vas a desplegar. Eso lo podés hacer después. De hecho, es muy común, digamos, de que se hayan liberado los grandes modelos del lenguaje y después la gente haya hecho cuantización post entrenamiento para justamente poder utilizarlo en recursos computacionales mortales. Un combo, un combo. Entonces, es, obviamente tiene un costo, no? Básicamente perdiste en la precisión en la cual el modelo te va a contestar. Pero la pudiste correr en tu máquina. Hay preguntas. Sí. La chava que te explica para medir la confiabilidad a nivel producto. No sé si la confiabilidad está la preguntaba en términos de un producto de software o por ahí si puede ampliar la pregunta como para entenderla mejor. Si, ¿querés la persona que se llama a Flores a invitar el interés por aquí? Entiendo que es a Ale Flores igual. Alejandro. Hola, hola. Me escuchan? No, digamos, se habla también de inteligencia artificial confiable. Quiere decir que los productos que yo saco de la inteligencia artificial sean precisos y responden a la realidad, digamos. O sea, todos convivimos con los bots y nos contesta cualquier verdura, ¿no? Entonces, bueno, la confiabilidad, o sea, la fiabilidad de un algoritmo de detectar determinadas cosas. A eso me refería. Perfecto. Hay dos cosas, digamos, ¿no? Estos grandes modelos de lenguaje han sido entrenados con información que vive en la nube, un millón, trillones de datos. Y básicamente todo depende de cómo pregunte, ¿no? O sea, si vos haces una pregunta muy general. El modelo no fue entrenado con información súper específica, ¿no? Sino que ha sido entrenado con muy grande información. Yo me refería sobre todo a la fiabilidad, por ejemplo, de un algoritmo de detección, ¿no? Algo tan sencillo como detectar una placa, por ejemplo, una placa. Bueno, ahí a veces puede llegar a fallar esa... Totalmente. Pero todo eso va a depender de tu proceso de etiquetado, ¿sí? Vos en tu etiquetado vas a terminar definiendo qué es la verdad absoluta, ¿sí? Y vas a tener que armarte un set de entrenamiento con datos etiquetados por un experto de dominio, ¿sí? Donde básicamente te digan que lo que estás viendo... Bueno, Adrián lo estaba moviendo. Es, no sé, un determinado componente en una determinada placa, ¿sí? Y por ahí está orientado de una forma y por ahí está orientado de otra. Y vos vas a tener que etiquetar las dos orientaciones, ¿sí? Porque básicamente le estás dando más información al modelo para que aprenda qué es un mismo componente rotado. Entonces, la confiabilidad va a depender mucho de los datos, ¿sí? Y de cómo vos, en este caso, por ejemplo, que pusiste, cómo los etiquetes, ¿sí? Por ahí le puedes mencionar que hay métricas durante el entrenamiento. Obviamente, hay métricas de entrenamiento. Y no deja de ser un modelo estadístico. Totalmente. Hay métricas de falsos positivos, falsos negativos, en tasas, ¿sí? Y después esas métricas hay que terminar pasándolas a lo que terminan siendo las métricas del negocio, ¿no? Entonces, por ahí yo te digo, che, tenés un 90% de falsos negativos. OK. Y eso a nivel negocio, ¿te importa o no te importa? Vos probablemente quieras saber si lo que está viendo es un capacitor o no. Y ahí se termina, ¿no? Entonces, hay que poder trasladar las métricas del modelo y alinearla con lo que espera que vea el negocio. Pero al final te cuentas, todo termina siendo una cuestión estadística de fondo, ¿no? Lo más rigoso... Lo más rigoso ahí es cuando vos haces un contrato con un cliente y le decís que... Lo de probabilidad siempre vas a detectar lo que él está buscando. Y en realidad tenés que decir una cifra estadística, ¿no? De probabilidad. Sí, pero en realidad está bueno. Pero es lo mismo que con la clarificabilidad o la disponibilidad, digamos. Es una cifra estadística. Sí, pero el problema es ponerlo en un contrato, eso. Pero lo ponemos. Ponerlo a priori. No creo que sea fácil ponerlo a priori. Pero ahí está bueno entender si la tasa que necesita el cliente realmente es la tasa de falsos positivos. O sea, es lo que necesita el cliente. Capaz que no necesita esa métrica de desempeño a ese nivel, ¿no? Es lo que necesita si tiene que detectar o no y en qué condiciones. Entonces podés expresar lo mismo de una manera diferente. Pero bueno, comparto que es bastante parecido a tratar de ponerla con claridad en un sensor que básicamente tenés que asegurar su operación continua durante cinco años en órbita, digamos, donde no lo podés intervenir. ¿Me des un ensayo de tres o de calificación que te permite una confiabilidad? Igual todo depende de los datos, ¿no? Entonces, por ahí me pedís una métrica de falsos negativos muy, muy baja. Después depende de cómo son los datos que me das de entrada. Si son los datos que me hacen una cagada, y yo no. No voy a hacer más que, digamos, ¿no? Pero también en un proceso continuo de aprendizaje, ¿no? O sea, vos vas a aprender... Totalmente, pero si de repente, tu fuente de datos no son confiable, digamos, vos magia, no vas a poder hacer. Si de repente, vamos al caso de imágenes. Si de repente vos me das imágenes que están ocluidas, o, como es, el lente está transpirado, o está empañado, lo que fuese, difícil poder extraer información desde ahí, ¿no? Entonces, hay que tratar de prever otra fuente de datos que complemente a esa. Entonces, por ahí, ese requerimiento lo terminás atacando a nivel sistema, con redundancias y demás cuestiones. Obvio. Y vas a tener que pensar una forma distinta de redactar contratos, obviamente, sí. Danilo Chidi dice que comente sobre los sistemas laborales. Ah, ok. Nosotros el año pasado, sí, desarrollamos una prueba de concepto para tratar de predecir la profundidad de los lechos de aguas residuales, la una de aguas residuales, con imágenes satelitales tanto de Sentinel-2 como de Google como de Lanza. Y básicamente eso consistía en que se habían levantado puntos de matematría de esas lagunas, donde se había medido con precisión cuál era la profundidad, con lo cual había un renovamiento de lo que vendría a ser el lecho de cada una de esas lagunas, y después a través de imágenes satelitales y tratando de utilizar básicamente la información multiespectral de esas imágenes, tratar de inferir si podíamos predecir la profundidad de una laguna que el modelo no había visto. Y lo utilizamos para el último lagunar de Río Tercero, del tren que la auquen, y sinceramente funcionó muy bien. Obviamente era una cuestión estadística. Nos pasaba de que, por ejemplo, dependía de la profundidad de la laguna, pero el otro sistema lagunar no lo tenía para el otro. Entonces si entrenabas con las lagunas que iban para un lado, el modelo había sido entrenado con un perfil de profundidad en una dirección particular, con lo cual cuando uno le daba un perfil diferente, el tipo no lo había visto y el desempeño en esa condición era malo. Entonces ahí entra el proceso continuo de tener que reentrenar, con, por ejemplo, aumentando datos y de la profundidad de la laguna. ¿Cuánto te llevó ese entrenamiento? El entrenamiento no era solo eso, porque los modelos eran chicos y los datos los teníamos disponibles. Y dando la campaña de datos, si yo me voy a decir, ¿cuánto te llevó entrenar eso? Bien. Y entrenamos dos modelos diferentes. Y en el caso de los modelos, los modelos que teníamos en el momento de entrenar, los modelos que teníamos en el momento de entrenar, los modelos que teníamos en el momento de entrenar, entrenamos dos modelos diferentes. Y entrenamos una autoencoda, y entre, perdón, entrenamos una red densa, de la autoencoda es una palabra para la telemetría, entrenamos una red densa y entrenamos un árbol de esos. ¿Una? Árbol de esos. Cuando finalizaste el entrenamiento, el periodo de entrenamiento, la larga esa producción, digamos, ¿no se vuelve a realimentar el...? Cuando terminaste el periodo de entrenamiento, separaste tu set de datos de entrenamiento, de evaluación y de test. Con tu set de test, básicamente sacás las métricas de desempeño de tu modelo, para datos que en teoría no vio, y te explicás si eso satisface o no las métricas que crees. Si eso satisface, lo desplegas. ¿Y el mantenimiento? Una vez que lo desplegas, tenés que monitorear si seguís manteniendo o no esa métrica de desempeño. Y ahí hay un filtro a continuo. Y ahí tienes que ver todo lo que tiene que ver con la política de reentrenamiento. No cumplo con la métrica que tenía. ¿Qué hago? ¿Baja por de tanto valor? ¿Considere una hipteresis? ¿Cada cuánto reentreno? ¿Reentreno siempre y solo despliego cuando cae? Es un mundo en sí mismo. Tenés los patrones iniciales, o en este caso, con la profundidad de algunas laburas conocidas, volvés a reentrenar y volvés a medir los mismos o similares patrones. Exactamente. En ese momento, lo único que teníamos era los datos de batimetría y teníamos la posibilidad de que los datos se pudieran ser de la misma manera en el mismo instante de tiempo. ¿Por qué? Porque de repente no era lo mismo el sol y por la latitud en la que estaban las lagunas en invierno que lo mismo en verano. El color que venía en las imágenes era distinto. Entonces, ahí tenés variedad. Pero al incorporar variedad, al incorporar volumen de datos, al incorporar volumen de datos, prolongás el entrenamiento. Entonces, de vuelta, caemos siempre en lo mismo. Una vez que vos traes los datos, los curaste y después hay toda una cuestión que es no menor alrededor de toda la gestión. Displacios, lucradadas, los dos de almacenos. Hay una pregunta que en el chat, hay otras más, pero bueno, pero esta es País y Terrande, de las que vimos. Pregunta Ricardo Loga. Las empresas que venden los mismos productos que vendemos nosotros, ¿sabes, hay alguna mezcora que hayan conseguido de esta tecnología? Sí, hay varias empresas que ofrecen o potencian su producto con inteligencia o al menos dicen que lo hacen. Pero sí, para ser específicos, en los casos de los radares, por ejemplo, ya hay sensores que no solo detectan, sino que además clasifican lo que han detectado. Bueno, no hablo más. ¿Cómo se inserta el tiempo de desarrollo de la IA en el ciclo de vida de nuestros productos? ¿Los tiempos de desarrollo de los productos son el tiempo de tecnología en clientes? Es buena esa, porque en algún punto lo que nos termina pasando también, es que nosotros necesitamos datos que sean estables o confiables. Entonces, ¿qué nos pasa a nosotros en el ciclo de desarrollo de los productos de IMAG? Hasta que nosotros logramos hacer la puesta en marcha de nuestro producto, voy a separar satelital de lo que es el gobierno. Es una etapa de configuración, calibración, hasta poder dejar los datos ajustados al desempeño que se firmó por contrato. Entonces, una vez que vos tenés eso, en teoría deberías poder empezar a transcurrir este ciclo. Lo cierto es que lo puedes hacer antes, nada te lo impide. El tema es que, para hacerlo antes, probablemente tengas que usar datos de sensores anteriores. Lo cual no es un problema. Con lo cual, ¿cómo se inserta? Bueno, nada, se inserta como se podría insertar también como en el esquema actual de IMAG, que quizás convenga revisar, como un paquete de trabajo más en un proyecto particular.