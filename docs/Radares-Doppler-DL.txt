 Dale, arrancó. Muy bien, gracias. La pantalla de la presentación se ve, ¿no? Se ve perfecto. Bien, OK. Bueno, esta es la segunda parte de la charlita, o bien la charla inicial que me había pedido Jorge, que se trata sobre clasificación de señales Doppler de ECOs Radar usando Deep Learning. En este caso son, particularmente, redes convolucionales. La primera parte fue cómo hacer una introducción para poder hablar un poquito más derecho y no ahondar mucho en algunos temas más básicos. Así que, bueno, esta presentación, la intención un poco es contar el proyecto que realicé, digamos, la tesis que realicé en el contexto de una maestría en ingeniería en el Balseiro para que aplicaba a un problema del área de radares. En este caso de radares terrestres para hacer clasificación de blancos, bien. En este caso, blancos terrestres, puntualmente. Así que, bueno, nada, la idea es contarle un poco la problemática, cómo se usó Inteligencia Artificial para resolver el problema y mostrarle el camino, ¿no? El workflow, un poco lo que habíamos charlado en la presentación anterior. Y luego terminan los resultados y hablar un poquito de los trabajos futuros. Así que, bueno, igualmente siéntanse libres de interrumpirme si por ahí surge alguna preguntita puntual. Bien, una breve introducción, entonces, a la aplicación un poco para los que no estén metidos en el mundillo de los radares. En cuanto a radares, digamos, Inbap más que nada trata básicamente todos estos tipos de radares en sus productos. Podemos tener radares que tienen aplicación de vigilancia, o sea, vigilancia del espacio aéreo en este caso, que se pueden realizar de manera 2D, o sea, en una representación de tipo mapa, ¿no? De las detecciones que puedan realizar los blancos o 3D en donde se agrega la información de la elevación o altura de esos blancos. También, obviamente, en lo que es vigilancia se hace una estimación de la distancia a la que se encuentra el blanco o los blancos del radar. Y también es útil hacer una estimación de la velocidad. En este caso, el radar puede medir una velocidad radial, o sea, respecto al radar o a la posición del radar en sí. Luego, bueno, hay otras aplicaciones como Asset Tracking, o sea, construir la trayectoria de los blancos que se están moviendo en el espacio escaneado, vigilado. Meteorología, ¿no? Los radares meteorológicos y demás. Para construcción de imágenes a partir de ondas electromagnéticas, fuera del espectro visible, como podrían ser los radares SAR, como SAOCOM, ¿no? O, bueno, otras aplicaciones como Inverse SAR, que no vamos a entrar un poco en detalle ahí, así que... Pero, bueno, una de las aplicaciones que estaba en interés y que hemos desarrollado poco dentro de la empresa es clasificación, ¿bien? O sea, el radar puede detectar objetos, pero no sabe bien qué es, ¿bien? O sea, se pueden utilizar un montón de información que hay alrededor de esa detección para hacer una estimación de lo que es, pero en definitiva, la clasificación en sí es el proceso que a partir de esa información me va a entregar a mi usuario la clase a la que pertenece ese blanco, ¿no? O al menos la probabilidad de pertenencia de ese blanco a un conjunto de clases. Bueno, y después el resto es identificación, que básicamente va más allá de la clasificación, es poder decir cuál avión es más allá de qué tipo de avión es, ¿no? O sea, poder asignar un identificador en otro contexto. Bien, el efecto Doppler es el que vamos a usar en este caso para hacer la clasificación. O sea, vamos a intentar recuperar información del Doppler de cada uno de esos objetos en el espacio para poder inferir de qué se trata, ¿no? Bueno, ¿qué es un poquito el... Hago una breve introducción al efecto Doppler. El efecto Doppler, no sé si recuerdan, es una alteración de la frecuencia percibida por un observador de una fuente que se encuentra en movimiento. O sea, ¿se acuerdan, no? Si un auto viene tocando la bocina y se nos viene acercando o se viene alejando, la frecuencia con la que percibimos ese sonido cambia en función de la velocidad en la que se estaría moviendo ese auto. Bueno, con los radares, si los objetos a los cuales reflejan las ondas que emite el radar, pasa lo mismo, ¿bien? Entonces, si suponemos que un radar emitió una onda electromagnética que viajó en el espacio y le pega a esta fuente que está acá, ¿bien? Esa fuente va a reflejar la onda y si esa fuente está quieta respecto al radar, que en este caso sería cualquiera de estos dos observadores, bueno, la frecuencia que emitió el radar, la onda electromagnética que emitió el radar, la frecuencia que va a recibir el radar como eco, va a ser la misma si está quieto. Entonces, si la posición relativa entre el blanco, que sería la fuente acá y el observador, que sería el radar, no cambia. Obviamente esto también vale si el radar está en movimiento y viceversa. Ahora, ¿qué pasa cuando la fuente se mueve alejándose o acercándose al radar? Bueno, la frecuencia percibida en el eco va a cambiar. Entonces, si la fuente se va acercando al radar, como sería el caso en este ejemplo con el observador de la derecha, la frecuencia que vamos a percibir en el eco recibido va a ser mayor a la transmitida, ¿bien? En cambio, si el blanco se va alejando del radar, que sería el observador de la izquierda, el radar siendo el observador de la izquierda, la frecuencia percibida va a ser menor. Bueno, esto se resume en esta formulita de aquí en la ecuación, en donde la frecuencia de recepción se ve afectada, digamos, por un término que depende de la velocidad del blanco en este caso y de la frecuencia transmitida. Si la velocidad es 0, ambas frecuencias son iguales. Bien, si desglosamos un poquito esto y hacemos la diferencia de la frecuencia recibida de la transmitida es lo que nosotros vamos a llamar la frecuencia doble, ¿bien? Entonces, si no hay movimiento relativo o aparente entre la fuente y el observador, la frecuencia doble es C. La podemos expresar de esta manera y si ampliamos un poquito más, digamos, los casos, podemos decir que la velocidad no necesariamente siempre es radial, ¿no? sino que podría tener un ángulo respecto al radar. Bueno, y ahí se aplicaría de alguna forma esto, ¿no? Donde este término de acá es el valor absoluto de la velocidad o la magnitud de la velocidad y este phi de acá sería el ángulo respecto al radar. Esto, básicamente, un caso particular sería si la fuente se encuentra girando alrededor del radar sin cambiar el radio de giro, o el radio en el que se encuentra el radar en el centro, la frecuencia doble sería 0, ¿bien? más allá de que el objeto estuviera en movimiento. Eso porque básicamente la distancia al radar no está cambiando. Bien, acá hay que introducir un término nuevo que se llama micro-doppler, que eso quizás ya no lo escucharon mucho, que básicamente lo que trata el micro-doppler es de decir, bueno, los objetos a los que en general yo le voy a apuntar con mi radar y van a reflejar mi sonda electromagnética que yo transmito, no son cuerpos rígidos, ¿bien? no hay una esfera flotando en general, no es una esfera flotando en el espacio, sino básicamente a lo que le pegamos, ¿no? un helicóptero, un auto, una persona, un animal, tiene partes móviles, entonces cada una de esas partes móviles, ¿bien? es como si fuera un elemento adicional, digamos, como decirlo, separado, que va a tener sus reflexiones Doppler particulares, entonces cuando yo arme el conjunto de reflexiones de todas esas partes de mi objeto, voy a obtener lo que se llama firma Doppler, que es ese conjunto de señales que vuelven de cada una de esas partecitas, del objeto al que le estoy pegando con mi onda, va a constituir una señal que va a tener muchas componentes Doppler, supongamos el caso de un helicóptero que está quieto, pero con su zaspa girando, bueno, cada una de esas zaspas en función de la posición en la que esté, cuando la onda llega y golpea a cada una de esas zaspas, me va a devolver ecos distintos, ¿no? porque la velocidad de cada una de esas zaspas en un momento determinado va a ser distinta, bueno, entonces yo lo que voy a ver desde el radar en realidad es una combinación de todas esas señales, que es un poco lo que vemos en esta expresión que está acá, la señal micro Doppler que yo voy a ver a lo largo del tiempo, está expresada como un montón de variaciones de fase de la onda que vuelve, que cambian el tiempo, que la podemos expresar como la sumatoria de todas esas señales que voltean cada una de esas partes que se están moviendo, ahora, no es necesario tener un radar que tenga una resolución tan alta que me permita discriminar cada una de esas partes para obtener esa información, sino que simplemente yo haciendo un procesamiento de la señal, más allá de que no tenga la resolución necesaria, yo puedo extraer cada una de esas componentes, que esto lo podemos expresar de manera general como este último término, como la señal que me retorna va a tener una fase que va a depender de la ubicación del blanco en general, como un todo, más una sumatoria de componentes que van a depender a la distancia que se encuentren del radar y cómo esa distancia cambia en el tiempo, más un peso, porque no todas las partes van a colaborar de igual manera o con igual magnitud, y una fase también que dependerá de cada una de esas cosas, de cada una de esas partes. Entonces, bueno, al fin y al cabo, lo que yo les voy a mostrar es que cada una de esas muestras, en realidad es una señal que está compuesta por un montón de componentes, entonces, un parámetro importante que yo tengo que definir en mi radar es lo que denominamos la frecuencia de muestreo o PRF, period repetition frequency, básicamente en un radar pulsado yo voy a ir emitiendo pulsos, ahora igual vamos a un detalle de eso, vamos a seguir emitiendo pulsos, y por cada pulso yo voy a ir recuperando información, es como sacar fotos básicamente a lo largo del tiempo, cada una de esas fotos me va a entregar cada una de estas componentes de fase que yo les mencionaba. Entonces, al fin y al cabo, cuando yo saco más fotos en un determinado tiempo, como si muestreara una frecuencia mayor, bueno, yo puedo tener información de frecuencias mayores, si yo no saco fotos a una tasa alta, yo me voy a perder información que está más allá de esa frecuencia, aplica lo mismo que el criterio de muestreo de Nightwish. Bien, entonces, lo que yo voy a poder construir una vez que tengo todas esas fotos de alguna forma es lo que se llama un espectrograma, un espectrograma, en este caso acá hay un ejemplo de una de las muestras, lo que voy a ver acá en el eje horizontal va a estar el tiempo, y en el eje vertical la velocidad o la frecuencia doble, la que hablábamos al principio, porque hay una equivalencia entre velocidad y frecuencia que mencionamos al principio. Entonces, ¿qué se ve acá, por ejemplo? Este espectrograma corresponde a una persona caminando, vista por un radar pulsado. ¿Qué es lo que se ve acá entonces? Fíjense que hay como un componente que nosotros visualmente podemos discriminar, que fíjense que va como cambiando a lo largo del tiempo, o sea, cambia su velocidad a lo largo del tiempo, y esas componentes precisamente corresponden a las distintas partes del cuerpo, por ejemplo, el brazo, que sería esto que se ve aquí, esta como oscilación que se ve aquí, a medida que va caminando, el brazo se va acercando y alejando el radar relativamente, así que muestra estas componentes. Luego, por ejemplo, vemos la pierna o el pie, que tiene una componente más alta de velocidad, porque la excursión radial es más grande, y vemos el torso que tiene mayor intensidad, porque es un cuerpo más grande, pero tengo menor fluctuación de esa velocidad. Ahora, otra cosa que se puede ver acá es que como que hay una media de la velocidad, no sé si lo llegan a notar, que estaría más o menos por acá. Bueno, esa media sería la velocidad media de la persona desplazándose respecto al radar, y todas las otras componentes están alrededor de esa media. Bueno, en definitiva, esto es lo que voy a usar luego para decir, bueno, ok, voy a sacar la firma Doppler de cada uno de esos blancos que yo detecto con mi radar, y voy a hacer algún procesamiento para decir si esa firma se corresponde a una persona, a un animal, a un auto, etc. Algo que quería mencionar muy al pasar es esta PRF, que es la frecuencia a la que muestreo el espacio, la frecuencia con la que emito los pulsos en el radar, también me define la velocidad máxima que puedo discriminar en mi espectro grama, eso porque tenía que ver de nuevo con el muestreo que estoy haciendo. Entonces, bueno, es un parámetro crítico, porque esta PRF me está definiendo la velocidad y también me está definiendo la resolución con la que yo miro todas estas componentes en frecuencia. Bien. Bueno, esto simplemente es un diagrama muy simplificado del radar que se utiliza para hacer la adquisición de las muestras. Tiene, digamos, genera sus osciladores, esos osciladores se utilizan para generar las formas de onda que voy a transmitir, básicamente son los pulsos que voy a transmitir en radar de frecuencia, se amplifican y se mandan a una antena. Bueno, obviamente las ondas electromagnéticas viajan en el espacio y empiezan a rebotar en todos los objetos que va encontrando a medida que va viajando, y esas recepciones se amplifican con un amplificador de bajo ruido, luego se convierten en frecuencias para bajarlo, no trabajar en radiofrecuencias, se digitalizan, se conviertan del mundo analógico al mundo digital y luego se mandan a un procesador de señales. Acá en este bloque es donde hacemos toda la magia de convertir las muestras digitales a poder emitir un reporte de decir, bueno, esto es una persona, esto es una auto. Y luego estaría la consola para desplegar los resultados, para mostrar los resultados. Bien, antes de continuar, un breve repaso de qué es lo que obtengo yo con mi formación, esto es básicamente lo que serían mis datos de entrada de alguna forma, en el mundo del procesamiento de radar, donde cada uno de, digamos, esto es lo que se llama cubo de datos, cada una de estas dimensiones se corresponden a distintas cosas. Esta dimensión de acá, que se llama RxChannel, se corresponde con cada antena que yo estoy digitalizando, digamos, independientemente de mi radar. Por ejemplo, mi radar tiene varias filas, bueno, yo podría usar un conversor analógico digital para cada una de esas filas, bueno, tendría tantos pisos acá, este N que estaría acá puesto. Esta dimensión, perdón, que está acá, es lo que se denomina Fast Time o tiempo rápido, y sería cada una de las muestras que iría entregando cada uno de mis ADC. Bien, si yo me paro en la fila 0, o en la fila N-1, para que me quede más fácil, esta sería la primer muestra que yo recupero de mi ECO, la segunda, la tercera, y así hasta la muestra número L, o L-1, si el índice es 0. Entonces, cada una de estas, de alguna forma, sería mi foto, y esto, que es Slow Time, sería mi video, de alguna forma, sería la secuencia de fotos que voy haciendo pulso a pulso. Entonces, a lo largo del radar, va emitiendo un pulso y escuchando un ECO, construye una feta de esto, aquí, en el segundo pulso construye la otra feta y así sucesivamente, hasta armar la película. Bueno, eso básicamente no lo puedo usar así directamente para construir el espectrograma, lo primero que se hace es construir lo que se llaman beams o aces, entonces, para eso se usa un proceso que se llama beamforming, en donde para, por ejemplo, en un pulso M, agarro todas las muestras en un instante de todos los elementos de la antena, y con esta columna naranja, construyo una de estos cubitos de acá azules, con las mismas muestras aplicando otros pesos, construyo otro y así sucesivamente. Y lo que me permite a mí es, con mi radar, apuntar a distintos sectores de espacio, eso por ahora no tiene peso en lo que vamos a explicar, este proceso para nosotros sería transparente, lo está haciendo el procesador radar, y nos está entregando básicamente información de un lugar del espacio, a una determinada elevación, a un determinado azimut, me está entregando información. Bien, entonces básicamente este cubo lo transformamos a otro cubo, pero en vez de tener canales, tengo aces. Bien, ¿qué hace el procesador radar? entonces agarra, por ejemplo, uno de esos aces, que se correspondería con un apuntamiento particular, el radar apunta a un lugar particular del espacio, bien, entonces se queda con las muestras en tiempo rápido y las muestras en tiempo lento para ese az, y ¿qué hace ahora con todas esas muestras? Bueno, si las recorremos así a las muestras, ese corte ahora horizontal es el que está mostrándose acá. Si yo las recorro acá en esta dimensión, que es el verde, que está acá, yo voy a poder hacer detecciones de los blancos. Si yo acá tengo un retorno alto de la señal, quiere decir que hay un blanco a un determinado rango. Fíjense que como esta es la primera muestra, es la muestra que llegó más rápido, o sea, son los secos más cercanos, serían los objetos que están más cerca al rado, y a medida que me voy alejando las muestras, serían objetos que están más lejos al rado. Entonces, bueno, acá detecto mis objetos, y una vez que detecto mi objeto, yo me puedo parar en esa celda y recorrerla en el tiempo lento. Entonces, así puedo ver a esa celda cómo fue cambiando la fase de ese objeto, o la frecuencia doble de la que hablábamos. Yo la puedo recuperar haciendo un procesamiento en esta dimensión. Si tengo varios blancos a lo largo de esta dimensión, puedo hacer lo mismo para cada uno de ellos. Entonces, de esa forma, yo extraigo la señal doble. Entonces, bueno, a partir de ahí empezamos a trabajar. ¿Cuáles son los alcances de la tesis? Bueno, la idea era implementar un clasificado de blancos, que solo se restringe a blancos terrestres. La idea era utilizar técnicas de, digamos, machine learning y especialmente de deep learning, porque ya había cosas hechas, al menos en la bibliografía o en lo que yo estuve estudiando un poquito al principio. Hay muchas cosas hechas de esta, no hay muchos datos disponibles, lo cual es un problema, pero en general se hacen con técnicas más convencionales, ¿no? O sea, tipo, se hacen identificación de patrones, o se calculan coeficientes, o se hacen reducciones de dimensionalidad, o se viene en distribución y con eso se calculan distancias entre distribuciones. Bueno, técnicas como más clásicas. Entonces, bueno, la idea era explorar este lado nuevo de aprendizaje profundo y más que nada usando redes convolucionales. La idea era usar un dato, digamos, un conjunto de datos reales, lo cual fue muy complejo de conseguir porque no hay disponible básicamente. Así que, bueno, al final yo conseguía algunos, así que pude hacerlos. Si no, había que recurrir a datos sintéticos porque hacer una campaña de adquisición de datos era medio difícil. ¿Qué se pedía? Y bueno, que para todas las clases obtengamos una precisión mayor al 80%. Este valor sale de más o menos lo que se obtenía con los otros procesos o modelos. Luego que puede implementarse en hardware que nosotros tenemos actualmente en los radares, o sea, que no sean demasiado grandes esos modelos para poder hacer clasificaciones que funcionen en tiempo real. Que esa clasificación no tarde mucho, ¿no? Desde que tengo los datos hasta que presento el reporte y no sea mayor a cuatro segundos. Y esto salió de algún radar de la competencia que formaba un poco estos tiempos. Bueno, nada, y el tiempo de clasificación que un poco lo mencionaba sea rápido de alguna forma. El alcance de esta tesis es evaluar los datos crudos del data se ha conseguido, o sea, hacer un estudio de los datos. Luego poder definir modelos de redes convolucionales o CNN, no convolution neural networks. Implementarlas, evaluar su desempeño, pero la implementación que no sea en el hardware definitivo, o sea, no en el radar, sino por temas de tiempo más que nada, tiempos míos dentro de la tesis. Y bueno, y que el entorno que se construya luego sirva para seguir trabajando y poder ampliar el trabajo. Bien, cadena de clasificación. Esta es la cadena o el pipeline de procesamiento que yo implementé para realizar la clasificación. No tiene nada de, digamos, fuera de lo común, pero acá está un poquito el detalle, ¿no? El cubo datos radar, que es lo que le mencioné recién, eso se manda a un bloque que hace la extracción de la señal Doppler, ¿bien? Y lo que me da ahora es el conjunto de datos de las señales Doppler para cada una de las adquisiciones que yo voy a hacer con el radar. Bueno, en este punto de acá es donde yo conseguí los datos, ¿bien? Yo no tuve que hacer todo este proceso, pero es algo que allá hacemos actualmente en la empresa, así que acá no hay ningún cuco, digamos. Bien, entonces, ¿qué hago yo con estas señales? Bueno, lo voy a mandar a lo que se llama un preprocesador del clasificador, que lo que va a hacer es construir a partir de esas señales en el tiempo, de esas muestras en el tiempo de la frecuencia Doppler, construir una secuencia de imágenes, es un poco lo que está graficado acá, esa secuencia de imágenes o frames van a ser secuencias de imágenes del espectrograma o de alguna otra representación parecida, ¿bien? Entonces, yo voy a tener básicamente en cada una de ellas información de frecuencia versus tiempo. Cada una de esos frames o imágenes es lo que yo le voy a mandar a mi clasificador propiamente dicho, ese clasificador, bueno, obviamente lo voy a configurar con los modelos que yo entrene, y ese clasificador me va a entregar un reporte de clasificación por cada uno de estos frames. Entonces, cuando esto opera en modo continuo, yo tengo un stream de frames acá y tengo un stream de reportes acá por cada frame. Entonces, stream se manda a un postprocesador y ese postprocesador lo que hace es mirar los reportes a lo largo del tiempo para un determinado blanco y sacar una conclusión y formatear un reporte que sea más amigable para el usuario. Bueno, datos crudos, esto es un poco lo que mencioné, es una base de datos que encontré de una universidad de Israel que, digamos, tenía disponible más cosas pero no estaban todas abiertas al público. Los tipos de blanco que tienen ellos ahí en esa base de datos son personas, básicamente hay conjunto de datos de una o dos personas moviéndose de distintas formas, hay vehículos, básicamente hay autos y demás, y hay tanques, y luego animales, en donde ellos discriminan vaca, caballo y luego hay otro que no se sabe bien qué es, no hay mucha información, pero bueno, estaban todas en esa manera. Se utiliza un radar en banda X, no sé, alrededor de los 10 GHz de frecuencia de portadora y se utiliza una PRF o frecuencia de muestreo de alguna forma de 5.6 KHz. O sea, quiere decir que se emiten 5.600 pulsos por segundo. Las señales, en este caso, son de tipo real, esto es una observación nada más, que en general son de tipo complejo, lo cual me permite mirar la fase, digamos, de esta señal, en este caso no la tenía de esa manera, sirvió igual, digamos, no afectó demasiado no tenerlo en complejos. Bueno, ¿qué hay entonces en ese dataset? Había esto que estaban mencionados como features, era más o menos como estaba agrupado la información dentro del dataset, en la primera estructura de carpetas había clases, y estaba diferenciado así, animales, personas, vehículos, dentro de cada una había sus clases, donde había auto, vaca, caballo, bueno, ruido es algo que en realidad ellos tenían pero no estaba disponible, así que es algo que construí yo sinteticamente. Una persona, estos safaris que bien no sé qué es, estimo que son aves, por la firma doble, o bien en conjunto animales, luego tanques y dos personas juntas en una misma celda de resolución de radar. Se tomaron a distintos ángulos respecto de radar, o sea, el vector SV de velocidad está a distintos ángulos, no todos los clases tienen todos los ángulos, a distintas velocidades, a distintas direcciones, como yendo, saltando, caminando en línea, haciendo zigzag, personas que usaban las manos, o sea, movían las manos o no movían los brazos, lo mismo con los pies, esto me parece que es como enfatizar el movimiento de las piernas, luego si llevaban equipamiento, esto long short, yo me imagino que es una antena, son mochilas de comunicaciones que llevan una antena, y luego había cosas misceláneas, personas dando saltos, varios autos en una autopista y cosas así. Bueno, nosotros le vamos a llamar clase entonces a un conjunto de features particular, y el mapeo va a ser cómo mapeo estos features en una clase particular, eso lo defino así porque había varios mapeos que yo fui probando para ver qué tal funcionaba. Básicamente el que yo les voy a mostrar a lo largo de la presentación es este mapeo, discriminó entre animal, tanques, autos, dos personas o una persona, y a veces exploto este animal en vaca, caballo y otros. Así que fíjense que acá lo que estoy mostrando es la duración total que yo tenía de adquisiciones, fíjense que es muy desparejo, hay mucho para una o dos personas o animales, poco para tanques y autos más o menos. Y esto es el número de muestras independientes, las adquisiciones que hicieron, y acá es más desparejo todavía. Así que bueno, estamos ante un dataset desbalanceado. Si miro ahora la distribución de las duraciones por cada muestra, en este programa de violin, fíjense que la mayoría de las muestras de animal duran bastante, cerca de los 90 segundos, lo mismo para tanque, sin embargo las adquisiciones para personas suelen ser más cortas, andan abajo de los 40 o 60 o del minuto. Bien, esa es otra también, otra particularidad de los datos. Bueno, acá les muestro un poquito los espectrogramas de distintas adquisiciones, fíjense este de una persona caminando, fíjense que acá se ve el torso y los pies, bueno, no logro tener alta resolución porque no tengo una perrefe alta. Fíjense una persona corriendo, tiene más o menos la misma firma pero como amplificada, ahora en el sentido vertical, y eso es porque tengo velocidades más altas, una persona saltando, una persona caminando en zigzag, fíjense que cuando cambia el ángulo respecto al radar, tengo mayor o menor amplitudes en esas frecuencias DOP, interesante. Bueno, una persona sin mover los brazos, perdí información de estos piquitos que suelen estar acá cuando tenemos personas, una persona arrastrándose, esto por ahí es lo más desafiante, dos personas caminando sincrónicamente, básicamente esto es también desafiante de poder discriminar cuando son dos de cuando es una persona, y dos personas cuando caminan desincronizadas, de manera no sincronizada. Bien, un auto a velocidad constante entre comillas, todo en este intervalo estaría más o menos a una velocidad constante, acá se ve una aceleración y un frenado relativo al radar, hay cosas así, donde hay aceleraciones, el tanque, el tanque tiene esta firma particular y tiene como un fantasma que copia ese mismo patrón fuerte que se ve acá, intenso, y eso se debe a las orugas que tiene el tanque. Bueno, y luego tenemos estos, vaca, caballo y este safari que bien no se sabe qué es. Bueno, ¿cuál es el preprocesamiento que hace? Se hace un preprocesamiento porque yo tengo estas señales crudas que ven acá arriba, a lo largo del tiempo, con esta señal yo puedo construir los espectrogramas, por ejemplo, este es un espectrograma de toda la señal, pero hay muchas señales que tienen problemas, bien, se pierde la señal, hay interferencias como ven acá, el 2, hay micro cortes, que eso yo estimo que es pérdida del tracking del blanco o algún problema del radar, hay frecuencias de interferencias o no sé, acoplamientos de algunas señales internas del radar, cosas así, pero básicamente hay señales que son muy problemáticas y otras que no tanto, que tienen buena calidad. En este caso, por ejemplo, pareciera que esto, el 6 de acá, el intervalo 6 es lo único útil de toda la adquisición. Como son muchas adquisiciones, había que hacer un preprocesamiento para poder extraer las muestras válidas, eso básicamente se hizo, digamos, haciendo un espectrograma de la señal y calculando la potencia de, digamos, de alguna forma, la componente principal que se veía a lo largo del tiempo, que es, por ejemplo, en este caso lo que se grafica acá en azul es la intensidad de la componente principal del espectrograma, y lo que se hizo es poner un threshold acá, un nivel de corte, y quedarnos cuando, digamos, quedarnos con las porciones de señal en donde la señal realmente parecía fuerte, básicamente. Bueno, eso es uno, también se aplicó un filtrado para sacarnos frecuencias altas que ninguno de los blancos que vamos a observar tienen componente en estas zonas, pero hay muchas interferencias que sí estaban en estas zonas, y bueno, con un filtrado lo sacamos. Entonces básicamente estamos convirtiendo esta señal, que es la señal original que se ve en gris, en la azul, y quedando en la zona de la punta de la señal, quedando esta señal, que es la señal original que se ve en gris, en la azul, y quedándonos con los intervalos que están resaltados ahí en celeste. Bien, pero ¿qué hay que hacer después con cada uno de esos segmentos que serían los segmentos útiles? Hay que extraer las fotos, este stream de fotos a lo que llamamos frames, ¿no? Entonces, lo que hacemos es definir una duración del frame, que en este caso se definió en cuatro segundos, y luego cuántos frames voy a extraer por segmento. Bueno, ahí hay una serie de cálculos que se pueden hacer para acomodar todo y que quede bien, pero básicamente yo lo puedo parametrizar diciendo el largo del frame y cuánto permito solapar un frame del otro para poder extraer distintos frames de una misma señal. Entonces, es un poco lo que se ve acá abajo, ¿no? Un frame sería lo que está resaltado acá en azul, el próximo frame me corre un poquito en tiempo, y extraigo esta porción, y así sucesivamente. Entonces, ahí hay un factor de solapamiento de frames. Cada uno de estos frames es lo que va a ser mi muestra luego de entrenamiento o de clasificación. Bueno, este es un poco el mapeo que más exploré o experimenté con este, que es el mapeo número cuatro, en donde discrimino una persona a dos personas, animal, que incluyo esto, no vaca, caballo, sepanis, auto, tanque y ruido. Que el ruido, como le había dicho, lo genero sintéticamente. A ese dataset lo voy a partir en tres, como ya lo expliqué en la presentación anterior, una porción, una partición de entrenamiento, una partición de validación, una partición de test. Al final la partición de validación no la uso, o sea, de cero, básicamente, porque no hago optimización de hiperparámetro, entonces uso una partición de entrenamiento y luego una partición de test para evaluar el desempeño, para muestras que no se usaron para entrenar. Algo importante hay que destacar es que yo tengo que obtener los frames, pero no puedo hacer un shuffle entre todas las muestras, porque tengo que asegurarme que todos los frames de entrenamiento no comparten adquisición con los de test. Tienen que ser adquisiciones independientes, no necesariamente ser los mismos frames, sino que tienen que ser adquisiciones independientes. Bien, esto es más o menos lo que obtengo en cantidad de muestras. La azul es la muestra que voy a usar para entrenamiento y la naranja es la muestra de test. Y esto es por clase, bien, alrededor de 3500 acá para entrenamiento 2000, 3000, 6000, 789, 6000. Entonces está bastante desbalanceado, no? Así que lo que, bueno, esto es probando otros mapeos se ve que en todos los casos está bien desbalanceado. Así que lo que voy a hacer a continuación es hacer aumentación de datos o data augmentation y voy a usar básicamente técnicas que uso, señales originales, las distorsionó de alguna forma y generó nuevas muestras. Bueno, ¿cuáles son las distorsiones que usé? Usé, digamos, time dilation, que es básicamente como estirarlas en el tiempo, o time contraction, que es como comprimirlas en el tiempo. Eso básicamente es como que si una persona caminara más lento o caminara más rápido, por dar un ejemplo. Y lo hago para distintas tasas. Entonces de esa forma voy obteniendo nuevas muestras como cada distintas velocidades de los blancos y me sirve como para mejorar la cantidad y la variación y la variedad de muestras. Y también agregué ruido. Hay muestras a las que le agrego ruido a distintas intensidades y en distintas distribuciones de ruido. Bueno, esto es un ejemplo de remuestreo. Fíjense que esta es la señal original, la que está acá arriba. Esto creo que es una persona caminando. Y a medida que le voy aplicando un factor de, en este caso, de compresión, fíjense que es como que la persona ahora caminara más rápido. Entonces, bueno, esto lo aplico para generar las nuevas señales y voy generando distintos factores para ir generando distintas combinaciones de cosas. A su vez, esa generación la hago de manera inteligente, de manera de generar más combinaciones para las muestras que tengo, digamos para las clases donde tengo menos muestras. Entonces de esa manera logro hacer una ecualización de los datos de entrenamiento, que es lo que se ve un poco acá, después de la aumentación de datos. Para llegar a algo que sería un dataset balanceado entre comillas, porque no es que son muestras totalmente independientes. Pero bueno, de esa manera, fíjense que me quedo básicamente con un poco más de 7000 muestras de entrenamiento por cada clase. Bien, para el detest no hago esto porque simplemente quiero evaluar el desempeño para muestras reales sin distorsión y nada por aquí. Bueno, paso siguiente, que hay que hacer con cada uno esos frames. ¿Se acordan? Los frames eran esas porcioncitas de aquí, ¿no? Esto que está acá, es esta porción de la señal Doppler en el tiempo. Entonces, bueno, ¿qué hay que hacer con eso? Bueno, ahora hay que convertirlas en imágenes. Bien, ese paso de conversión de imagen se hace en distintas etapas. Lo primero que se hace es aplicar un filtrado y una decimación de la señal para quedarme por ahí con menos muestras. Se aplica una decimación por 4, entonces ya en vez de estar muestrada a casi 6 kHz, están casi a 1.5 kHz. Lo cual me define que voy a poder discriminar a una velocidad máxima de 83 km por hora, con lo cual en los blancos que manejábamos estaba bien. Luego se aplica un método de conversión de esa señal temporal a imagen, que ya lo vamos a ver en detalle porque hay varios métodos. Luego, una vez que yo obtengo la imagen, se aplica una corrección del rango dinámico. Básicamente es ajustar el rango dinámico de esa imagen a lo que se nos va a acomodar a nosotros como intensidad de píxel. Entonces, bueno, de alguna forma hay métodos de cómo regularizar eso y acomodar ese rango dinámico a la señal para obtener finalmente lo que van a ser nuestras imágenes para el clasificador, que son de 128 x 128 píxeles y usan 8 bits por cada píxel para representar cada píxel. Obviamente es una imagen escala de grilla. Bueno, los métodos de conversión que probé es usar el espectrograma. Ahora vemos un ejemplo, digamos, usar el espectrograma que le mostré al principio. Luego hay unas variaciones en el espectrograma que se llaman, por ejemplo, MEL, que hace un mapeo logarítmico de las frecuencias. Eso se usa mucho porque emula el comportamiento de nuestros oídos, digamos, que no tenemos igual respuestas a las frecuencias altas que las frecuencias bajas. Bueno, probé eso, probé usar múltiples bancos separados en frecuencias. Ya les voy a contar para qué. Usé también wavelets para construir lo que se llaman escalogramas. Y estos que son básicamente aplicar una transformada coceno a la señal o al MEL Spectrogram. Esto se usa mucho en compresión de imágenes, por ejemplo, en MPEG o MP3, también para compresión de audio y demás. Así que, bueno, también evalué algunas de estas cositas. Esto está bien detallado en el documento de tesis. Si alguno le interesa, después lo puede mirar ahí en detalle en todos los formulerios. Este sería un ejemplo, por ejemplo. Este corresponde a la clase de una persona usando un espectrograma de dimensiones fijas. Esto sería la imagen, pixel o pixel, 128 por 128. Y luego, cuando aplico la escala, el stream y aplico la conversión de imágenes, bueno, básicamente de una adquisición obtengo eso. Cada uno de estos frames es lo que yo voy a mandar al clasificador y esto lo que estaría viendo mi clasificador en tiempo real. Estos son los resultados de distintos métodos. Si voy muy rápido me paran cualquier cosa. Estos son los distintos métodos de conversión. Un espectrograma común y corriente. Y finalmente, el MEL, que apiña acá, junta las frecuencias altas y tiene mejor resolución en frecuencias bajas. Entonces, como que de alguna forma tengo mayor resolución, al menos de esta porción del espectro. Aparecen ahí algunos artefactos, frecuencias, estas líneas horizontales que se ven. Y después tengo estos múltiples bancos, que es básicamente una generalización del MEL, pero yo puedo controlar cuántos bancos de filtro o cómo es la respuesta de mapeo de frecuencia. El MEL está fijado. Es una forma que no lo puedo cambiar. Acá sí, podría inclusive darle más amplitud a esto, juntar más las frecuencias altas, expandir más las frecuencias bajas y demás. Esto lo hice precisamente buscando esto, donde está la mayor información, que son las frecuencias bajas, ocupe mayor porción de la imagen. Bueno, y estos casos puntuales son usando wavelets y usando estas transformaciones coceno que le había mencionado. Fíjense que aparecen muy distintos, digamos. Ya es más difícil de interpretar para nosotros, con nuestro ojo, la firma doble de eso. Bien, y esto es para el caso del espectro más común, los resultados para distintas clases, simplemente como ejemplo. Una persona, dos personas, un animal, que en este caso es un caballo, un auto, un tanque y ruido. Bien, todo eso está implementado todo en Python. Es un pipeline de procesamiento en donde agarro el dataset crudo, parametrizo mediante archivos de configuración que son JSON. Parametrizo o configuro cada una de estas etapas. Primero voy haciendo la extracción de los segmentos válidos, luego hago la repartija entre dataset de entrenamiento y de test. A los de entrenamiento le hago el data augmentation, a los de test lo paso derecho. Y todo eso lo mando al preprocesador, que es el que convierte finalmente todos estos pedacitos de señal en imágenes. Y ahí construyo mi dataset propiamente dicho. ¿Qué modelos usamos? Los modelos que se evaluaron acá son redes convolucionales. Ahora vamos a hacer una breve introducción a esas redes convolucionales. ¿Por qué se eligió eso? Bueno, porque la idea detrás de esto era esto. De convertir las imágenes en espectrogramas y usar un clasificador de imágenes básicamente para clasificar. Esto de alguna forma también fue respaldado un poco analizando los trabajos hechos hasta ese momento. De radares hay nada publicado prácticamente, hay muy poco. Yo imagino que debe haber un montón de estas cosas hechas, pero son confidenciales. Pero hay mucho hecho para clasificación, por ejemplo, de cantos de aves o cosas así. Entonces usan básicamente este mismo proceso, este mismo método. Bien, entonces la idea es evaluar distintas redes convolucionales y ver que también se desempeñan. Entonces lo que hice fue analizar distintas arquitecturas de esas redes, cambiando, tocando las capas que usan, los filtros que usan. Ahora igual entramos un poquito en detalle. Si se normalizan o no se normalizan los datos, distintas funciones de activación. También evalué distintos optimizadores, distintas funciones de costo. Y también tocar algunos hiperparámetros a mano en este caso, para, por ejemplo, definir el número de, digamos, el tamaño de cada una de estas capas, el batch size, cuántos datos uso para ajustar el modelo y demás. Bueno, ¿qué es una red convolucional? Una red convolucional es una idea que está muy buena y que se hizo popular porque mejoró bastante el desempeño en clasificación de imágenes de los clasificadores convencionales que se vinieron usando o de los que usaban redes neuronales clásicas, esas fully connected o con un tipo de clasificación. Las fully connected o densamente conectadas. Bueno, la idea detrás de ello es aplicar filtros, básicamente que eso también es una idea que se hacía ya hace tiempo en el procesamiento de imágenes. Filtros, básicamente que yo los voy desplazando a lo largo de la imagen. Acá ven un filtro, sería el equivalente a un filtro. Estas tres capas serían los canales de color de la imagen. Entonces, ese filtro yo lo voy desplazando, ahora lo vemos igual más en detalle, y voy generando lo que se llaman activaciones de esos filtros. Luego puedo poner muchos filtros, o sea filtros diversos a lo largo de una capa y construir una salida de esa capa, aplicarla a una función de activación, como habíamos visto para las otras redes, y así sucesivamente aplicar distintas capas de convolución. ¿Qué tiene particularidad en esto, esta red? No es que para cada posición de la imagen yo tengo pesos distintos, es un mismo peso que serían los coeficientes de ese filtro, que yo lo muevo a lo largo de toda la imagen, y eso me genera distintas salidas para cada una de las porciones de la imagen. Eso ahorra muchísimo en pesos y en parámetros de la red, porque básicamente no estoy poniendo pesos para cada porción de la imagen, sino que los reutilizo. Y lo que va a hacer el entrenamiento es encontrar cuáles son los mejores pesos a reutilizar. Bueno, básicamente es esto, una convolución 2D o en dos dimensiones, que es lo que en general se usa para imágenes, es construir un filtro que se aplica a todos los canales de la entrada. Este sería, por ejemplo, un tensor, le digo tensor, pues una matriz de muchas más dimensiones. Aplicó un filtro, lo aplico en un sector de la imagen, entonces eso me da como un colapso de esos valores mediante los parámetros del filtro y me entre en una salida. Esa salida es lo que se denomina feature map, es como por cada uno de esos filtros yo genero una salida particular para cada posición de la imagen. A ese filtro se lo llama kernel en general. Luego hay parámetros que trascienden al filtro, que tienen que ver con cómo muevo yo ese filtro a lo largo de la imagen. Por ejemplo, este es un parámetro, el horizontal strike. Cada cuántos pasitos me muevo yo a lo largo de la imagen. Acá en este caso muestra que se movió solo un paso, pero se podría mover cada tres pasos. Eso genera una compresión mayor de lo que tengo acá a la salida, porque tendría menos datos. Bueno, así que esos son muchos los parámetros que yo voy ajustando para definir una capa convolucionada. Como le dije, se puede usar más de un filtro por cada capa. Entonces, por cada filtro que yo utilice o por cada kernel que yo utilice, genero un feature map distinto. En este caso, por ejemplo, estaría usando tres filtros. Entonces, por cada uno de esos filtros o kernels, yo genero un mapa distinto de activación. Fíjense que lo tengo que recorrer siempre con igual manera y tener la misma dimensión. Eso sí, para que yo luego a todos esos feature maps los pueda estaquear. Puedo hacer un stack de eso y construir nuevamente un tensor uniforme a la salida de esta capa. Bien, esos son lo que se llaman canales de la capa, básicamente. El conjunto de esos se llama conjunto de mapas de activación. Bueno, acá se ve un GIF en donde se aplica un filtro. Esto es un filtro para detectar líneas verticales. En donde, fíjense, que se recorre con un stride uno horizontal, una imagen que sería representada por esta, y eso va generando las salidas o las activaciones. Cuando tengo una imagen con más canales, es lo mismo. Se aplica el mismo filtro, solamente que tengo más dimensiones. Y todo eso se colapsa de nuevo en un solo canal. Obviamente este filtro acá está definido arbitralmente, pero es lo que la red va a aprender cuando se entrene. O sea, estos pesos que están aquí. Bueno, acá se ve el caso en donde tengo colores, rojo, verde y azul. Cómo se aplica un filtro. El filtro estaría definido por cada uno de estos. Entonces, cada uno aplica un canal distinto, me entrega un valor, y todo se colapsa en una única salida. Si yo quiero otro mapa de activación, obviamente tengo que definir otro conjunto de estas tres matrices que están aquí. Aquí es un poco lo que se muestra este ejemplo acá. Bien. Luego que tengo los mapas de activación, o los feature maps, ahí le aplico la función de activación. O sea, cada uno de esos valorcitos que veíamos recién, cada uno de esos valores que se van generando acá abajo, lo voy a pasar por una función de activación. O sea, ese sería mi valor x acá y entraría mi función de activación y el valor de salida sería lo que entregaría mi capa convolucional. Bueno. Ahí, yo lo que voy a mencionar en general es, las que se utilizan mayormente son las relu o lichirrelu, y no estas de acá. No sigmoide, no tasquenta hiperbólica, que son mucho más comunes en redes neuronales densas y chicas. Se utilizan estas precisamente porque estas funciones que están acá, si miramos la derivada, en estos sectores de aquí, tienen derivada cero. Eso, básicamente, sin entrar en detalle, genera de que cuando yo calculo el gradiente, si acuérdame, veíamos el gradiente descendente, cuando yo calculo el gradiente, hay momentos en donde ese gradiente se puede hacer cero, básicamente, porque la derivada está cero. Eso genera el problema de desvanecimiento del gradiente. Lo que genera de alguna forma es que cuando yo pongo redes muy profundas, hay redes que básicamente no les llega la información del gradiente y no terminan ajustando sus pesos. Entonces, se entrenan solo, se terminan entrenando solo pocas capas de esa red. En cambio, estas funciones de activación, no tienen ese problema porque la derivada, al menos en un sector, no se hace cero, con lo cual se pueden ajustar mejor en esa función. Y a su vez también se ve que en la práctica, cuando se usan capas con Relu, convergen mucho más rápidamente que cuando se utilizan estas. Bueno, y estas son básicamente variaciones de esta. Básicamente que trabajan en cambiar el comportamiento en esta zona. Pero no voy a entrar mucho en esto. ¿Qué se suele hacer luego de las redes convolucionales 2D? En general se suele aplicar lo que se llama pooling. El pooling es como quedarme con un valor de acuerdo a un sector de esa salida de la red. Lo más usado en realidad en estas redes es MaxPool. ¿Qué quiere decir MaxPool? Bueno, por ejemplo, me paro en estos valores y me pongo en este valor. Luego me muevo, pongo mi atención en estos otros cuatro valores de aquí, me quedo con el máximo y entrego esto. Bien, y así sucesivamente. Obviamente esto también puedo definirlo como lo recorro, los straight, horizontal y verticalmente, y cuál es el tamaño. Esto básicamente lo que me va generando es una reducción de la dimensionalidad y cosa de que mis parámetros de la red se van a cambiar de una vez que se va a cambiar de otra vez. Y al final de estas redes se pone al menos una capa, en general se pone más, pero al menos una capa de redes totalmente conectadas, de neuronas, perdón, totalmente conectadas. Esta etapa, lo que se le llama, es la etapa de clasificación. Déjenme volver un casquito acá. Y ahora, por ejemplo, si yo pongo un parámetro de una red, déjenme volver un casquito acá. Yo pongo capas convolucionales, a toda esa parte se le llama etapa de feature learning, ahí se están aprendiendo como los patrones de los patrones 2D de la imagen, y luego uso una capa o una etapa con capas densas para hacer la clasificación de esos patrones. Estos patrones se corresponden a uno a otro, estos patrones se corresponden a una persona. Y otra parte, en general cuando hago clasificación, la última capa usa una función de activación que se llama softmax. Lo que hace softmax es asegurar que cuando yo sumo todas las salidas de mi red, todas den 1, básicamente es lo que está asegurando mi softmax. Eso porque se corresponde con el tema de probabilidades, si yo sumo las probabilidades de que esa imagen pertenezca a todas las clases, esa sumatoria debería dar 1, lo ideal sería que 1 de esas salidas sea 1 y todos sean 0. En general se usa esta función de activación que lo que asegura es eso. Bien, este es un ejemplo de una red muy usada en la clasificación de imágenes tradicionales que se llama AlexNet. Simplemente voy a poner un ejemplo. La imagen de entrada toma 227 por 227 píxeles y 3 canales, los 3 canales de color, y luego se van aplicando, por ejemplo, una red convolucional 2D con una activación relu y luego un max pooling 2D. Acá están los detalles, por ejemplo, esta usa 96 filtros que tienen tamaño 11 x 11 y se mueven de a 4 píxeles por cuarto píxel, o sea, 4 píxeles horizontal y 4 píxeles vertical, y el pooling es de 3 píxeles por 3 píxeles y se mueve de a 2. Bueno, y así sucesivamente, ese sería el detalle de cada una de esas capas. Fíjense que esta imagen de 227 x 227.3 se convierte ahora en una imagen de 27 x 27 x 96, 96 mapas básicamente es esto. Pero ahora la imagen es más chica, dimensionalmente, al menos en el 2D. Luego se aplica otra capa igual y fíjense que se va reduciendo la dimensión 2D, pero se va aumentando la cantidad de canales. Bueno, nada, hay una arquitectura particular que fue pensada para esto. Al final se colapsa ese tensor en un vector unidimensional y se lo pasa a la etapa de clasificación que son regres densas de 4096 neuronas por capa y la final tiene 1000, porque se quiere clasificar en 1000 clases distintas esas imágenes. Bueno, este modelo tiene 62 millones de parámetros a entrenar. La idea entonces que hay atrás del trabajo este es construir modelos más chicos que tengan un buen desempeño para este problema. Bueno, hay paso rápido, hay distintos modelos que fui evaluando, fíjense que ya de por sí son modelos más chicos. Acá están los 128 por 128 por 1 que serían los espectro, las imágenes, los frames. Y acá bueno, esta tiene dos capas convolucionales, dos capas densas. Acá está un poco el detalle, me apuro porque si no, se no va a ser largo. Y fíjense que tiene 670 mil parámetros a entrenar. O sea, bajamos razonablemente la cantidad de parámetros. Muchos esos parámetros están acá en esta etapa y no acá en esta etapa de feature learning. Este modelo que es el más grande que construí tiene cuatro capas convolucionales y tiene igualmente una etapa de clasificación no con más capas, pero sí con capas con más neuronas. Y este que es una red neuronal convencional que simplemente es un modelo de clasificación la implementé simplemente para comparar redes convolucionales versus redes tradicionales. Fíjense que este tiene ocho millones de parámetros y el anterior que es el más grande de convolucionales que he evaluado tiene cuatro millones de parámetros. Este es mucho más chico que este. Para el entrenamiento, uso el esquema tradicional de entrenamiento que ya un poco lo vimos. La presentación anterior, no hay mucho que agregar acá. Sí que se parametriza, mediante un JSON puedo elegir las funciones de costo, las métricas, qué optimizador uso y cosas así. Bueno, esto ya lo charlamos en la charla anterior. Es un poco el proceso que voy haciendo de configuración de la etapa de entrenamiento. La función de costo que utilizo, que es un poco la particularidad acá, no puede ser cualquiera porque quiero hacer clasificación y en general se utiliza esta función que se llama categorical cross entropy que en resumidas cuentas queda simplificado a esto. Donde este I sombrero es lo que va a entregar mi clasificador donde es un arreglo de tantos valores como clases tengo y me indica el valor de probabilidad de cada una de esas clases y este es el valor real que tendría un 1 en la clase real y un 0 en el resto de las clases. Bien, entonces esta es la función de costo que voy a querer optimizar. Resultados. Acá muestro los resultados por esos tres modelos que les mostré. Fíjense que el modelo más grande, el entrenamiento converge a las 100 épocas más o menos, casi al 100%. Bueno, acá está un poco el detalle. Para los datos de entrenamiento y para los datos de test cerca del 90%, lo cual muy bien. Este modelo más chico, convolucional, en naranja, fíjense que para entrenamiento es mucho más lento la convergencia y tiene un poquito menor de desempeño, un poco cerca del 86% para los datos de test. Lo particular acá es la red neuronal convencional, fíjense que converge más rápido, que inclusive tiene mejor desempeño para entrenamiento en algún punto, pero para los datos de test no anda bien. Esto es lo que habíamos mencionado como capacidad de generalización. Y esto es lo que en general se ve para las redes convolucionales. Las redes convolucionales, para la clasificación de imágenes, suelen tener mejor generalización, o sea, suelen usar los datos de entrenamiento de una manera mejor para luego generalizar sobre los datos que no conoce. Bueno, acá se ve, es bastante significativa la diferencia. Esto anda alrededor del 78%, 77% y no estoy cumpliendo los regimientos, que era estar arriba del 80%. Bien, esto simplemente es mostrar la función de costo, cómo se va optimizando. Acá simplemente quería mencionar esto. Para la red que es muy grande, muy tempranamente el desempeño para los datos de test en lo que es función de costo ya empieza a ser malo. Eso quiere decir que estoy haciendo overfitting, sobreajustando mi red. Bueno, este sería un buen momento para parar el entrenamiento en este caso. En estos modelos acá hay un mayor detalle. Para la red neuronal convencional, fíjense que también empieza a hacer overfitting en algún punto, no así para esta. Bien, resultados. Lo que obtuve en cuanto a desempeño, esto ya es test solamente, no estoy mostrando entrenamiento, que casi todas en algún punto llegan al 100%. Fíjense que para test, este que es el modelo más grande, anduvo cerca del 88%. Este que es una red más chica, mucho más chica. Fíjense que en desempeño está prácticamente igual al 85%. Bueno, hay gente que mataría entre comillas por esos 2%, pero no hay tanta diferencia. Sí hay mucha más diferencia con la red neuronal convencional que anda alrededor del 77%. Bien, esto es accuracy. Esto es una métrica. Accuracy quiere decir, es el porcentaje de muestras que clasificó en la clase correcta, básicamente. Acá muestro unos diagramas tipo radar, ya no tiene nada que ver con el radar que estamos hablando nosotros, por clase. Porque este es el desempeño medio a lo largo de todas las clases. Pero ahora quiero ver el desempeño por clase. Bueno, si yo miro la precisión, por ejemplo, acá en azul, tengo esto que es para el modelo que mejor anduvo, la precisión para las distintas clases. Fíjense que en general es bastante parejo, anda alrededor del 90%, para acá un poquito menos, un poquito menos, pero para tanque anda mal en precisión. Sin embargo, anda bien en recall. ¿Se acuerdan de la diferencia? Recall, que es esto de acá, es un poco más grande. Y aquí, en el modelo que mejor anduvo, es la precisión para las distintas clases. Y ahora vamos a ver la precisión en recall. ¿Se acuerdan de la diferencia? Recall, que es esto de acá, es que también clasificó cuando realmente era esa clase. ¿Bien? La muestra. Es esa proporción de alguna forma. Y la precisión es al revés. De todas las que clasificó, en este caso como tanque, ¿cuál eran realmente tanques? Bueno, esto luego se acuerdan que habíamos dicho que el F1 era como una especie de media armónica entre estos dos. Bueno, fíjense que estaría acá en el medio, básicamente. Y nos podríamos quedar con esa métrica como para evaluar y comparar los modelos, así nos estamos viendo todas estas a la vez. Este es el modelo que mejor anduvo y este es un poco más chico. Fíjense que el fenómeno del tanque pasa igual, pero para dos personas anda bastante mal en cuanto a recall, en este caso. Pero ahí es donde se ve la mayor diferencia entre los dos modelos. Bien, bueno, desempeño por clases. Esto es comparando los tres modelos. Ya ahora solamente me quedo con el F1 score. Y fíjense, lo azul sería el modelo más grande. Que anda cerca del 90%, bueno y acá 85% más o menos en esta clase. El modelo más chico, convolucional. Fíjense que tiene un menor desempeño en lo que respecta a personas. Ven, acá y acá. En el resto está un poco más parejo. Y acá está bastante mal el otro, el de la red neuronal convencional. Bien, bueno. Matrices de confusión. Fíjense que para el caso del modelo más grande, para el dataset de entrenamiento, los clasificó a todo casi perfectamente. Entonces, esto anduvo muy bien para entrenamiento, pero qué pasó con test? Bueno, me entrega estos resultados, donde para algunos anda muy bien. Por ejemplo, para tanque, todos los que eran tanque prácticamente los clasificó como tanque. Pero por ejemplo, para auto no, solamente el 90% clasificó como tanque. Bueno, es muy bueno igual, pero dígense que muy bueno. De los de auto los clasificó como tanque. De ahí la diferencia en el recall de estos. Tiene buena precisión, pero si yo miro de acá, hay muchas veces que me dio como tanque los que eran auto. Bueno, pero en general es bastante satisfactorio porque todos estos están arriba del 80%. El que peor anduvo es para dos personas, que hubo gran confusión con una persona. Lo cual era previsible porque había muchas muestras de dos personas caminando sincrónicamente, lo cual es muy difícil de separar. Otra particularidad es que, fíjense que hay un 4% que los clasificó como animal. Tanque también tiene algo de coherencia porque los animales tienen cuatro patas. Bueno, nada, puede haber una relación muy buena con el animal. Bueno, qué más, qué más de acá, no mucho más. Esto es un poco el reporte de los tiempos. Acá está en mi computadora la que usé. Está bastante bien, esto porque había un requerimiento de tiempos, no había que corroborarlo. Fíjense que para preprocesar cada muestra, no hay nada que no sea de la misma manera que la de los animales. No hay requerimiento de tiempos, no. Había que corroborarlo. Fíjense que para preprocesar cada muestra, me tardaba casi dos milisegundos y para hacer la inferencia, o sea poder asignar la clase, unos 20 milisegundos, esto lo hago en una GPU, con lo cual, por cada muestra necesito 22 milisegundos, lo cual está bastante bien. Bueno, eso me permitiría tener una tasa de clasificación de este valor para mi máquina, que supone que para el radar tendríamos mayor capacidad de procesamiento. Bueno, también probé distintas combinaciones de dataset. Los dataset, obviamente, hay muchos parámetros que me definen un dataset, los que le dije, ¿no? ¿Qué duración del frame, en cuanto lo solapo, qué método utilizo para convertirlo a imagen, cómo estandarizo las muestras, cómo ajusto el rango dinámico, si uso wavless y no uso wavless, si uso múltiples bancos de filtros, si uso, en este caso, data augmentation, bueno, un montón de eso fui evaluando. El que vamos a tomar como referencia es este, que es un espectrograma común y corriente, con frames de 4 segundos, solapados 3 segundos y medio y un rango dinámico de 100 decibeles. Y lo que se muestra acá es la diferencia de estos datasets respecto al de referencia. En este caso usé múltiples bancos, este es el mismo pero sin usar data augmentation, este es medio lo mismo pero con un rango dinámico menor, este lo mismo que este a una forma pero usando más solapamiento, con lo cual tengo más muestras, y bueno, y así sucesivamente. Y esto uso wavless, conversión discreta de Coseno. Mira, aquí está el desempeño para todos los datasets, bueno, dataset es el convencional, del espectrograma convencional, anda en el 888 que es el que les mostré hace un rato. Y fíjense que a medida que voy cambiando algunas configuraciones, va cayendo, las de wavlets y anduvieron medianamente mal, si se quiere, todas pasan los requerimientos, pero si anduvieron son esos, yo esperaba que anduvieran mejor, ahí para mí es un gran nicho para trabajar, las de transformada Coseno fueron las que peor anduvieron. Y después qué sé yo, algunas consideraciones particulares, por ejemplo, si no uso data augmentation paso de este a este, es un 1%, no es mucho, yo esperaba que sea más, pero bueno, se logra una mejora haciendo data augmentation. Cambiando el rango dinámico, de acá a acá un 2%, más o menos se ganó. Bueno y así, ahí creo que es interesante hacer algunas lecturas sobre eso y acá están lo mismo, pero por clases. Ya es que me doy lo último, en cuanto a capacidad de modelo, de cada uno de los modelos, creé como una versión alternativa en donde hacían las capas densa de clasificación, la hacía mucho más chica, porque ahí es donde se me va la mayor cantidad de parámetros y el tiempo de cómputo. Entonces, probé de minimizar un poco esa etapa de clasificación y ver qué pasaba. Fíjense que la caída del desempeño es muy mínima, de uno al otro y en este caso redujo, esta creo que tenía 4 millones de parámetros, lo redujo a 800 mil o algo así que tienes, no me acuerdo el número exacto, pero hay una gran reducción de parámetros y fíjense que la reducción del desempeño no es significativa. Con lo cual, en un modelo a desplegar está bueno tener esa observación porque me conviene reducir el número de parámetros, tener un modelo más chico. Bueno, eso lo mismo lo evalué para los otros, pasó básicamente lo mismo, es un 2% en algún caso, como mucho, pero en este pega un poquito más en el desempeño por clase, en una o dos personas cambia de uno a otro. ¿Qué más? Bueno, probé los optimizadores, probé diversos optimizadores, no hay una conclusión significativa sobre que haya alguna variación, cambian un poco los tiempos de convergencia de entrenamiento, pero no nada significativo, bien ajustados, si los ajusto mal, si los parámetros son mal, obviamente no pasa esto, pero fíjense que no hay grandes diferencias entre usar un optimizador y el otro, al menos en este problema. Bueno, y por último, post-procesamiento, ¿qué hago con el post-procesamiento? Se acuerdan que luego de todos los reportes, por cada frame el clasificador me entregaba un valor de probabilidades de cada clase. Bueno, yo ahora voy a agarrar toda la adquisición sobre un blanco y voy a sacar una conclusión, básicamente ese es el post-procesamiento que voy a hacer. Entonces, ¿qué hago? Bueno, digo, si yo agarro esta adquisición, es una adquisición que la agarré particularmente, fíjense que lo que hice acá, esto es para una persona, caminando de manera normal en zigzag, fíjense lo que me entrega el clasificador, en general me entrega que es más probable que sea un animal y una persona, están casi iguales, dependiendo, esta es la distribución de los reportes de clasificación, y en algún caso los clasifico como dos personas, si lo veo a lo largo tiempo, estoy viendo esto, en este intervalo lo clasifico como una persona, en estos intervalos de acá, Azules los clasifico como animal, bueno y dos personas sería lo que quedan en marrón, que aparecen por ahí unos pititos. Si yo aplico el mismo pre-procesamiento que hacía para identificar los segmentos útiles y en función de eso descarto el resto de la adquisición y sobre eso no más hago clasificación, el reporte cambiaría de esta manera. Descarté lo otro y fíjense como cambió el reporte, ahora me dice que mayormente es una persona, hay alguna que otra chance que sean dos personas, pero en general ahora animal, fíjense que prácticamente lo descarta, eso es un post-procesamiento bastante sencillo y reutilizo lo que hacía en el pre-procesamiento de alguna forma. Bueno, qué pasa ahora con esa matriz de confusión que le había mostrado recién, pero aplicándole post-procesamiento, bueno cambió a esto. Entonces fíjense que prácticamente ya había valores con el 84% y demás, no sé si se lo recuerdan, ahora el mínimo es 91%, o sea ha mejorado significativamente el desempeño para los datos de test en este caso. Así que bueno, nada, hubo una mejora significativa, acá un poco se ha pasado números sobre todas las, esto es para el data CTT, fíjense que acá este sería como de alguna forma mi conclusión, es un 97% usando solo segmentos válidos y un 96% usando todas las señales, no hay gran cambio pero algo mejor. Bueno, nada más, una mención a las herramientas que utilicé, todo software libre, open source, en general todos los modelos están implementados con Keras, TensorFlow, bueno y librerías anexas, todo el desarrollo está hecho en Python y algunos casos o ejemplos o detalles están implementados en notebooks de Jupyter. Tengo algún visualizador de datos y demás hecho en plotly, que se levanta como un servidor, así que bueno, nada, eso es un poco el resumen, uso mucho TensorBoard para ir haciendo el seguimiento de los modelos y del entrenamiento a medida que se van entrenando, esto en general dependiendo del modelo pero tardaba y el data CTT obviamente algunos tardaban dos horas, otros tardaban a veces ocho horas en entrenarse más o menos para dar una idea. Conclusiones rápidas, bueno, cumplieron los objetivos planteados todos, lo cual está muy bueno y bueno, algunas observaciones, ya se las fui diciendo, mejor desempeño se logró con este modelo y para este data set, un programas común, se podría haber logrado mejor desempeño si el data set fuera de mejor calidad, cierto es que el data set es medio pelo, pero bueno, al menos sirvió bastante. El desempeño es bastante sensible a la parametrización, o sea el preprocesamiento, lo que más se vio es sensibilidad respecto al rango dinámico, a las señales, los métodos de aumentación de datos, de data augmentation, mejoraron levemente el desempeño y la generación, no mucho. Las etapas de feature learning influyeron más en el desempeño que en la clasificación, o sea las etapas convulsionales, más que las etapas de redes densamente conectadas, probé muchas técnicas de regularización para mejorar la generalización de los modelos, no logré mejoras significativas, muy poco, digamos, a costa de inclusive tardar más en entrenar, pero no logré mejores resultados. Optimizador es lo mismo y el post-procesamiento mejora el desempeño y obviamente me permite dar un reporte más amigable, que es ese gráfico a lo largo del tiempo. Tragajos futuros, bueno, me parece que si esto lo quisiéramos implementar en la empresa y desplegar, tendríamos que hacer una campaña de adquisición de datos, al menos más amplia y con los plancos que nos interesan, con nuestros radares. Se puede hacer un mejor análisis de la sensibilidad a todos los hiperparámetros, muchos hiperparámetros, con lo cual hay muchas combinaciones de datos que podríamos evaluar automáticamente y ver cuáles influyen más en el desempeño. Bueno, nada, evaluar otros preprocesamientos, usar transfer learning, o sea, utilizar modelos estándares que ya entrenados, grandes y partir de esos modelos entrenados para entrenar para nuestros datos y ver qué pasa. Poder detectar múltiples patrones simultáneos, esto es para simplificar la detección, porque fíjense que en nuestros patrones siempre hay un solo blanco por adquisición, no los tengo combinado. Es raro que eso pase en el radar en la vida real, pero podría darse. Bueno, usar otro tipo de redes como las redes recursivas, eso podría andar bien también. Bueno, y implementarlo, eso está bueno. Si a alguno le interesa mirar el documento de tesis, está publicado por el eBay y si no me lo piden y se lo paso. Hay un repositorio también en el Inbap, que es mío, del GitLab, también está todo subido ahí. Y bueno, con eso termino. No sé si tienen alguna pregunta. Yo sí tengo una pregunta. Vos decís o nos comentabas que uno de los limitantes son la cantidad de muestras debido a la cantidad de muestras que puede tomar el radar, ¿cierto? Sí. No pensaste o no tuviste la idea, no sé si lo intentaste o no, de hacer algo similar a lo que es la superfluidez o los time. ¿Qué sería eso? El método se llama superfluido, se le dice así mejor dicho en español, es aumentar los frames per second, por ejemplo, de una foto y crea una foto trucha, ficticia, digamos, entre esas dos fotos. Así con eso poder aumentar la cantidad de muestras, es decir, hacer un modelo previo a este de reconocer las imágenes, que son las imágenes, para aumentar la cantidad de datos que tenemos. Sí, podría, digamos, o sea, se podría usar como proceso de aumentación de datos. Claro, porque lo que te limita a vos justamente, vos a lo que nos comentás mejor dicho, es que lo que te está limitando es la cantidad de datos. Sí, en realidad lo que se necesita más que nada es más variación de escenas, más que, digamos, más frames en ese sentido. Fíjate que hay una particularidad acá, déjame que vuelva un cachito que no, ya que lo preguntás entro un poquito más en detalle. Fíjate que en este caso, en este dataset de acá, aumento la cantidad de frames, o sea, que hay muchas más muestras, tengo un dataset con más muestras, haciendo esto, o sea, solapando más, con lo cual de alguna forma es como que tuviera más imágenes por segundo de una misma. Pero ahí está, pregunto de nuevo, ahí estás, para el entrenamiento del modelo este, para detectar, no a los datos que entran al modelo, ¿no es cierto? No, no, no, acá es, estos sí, son los datos, porque, digamos, yo tengo una adquisición en el tiempo y obtengo muchos más frames de esa adquisición. ¿Cómo, cómo? Que le haces algunas cosillas para que sean más datos, sí, entiendo. Claro, claro, ahí es como que tuviera una película a más frame por segundo, pero sigue siendo la misma película, solamente que a más frame por segundo, ¿no? Y fíjate que ese dataset, entre estos dos, la única diferencia es esa, ¿no? Que entre Scuto y de esta corrida tengo más frame por segundo. Y fíjate que, inclusive tiene menor desempeño. Entonces, si bien, digamos, puede haber un montón de factores ahí en el medio, yo lo que necesito y lo que decía con mayor cantidad de datos es poder tener más escenas, ¿no? O sea, poder tener más películas más que más frame por segundo, ¿no? Pero es viable, digamos, se podría pensar algo así, pero vos para hacer eso de alguna forma tendrías que, primero, tener una buena película con alta resolución para poder construir el modelo que luego te entregue y te permita mejorar las adquisiciones en las que no tenés tan buena resolución, para sacar más. Por lo que yo, bueno, que se entiende lo que entiendo, si lo que detecta es una, es alguno de los objetivos dentro de esa que inventaste, hay un error grande, supongo, ¿no? Claro, puede ser, habría que evaluarlo, digamos, interesante, pero sí, podría usarse como método de aumentación de datos, podría ser. O sea, hay muchas veces que se utiliza, si vos conoces el modelo de distribución o los fenómenos que hay detrás, puedes usar eso para construir muestra sintética o semi sintética a esas cosas que dan buenos resultados. En este caso, yo, habría que valorarlo más, ¿no? Hay muchos frentes que tocar, pero no logré grandes diferencias implementando, hay muchas cosas que no puse acá, pero, voy probando, pero no logré nada como significativamente, bueno, si no lo hubiera puesto, pero bueno, es cuestión de explorar un poco más, ¿no? Claro, eso no más, listo. Bien, Martín. Hacía una otra preguntita. Hola, Franco, buenas tardes. ¿Cómo andas? Muy bueno, la verdad, increíble. Bueno, gracias. Tenía algunas preguntas. Cuando nos explicaba, cuando armaste el dataset, no me quedó claro, ¿vos sabés muy bien en qué instante el tiempo comienza y termina cada feature? No, porque, o sea, de alguna forma sí, cada el dataset ya viene armado, extraído, digamos, o sea, lo que hicieron los tipos son, ponen el radar, pusieron a una persona a caminar, o sea, te cuento lo que dice el paper del dataset, ¿no? Pusieron a una persona a caminar, le apuntaron a esa persona y no había más nada, digamos, en la escena y sacaron de ahí la información. Luego para poner, qué sé yo, el auto, lo mismo. Entonces, están bien diferenciadas, eso no pasa en la vida real, básicamente, digamos, ¿no? Claro, pero justamente ahí me contestaste a la otra, por ti a preguntar cómo viene armado, si sabías cómo viene armado. Lo hicieron así, digamos, y por eso es bien controlado, digamos, la adquisición de esos datos. Ahora, ¿qué pasa en la vida real? La vida real nosotros lo probamos, por ejemplo, agarramos un radar nuestro, lo llevamos ahí a la entrada de Bariloche y lo apuntamos hasta el lugar. Claro, ¿qué te pasa luego? Que tenés un montón de blancos que se pueden estar mezclando y demás, pero ahí viene toda la parte de procesamiento radar en donde vos identificás un blanco dentro de una celda de resolución del radar, ¿ven? Y una vez que lo tenés ahí detectado, digamos, vos sobre eso podés hacer un procesamiento para sacar la señal Doppler, digamos, entonces de alguna forma cuando vos detectás, empezás a sacar la señal Doppler y eso es lo que vas a alimentar a tu clasificador. ¿Qué puede llegar a pasar? Que a lo largo del tiempo ese blanco se mueva de esa celda de resolución del radar a la próxima, a una junta, digamos, ¿no? Ya no lo tenés más y tu señal desaparecería. Bueno, entonces ahí aparece otro elemento más que es el tracker, entonces vos vas trackeando la detección de tu blanco y como que vas poniendo tu atención sobre ese blanco y sobre eso hacer la recuperación de la señal Doppler, entonces de alguna forma eso te aseguraría que cuando vos extraés la señal Doppler de un blanco, bueno, ese blanco en principio siempre es el mismo y vos podrías tener en paralelo, vas a tener en realidad en la vida real en paralelo todas las señales Doppler de todos los blancos que estés detectando en ese momento, ¿entiende? Sí. Entonces ya ahí ya tenés esa separación. Lo que sí te puede pasar es que dentro de una misma celda haya más de un blanco, como en el caso de dos personas, lo que te podría pasar, por ejemplo, en un caso nosotros teníamos auto y bicicleta, veníamos trackeando una bicicleta y un auto venía al lado de la ruta o en un momento le pasó por al lado a la bicicleta, bueno, si vos mirás la señal Doppler de la bicicleta porque es la que vos venís trackeando, viene bien bien bien bien y en un momento se ve que aparece el auto al lado, entonces ahí se te combinaron y ahí sí, y ahí digamos lo que aplica un poco lo que decía recién, hay otros métodos para poder separar o clasificar ambos a la vez, decir che, esta porción del Doppler sigue correspondiendo a la bicicleta y esta porción del Doppler corresponde a un auto, con eso yo no lo hice, estaría bueno hacer lo que es como poder hacer segmentaciones o cosas por el estilo de ese mismo espectrograma. Y si vos tuvieras que hacer construir el dataset, lo harías así y no como hicieron ellos? Claro, claro, o sea, a ver, yo en principio armaría un dataset de muy buena calidad como lo hicieron ellos, pero usando nuestros radares, porque el radar tiene muchos problemas, digamos, por interferencia y cosas así, entonces primero usaría nuestros radares y usaría escenas que nosotros estaríamos más sujetos a clasificar, en ese caso. Eso por un lado, la armaría de esa misma forma, así bien aislado, ¿por qué? Porque es como la mejor forma de entrenar, pero también luego crearía escenas de estas que yo te digo para ver el desempeño del radar del clasificador en esos casos o posteriormente en un trabajo futuro hacer un clasificador múltiple. Ahí lo que sí el dataset ya es más difícil de etiquetar porque vos tendrías que armar tus espectrogramas y luego generar los boxes, digamos, las cajitas diciendo, esta parte del espectrograma corresponde a una persona y esta parte del espectrograma corresponde a un auto, cosa que la red pueda aprender a diferenciar y luego cuando el usuario del radar mire el espectrograma o cualquier otra cosa, vos puedas discriminar esas zonas y clasificar de manera distinta. Eso si lo ven en imágenes, en clasificación de imágenes, vieron cuando una imagen detecta ambas cosas distintas en una misma imagen, bueno, el mismo proceso. Lo que pasa es que vos tenés que construir tu dataset y etiquetarlo y hacer todo el proceso de generar los boxes para poder discriminar y enseñarle a discriminar a tu clasificador. Perdón, sigo. ¿Pudiste ver qué aprende cada, no me acuerdo si era cada filtro, cada gata, pero ¿pudiste hacer eso de ver qué aprende? Sí, sí, lo, digamos, sí se viste que vos podés de alguna forma ver la activación que maximiza la salida de un filtro y eso con eso con eso construís como las imágenes que activa cada filtro. Y principalmente si es algo que uno es capaz de interpretar, porque capaz que si lo ves, pero no... Lo hice, lo hice, pero no, al menos lo que yo pude rescatar de eso es que no es, es ininterpretable para al menos para mí, no lo debería ver. Cosas viste como ahí, papers de las redes convocaciones que te dices, bueno, mira acá ve un patrón de líneas horizontales, otro patrón de líneas diagonal, cosas así, no aprendió por ese lado y eso para mí tiene que ver con que son imágenes muy particulares estas, en principio tienen como mucho ruido por un lado y luego tiene patrones que son diversos y no es como las imágenes nuestras que tienen por ahí líneas muy definidas o patrones de colores o cosas así, entonces estimo que debe venir por ahí, pero no, el tema de interpretar el aprendizaje en este caso intenté, hice algunas cositas pero nada, no rescaté nada, por eso no voy a hacer una tesis. ¿Tiene sentido usar alguna otra arquitectura pero para no trabajar con imágenes y no con la serie temporal? Sí, por eso un poco puse lo de las redes recurrentes, las RNS, claro, claro, ahí podemos usar LSTM o Jereguz o cosas así, yo creo que tiene mucho potencial ese lado, en el momento que investigué un poco no había, al menos yo no había visto nada hecho, después aparecieron cosas y por el lado de clasificación, si buscan audios de aves y cosas así, ahora en el último tiempo aparecieron unas cosas hechas con redes recursivas y que andan mediadamente bien. ¿Y el beneficio sería minimizar, no sé, o bajar el tiempo de clasificación? Se supone que sí, yo de alguna forma también lo dudo, ¿por qué? Porque ya son cosas más sutiles, tendría que probarlo y ver si mi hipótesis es verdadera, pero las redes recursivas está bueno porque vos mirás el patrón directamente sobre la señal, podrías hacerlo eso, ojo, podés hacer la combinación, podés ver el patrón, podrías hacer la red convolucional sobre los frames, sobre las imágenes y luego usas una red recursiva para ver la secuencia de frames, eso también lo puedes hacer en una combinación de las dos, pero podrías ver directamente la señal, el time series, la serie temporal con esa red y hacer la clasificación directamente ahí. Ojo que esas redes andan lentas también porque los forks que tienen implementados adentro caecen en estructuras tipo forks que suelen ser más lentas, en cambio las convolucionales, si bien tenés un recorrido de la imagen que hace que sea un poquito más lento, muchas de las GPU están bien preparadas para hacer esas cosas. Claro, ganan mucho tiempo por ese lado, entonces yo querría ver, no sé si en cuestión de tiempo le va a ganar, pero en cuestión de capacidad también hay que ver el tamaño y cosas así, pero es interesante ver, para mí esa rama es una rama que estaría bueno investigar. Che y la última, supongo que el objetivo final sería reemplazar a la cadena de procesamiento, no entiendo mucho de RAV, pero hoy pusiste un tiempo máximo, ¿va por ahí? Hoy para nosotros en Inbap es un bloque que nos falta, hoy no está, o sea la cadena de procesamiento que nosotros tenemos es detección y extracción del Doppler, pero para medir velocidad, no mucho más que eso, y luego viene otro que es la representación de la imagen de las detecciones y demás. Hoy no está el bloque clasificador de nuestros radios, entonces hoy no reemplazaría nada, simplemente mejoraría. Si hay algún diseño que se hizo en algún momento con unos radares chiquititos que se llama RACID, que se utilizaban para control de fronteras y cosas así, en donde se hizo un clasificador en Inbap hace mucho tiempo, pero se usaban metodos más convencionales, como vecinos más cercanos, se calculaban distancias en probabilidad, yo miré un poco, está bien hecho, pero no tenía un buen desempeño, tampoco había un buen ataset con el cual entrenarlo, pero no, esto no reemplazaría ninguna etapa de las que tenemos ahora. Le agregas un producto más que es clasificación, de ese dataflow que hoy tenemos de procesamiento, vos te colgas de ahí, sacas la señal Doppler y entregas otro producto, básicamente es eso. ¿Y está pensado construir algún data set? Y yo hago fuerza, hay una idea de empezar a implementar seriamente esto, lo cierto es que hoy por hoy en la explosión de proyectos que hay es medio difícil, pero sí, porque de alguna forma el valor agregado que le terminás dando luego al radar quizás lo vende más, pero todo depende de la aplicación, o sea el control aéreo en general no necesitas clasificar, sabes, que te pegás un avión, un helicóptero, pero sí en control de fronteras o cosas a veces que quieres mirar adentro de un bosque, esas cosas, para esas cosas sí, en general aplicar los radares más chiquititos, son radares más portátiles y que los puedes distribuir a la frontera en general, o a veces para control de instalaciones críticas como industrias o aeropuertos, para detectar drones y discriminarlo entre las aves y cosas así, para eso, por ese lado es por donde más que nada va a salir. Che, muchísimas gracias y de nuevo muy bueno. Bien, bueno, ¿alguno otra más? Bueno, parece que no. Bueno, Franco ahí, yo me enganché, Carlos acá me enganché muy tarde, perdí la primera parada. No, pues nada, todo bien, todo bien. Bueno, ahí está, Carlos, bueno, Carlos estuvo metido ahí con el RACI también. Sí, en algunas épocas ahí. Ahí yo me perdí la primera parte, estas muestras, yo no sé si de un paper, no sé, el origen de las muestras que estaba presentando, ¿de qué radar era? No, no está el detalle del radar, pero sé que es un radar tipo RACI, pulsado en banda X, que las adquisiciones las publicó una universidad de Israel. No sé bien de quién es el radar, pero se ve que a ellos les dieron algún trabajito de clasificación y ellos publicaron. Después no hay mucho más de información, está, y es más, si vos buscas la TACED, o al menos yo busqué, no encontré más que ese, o sea, está todo super confidencial, así que, pero bueno. Mira, ahí comentaron que, no sé si será cierto, pero como una curiosidad, vieron que tiene que ver con el procesamiento de imágenes, el entrenamiento. Sí. No puede ser posible, yo creo que los CAPTCHA estos que nos ponen los métodos de validación humana, no soy robot, esos aparentemente somos parte de la humanidad de un sistema de validación, contribuimos a un entrenamiento, ¿no? O sea, a la vez que estamos haciendo algún trámite, estamos validando alguna imagen a Google, algo por el estilo, ¿no? Sí, sé que se usan, y está publicado que se usan, no sé si todos los CAPTCHA se usan, pero sé que sí, que se usan para etiquetar, más que nada, o validar también, sí. Claro, eso, o sea, sin querer estamos formando parte de un entrenador, digamos, ¿no? Sí, hay gente de Stanford que es, por ahí los que lideran mucho este desarrollo, al menos en el ámbito universitario, y yo hicieron toda una campaña de, es más, paga de clasificación de imágenes y etiquetados, clasificaron billones de imágenes, y pagaron a gente para que etiquete, y en función de eso entrenaron algunos de estos modelos grandes, digamos, ¿no? Porque si no, no lograban buen desempeño, necesitaban muchos datos para lograr buen desempeño y entregar esos modelos tan grandes. Pero eso es interesante, para que se lo... Sí. ...del industria. Bueno, por eso digo, si vos querés desempeños altos, por ahí lo primero que tenés que tener es buena cantidad y buena calidad de datos, al menos para estas cosas. Claro. Así que ahí es fundamental tener un buen data set. Tiene que ser bueno, así que bueno, nada más por este lado, así que muy bueno, Che. Bueno, Carlos, gracias. Gracias. Bueno, Che, no sé si anda celeste por ahí, o no sé si ya no. Si les parece, cortamos por acá, no sé si quedó alguna otra pregunta. Ah, para no mirar el chat. Ah, no, no hay pregunta ahí. Bueno, gracias, Lauti. Sí, buenísimo, muchas gracias. Nada, rojo, y gracias por prestarme la oreja, un rato. Muy bueno, Franco. Y que bueno que hay gente que sabe del tema y también pueda hacer preguntas interesantes para los... Sí, eso ni lo duden. ...que no saben lo que hay. Sí, ni lo duden. O sea, esto es un campo grandísimo, yo te laburé con algunas cositas de esto, estoy metiéndome en otras, pero bueno, cualquier duda que tengan o cualquier comentario, charla la organizamos, yo tengo un problema y lo discutimos. Así que nada, escriben o nos juntamos en alguna reunión. Así que bueno, eso sí. Ah, y buen fin de para todos. Desde Franco, felicitaciones, buen fin de. Hasta luego. Lo vemos. Muchas gracias, muy bueno, Franco. Saludos. Gracias, Chris. Saludos.