 introducción de quien es Germán, para que todos los conozcan. Germán Tejeros, en licenciación informática, he regresado a la Universidad Nacional de la Patagonia, San Juan Bosco, trabajo casi 20 años en la municipalidad de Ushuaia, desempeñándose en diferentes roles dentro del director de la Dirección Informática, además es docente universitario desde hace casi 20 años, desempeñándose en la Universidad Nacional de la Patagonia, San Juan Bosco, en la Universidad Nacional de Tierra del Fuego, y actualmente da clase en la Universidad Nacional de Riones, amante de la base de datos, desarrollador de software Java, feliz, puede desempeñarse en la especialidad de ingeniería de datos dentro de MAP. Bueno, no sé si esto lo escribiste vos Germán, pero... Eso era grotillo eso, pero... Pero bueno, vos decís si corresponde, no? No, si sí corresponde. No sé si le estás feliz. No, no, sí, sí, sí, la verdad que sí. Hace poquito estoy acá en Bariloche y hace poquito estoy en MAP, pero sí, la verdad, estoy muy contento, y más de estar en la especialidad. Bueno, buenísimo. Bueno, lo dejamos con Germán, que nos va a hablar de un viaje de términos, con ciertas herramientas a través de la historia del almacenamiento de datos. Bueno, gracias Jorge. Bueno, la idea no es hacerlo muy tedioso ni muy extenso, pero bueno, en principio la idea es recorrer un poquito la historia de las bases de datos y demás de cómo nacieron, más o menos desde los años 60 hasta ahora. No solo de las bases de datos, sino de los datos en general. Y la idea es que igual, no sé, hacerlo lo más interactivo posible, o sea, abran el micro y me preguntan, charlamos, discutimos, lo que sea. No, no, no esperen a que llegue al final y nada, porque tiran ningún problema. Vamos a tratar de hacerlo lo menos aburrido posible. Bueno, entonces, se ve bien ahí, ¿no? Sí, está bien. Sí, perfecto. Bueno, yo tengo puesta la presentación y no veo el chat, porque el chat me avisa, no abro el micro. La idea es ir un poquito desde los 60, cuando arranca un poquito la cuestión, hasta la actualidad, yendo por varios temas, personas, hay algunos conceptos y herramientas que han aparecido en el camino. Vamos a arrancar por los años 60. Acá el problema, la informática estaba bastante en sus pañales y el principal problema era que casi todo lo que era almacenamiento de datos y manipulación de datos pasaba por lo que es archivos, archivos y programación. Programar 100, 1000 millones de veces los mismos algoritmos para ordenar datos, para buscar en los archivos, para mezclar datos, para procesarlos, como utilizamos datos en formato binario en archivos, la estructura de los archivos que daba impresa en los programas, si había que reestructurar un archivo, era un dolor de cabeza impresionante. Hasta que apareció este hombre, Charles Bachman, trabajando para Xenon Electric, allá por los años principios de los 60, y se le ocurrió dejar de hacer esta tarea tan tediosa y tan repetitiva, y empezar a trabajar en lo que luego se llamaría un gestor de base de datos, el cual nos va a permitir justamente a través de su interfaz, crear, manipular y consultar la base de datos. Todos esos términos vinieron después, digamos, en ese momento obviamente no existían. Años después, Bachman fue reconocido con el premio Turing ACM. Los informáticos no tenemos, digamos, premio Nobel, como otras ciencias, otras disciplinas de la ciencia, como puede ser matemática, física o literatura, eso es arténode ciencia, pero bueno. En cambio, nosotros lo que tenemos es el premio Turing ACM. Bachman fue uno de los primeros, fue el octavo en conseguir este premio, que consiguieron la mayoría de las personas que más aportaron a la ciencia de la computación. Pero el trabajo de Bachman no solo fue desarrollar un gestor de base de datos que se llamó IDS, sino que con él nacieron uno de los primeros conceptos, nació el concepto de base de datos, que antes obviamente no existía. Esa definición es del libro The Date. Lo más importante de mencionar es que cuando hablamos de base de datos, de lo que estamos hablando es de datos, un conjunto de datos que están estructurados, organizados y relacionados. Eso es lo que hablamos cuando hablamos de base de datos. Ahora, para poder almacenar esos datos en nuestra base de datos, para poder estructurarlos, organizarlos y demás, necesitamos de un gestor de base de datos y el gestor de base de datos lo que nos provee básicamente es lo que se conoce como un modelo de datos. La idea es que tengamos una separación entre la organización física de los datos en el disco y lo que el gestor de base de datos nos presenta a nosotros. Entonces, justamente surge ese concepto, el concepto del modelo de datos, que básicamente es la estructura lógica de la base de datos, de cómo se encuentran organizados, estructurados y relacionados esos datos entre sí, separándonos de cómo están físicamente estructurados y organizados. También Bachman, cuando creó IDS, creó el primer modelo de datos que se utilizó, que fue el modelo de redes, que luego con los años el modelo de redes se transformaría en estándar bajo el estándar de codasil, que se utilizó mucho en los tiempos de de cobol y demás. También surgió después el modelo jerárquico también de datos, que son modelos que se dejaron de utilizar en el tiempo, digamos, fue reemplazado después con lo que aparece en los años 70. Y obviamente nació el sistema gestor de base de datos, que básicamente es esa herramienta, ese conjunto de programas y aplicaciones que nos van a permitir crear, estructurar, organizar, reorganizar, manipular los datos de nuestra base de datos. Todo ese trabajo de Bachman permitió crear y crear estos conceptos. Lo más loco de eso, de esos gestores de base de datos iniciales, el IDS de Bachman, luego IBM lanzó el IMS, también nació otro llamado IDMS. Esos gestores de base de datos, no el de Bachman, pero esos dos gestores de base de datos, IDMS y IMS, todavía siguen existiendo. Si bien nacieron a mediados de los años 60, todavía siguen teniendo soporte. En el caso de IMS, es propiedad de IBM, siempre lo mantuvo IBM, pero en el caso de IDMS pasó por varios fabricantes y por varias empresas, pero todavía siguen existiendo y todavía siguen teniendo soporte. Ya llevan unos cuantos años en el mercado. Son históricos, digamos, porque no se hacen nuevos desarrollos con esos, pero siguen existiendo. Para los 70 ya teníamos, si bien no era un uso muy difundido de las bases de datos y los gestores de base de datos, ya teníamos, pero seguían presentando algunos problemas. Esos modelos, el modelo de redes, el modelo jerárquico que se habían impuesto, tenían problemas, se los conoce como modelos navegacionales, porque nosotros para poder ir de un dato hacia otro, teníamos que ir navegando como si se tratara de una estructura, de un grafo o de una estructura, de un árbol binario. También teníamos que ir navegando de un nodo a otro para conseguir el dato que necesitaba, por cómo se estructuraban los datos. Eso es lo que hacía, que si bien no teníamos la estructura en disco de los datos, teníamos una independencia física, la estructura navegacional para conseguir los datos quedaba impresa en los programas. Si nosotros reestructurábamos el modelo por alguna cuestión, nuestra base de datos sonábamos. Todos los programas que accedían a esos datos, como tenían impreso el camino para llegar a ellos, dejaban de funcionar. Y apareció este muchacho, Edgar Frank Codd. Ahí por los 70 él trabajaba en IBM, en la parte de investigación de IBM y publicó un paper, ese que está mencionado ahí, a Relational Model of Data for Large Shared Databanks. Fíjense que ni siquiera se utilizaba demasiado el término base de datos todavía. Y ese trabajo lo que implicó fue que él propuso un nuevo modelo para las bases de datos, que es el modelo relacional, que es el que básicamente hemos utilizado, la mayoría de nosotros cuando utilizamos una base de datos que conocemos como base de datos relacional. Si bien las bases de datos relacionales no implementan exactamente el modelo teórico que él propuso, es bastante parecido y se basa sobre todos esos conceptos que él propuso. Y en base a este modelo se venían otros conceptos que no vale la pena gastar demasiado tiempo en ellos, como lo que tiene que ver con normalización, lenguajes teóricos como álgebra relacional, cálculo relacional, todo el modelo como se basa en principios de la matemática, básicamente en la teoría de conjuntos, en las relaciones, se pudió todo un montón de algoritmos y un montón de conceptos con bases y fuentes bastante matemáticas, digamos, de matemática discreta pero de matemática al fin. Este es el modelo que utilizamos por lo general cuando hablamos de base de datos. Cuando nosotros nos referimos en general con de base de datos, hablamos de este tipo de base de datos, de base de datos relacionales. Si bien en las bases de datos relacionales, entonces hablamos de tablas, hablamos de filas, hablamos de columnas, el modelo teórico habla de relaciones, habla de tuplas, habla de atributos, de dominios, pero todas las bases de datos relacionales se apoyan sobre el trabajo que desarrolló Kod en los años 70. También en los años 70 apareció este muchacho, Peter Chen, que luego se hizo también bastante conocido, no ganó ningún premio. Antes me olvidaba decir también que Kod también, por su trabajo, ganó el premio Turing. Este no, Peter Chen no, pero bueno, por ahí en los 76 propuso en un paper también lo que nosotros conocemos como el ERD, por lo general se le llama. En realidad se llama el modelo de entidad interrelación, hacer los famosos diagramas ERD cuando hacemos diseño o cuando modelamos datos. Fue propuesto por Chen, si bien el trabajo original o el modelo original que propuso Chen allá por los años 70, ya hoy no es tan habitual verlos. Se usan ciertas modificaciones que fueron realizando distintos autores a lo largo del tiempo, quedó como una herramienta que incluso a veces se sigue utilizando hoy por hoy para hacer todo lo que tiene que ver con diseño de datos. La idea de Chen era justamente esa, tener un modelo unificado de alto nivel abstracto independiente de la base de datos en donde uno diseñara y modelara los datos que luego iban a ser implementados quizás en una base de datos. Y por ahí por los años 70 también, a fines de los años 70 todavía no existía un lenguaje unificado para acceder a los datos, IBM estaba trabajando, estaba trabajando en un lenguaje que en ese momento se llamaba SQL y que luego devendría en los años 80 en el lenguaje SQL. SQL se estandarizó más o menos por los años 89, empezó el primer estándar y fue la segunda estandarización que hubo después de codacil en lo que es referido a bases de datos. SQL tuvo otro estándar de 92 que fue bastante conocido y a partir de ese momento cada tres o cuatro años sale un nuevo estándar que va incorporando cada vez más funcionalidad y más herramientas para poder trabajar y manipular los datos y estructurar las bases de datos y más. Siempre dentro de lo que es base de datos relacionales. Más o menos lo que quería graficar es que no nos tenemos que olvidar que a veces cuando hablamos de SQL parece que fuera solamente la sentencia select, en realidad SQL tiene muchas sentencias, ahí solamente están puestas algunas y están puestas en diferentes conjuntos donde dice DDL, se refiere a Data Definition Language que es la parte del lenguaje, las sentencias que forman parte del lenguaje para dar forma y estructurar nuestra base de datos donde después vamos a alojar los datos. El DML es la parte de manipulación de datos, consulta, modificación, insacción. El TCL es la parte de control de transacciones y el DSL es la parte de control de la seguridad sobre los datos. Y también por los años 70 algo que parece, no sé si todos lo sabrán, pero bueno, dos investigadores. Hasta ese momento el acceso a los datos era bastante mal, no había estructuras de disco bastante desarrolladas para acceder a los datos. La performance de las bases de datos hasta ese momento era bastante bastante mala. Se utilizaban las mismas estructuras que utilizan en memoria por lo general que estudiamos cuando estudiamos estructuras de datos como árboles binarios, listas, enlazadas y demás. Hasta que llegaron estos dos investigadores, Bayer y McRae en el 72 y crearon una estructura que se llama el árbol B. No vale la pena entrar demasiado en detalles pero bueno, es un árbol multicamino autobalanciado. La gran diferencia que tiene con un árbol binario de búsqueda que por ahí es más conocido es que es multicamino. Cada nodo puede tener más de dos hijos y además la gran diferencia es que es autobalanciado. Los árboles binarios de búsqueda que ellos que lo estudiaron saben que se desbalancean y es bastante trabajoso llevarlos, volver a balancearlos. Hay varias técnicas, árboles ABL y demás, pero estos árboles tienen la característica de que siempre están balanceados porque en realidad se construyen al revés, no se construyen de la raíz a las hojas, sino se van construyendo a partir de las hojas hacia la raíz. Cuando nosotros en las bases de datos relacionales creamos un índice, los famosos índices, lo que estamos haciendo básicamente es crear un árbol B. Por ahí no un árbol B puro, hay otras variantes que otros investigadores fueron incorporando en el camino, como un árbol B estrella, un árbol B plus y demás. Hay montones de variantes, pero estos sentaron las bases para que justamente las consultas de los datos sean mucho más rápidas y no sean tan lentas como eran en antaño. Y aquí le voy a usar algunas bases de datos estructuradas o relacionales que utilizamos, obviamente que no son de esa época. La mayoría de estas bases de datos surgen de investigaciones hechas en los años 70 y 80, otras son más recientes como por ejemplo María de Veo, MySQL son mucho más recientes, pero Pogre, Ingress, Cibase, SQL Server, Oracle incluso, son todas gestores de bases de datos que tienen ya unos 40 años encima. Todas surgieron de investigaciones de los años 70 y 80, incluso muchas surgen del mismo trabajo de investigación. Si quieren, ahí abajo dejé también un enlace, hay una página que es muy buena, que se llama The Benchains, que tiene un ranking que se va llevando a lo largo del tiempo sobre todos los gestores de bases de datos más conocidos, no solo los relacionales, sino también los más actuales que ahora después vamos a ver. Y ahí se puede ver cómo va yendo la tendencia en el uso y en la demanda de cada uno de estos gestores. Llegan los 80, ya para este entonces se empiezan a popularizar las bases de datos relacionales, ya hablar de bases de datos, ya hablar de bases de datos relacionales, prácticamente no había otra cosa. Y uno de los grandes culpables de todo esto es este muchacho, Michael Stonebreaker. Michael, en la Universidad de Berkeley, llevó adelante un proyecto, él llamó el proyecto Ingress, que es uno de los proyectos que dio lugar a la mayor cantidad de gestores de bases de datos. De ese proyecto de investigación sale el propio gestor de bases de datos Ingress, C-Base, SQL Server, indirectamente sale Oracle, y después con los años crea otro proyecto, como continuación del proyecto Ingress, que fue el proyecto Postgres, que dio lugar a lo que es Postgres, SQL y otros gestores como Fiverr y demás. Si bien indirectamente, no directamente, pero hizo su trabajo de investigación en Berkeley y dio lugar a un montón de desarrollos de gestores de bases de datos, y más actualmente estuvo participando de otros proyectos como Teradata y demás. También es uno de los ganadores del premio Turing de ACM por todas las contribuciones que realizó la industria de lo que tiene que ver con bases de datos. En estos años también aparecen las bases datos distribuidas, hicieron los trabajos de investigación, ya venían un poco antes, pero empiezan a hacerse mucho esfuerzo en la investigación para dejar de tener una base de datos centralizada en un único lugar y tener nuestra base de datos descompuesta, partida, particionada y distribuida en distintas localidades, en distintos nodos. Cuando surge esto de las bases de datos distribuidas, surge principalmente, no surge tanto por los volúmenes de los datos ni por la capacidad de procesamiento, sino surge más que todo por una tema de distribución geográfica de conexión. Si una empresa grande tenía varias supursales, tener una base de datos centralizada el acceso de las sucursales distantes a la central era extremadamente lento. Otra posibilidad era tener que cada sucursal tuviera su base de datos, pero yo después no podía tener una visión única de todos los datos de la empresa. Entonces la idea de esto fue justamente eso, fue tener una única base de datos pero con distintas partes en distintas localidades, que cada localidad pudiera trabajar sobre sus propios datos, pero uno pudiera tener una vista unificada de toda la base de datos. Por eso dice que es una colección de datos, una base de datos común, pero está integrada lógicamente, lo vemos como una única base de datos, aunque físicamente distintas partes de la base de datos están distribuidos en diferentes nodos y conectados por una red de computadoras. No se popularizaron en esta época, pero habían empezado los desarrollos y las investigaciones que llevaron bastante tiempo. Y así como tenemos gestores de base de datos para las bases de datos tradicionales, tenemos gestores de base de datos distribuidas para justamente dar lugar a bases de datos distribuidas. Dentro de lo que son bases de datos distribuidas tenemos varias clasificaciones, lo que son con homogéneas y heterogéneas, las homogéneas son porque en cada uno de los nodos donde se encuentra la base de datos tenemos un gestor de base de datos y son todos iguales y todos se comunican entre sí. En cambio en heterogéneas también tenemos un gestor en cada nodo, pero pueden ser de diferente fabricante o diferente tipo. Puedo tener en un nodo un MySQL y en otro un PostgreSQL. Sin embargo, puedo tener una vista unificada de todos los datos y puedo hacer que ellos interoperen y cooperen entre sí, sin perder la independencia que tiene cada uno de ellos. Y por estos años también se populariza uno de los primeros roles de lo que fueron las bases de datos. Todavía no había este concepto de distintos roles o distintas especializaciones, o por lo menos no estaban tan popularizados, pero uno de los que sí se populariza por este entonces es el rol del DBA, del famoso administrador de base de datos, que es la persona que se encarga justamente de administrar la base de datos, de llevar adelante tareas administrativas, de verificación de la performance, de corrección, de verificación de los logs, de verificación de errores, de establecer la organización física más adecuada para solventar los problemas que lleva adelante la empresa. Es un rol que se populariza bastante y si bien, vamos a ver, que a largo del tiempo un poco se fue iluyendo, todavía sigue bastante vigente. Más cuando hablamos de bases de datos un poco más grandes como ORACEL y demás, que necesitan de mantenimiento y no dejarlas de forma desatendida. Son cosas que hay que monitorear, la performance, monitorear el rendimiento, la capacidad y cómo va operando en el tiempo. Ahí dejé, estos son gestores actuales, obviamente no son gestores de esa época, que permiten implementar bases de datos distribuidas. Los que están del lado izquierdo son para bases de datos distribuidas homogéneas. En realidad son extensiones o modificaciones de bases de datos como POSGR o como MariaDB para volverlas distribuidas, porque de por sí no son distribuidas, son centralizadas. Y el lado derecho, en realidad, Tade C es un gestor de bases de datos para bases de datos distribuidas heterogéneas, para hacer lo que se conoce como bases de datos federadas. Presto de SQL o PrestoDB en realidad no tanto, es más que todo un ejecutor de consultas distribuidas, pero no sigue tanto los conceptos de bases de datos distribuidas en el sentido de que yo no tengo una vista unificada, lo que se conoce como un global concepto al esquema, un esquema único global y de ahí puedo consultarlo y las consultas se derivan a las localidades donde se encuentran los datos, sino más bien lo que me permite es tomar datos de diferentes fuentes y mezclarlos y combinarlos para obtener resultados. Pero bueno, está dentro de la tónica más o menos. Son herramientas bastante utilizadas, principalmente Presto y Citus también. Llegaría a los años 90 y con los 90 llegaría a uno de los grandes fracasos del mundo de los datos y de las bases de datos que fueron las bases de datos orientadas a objetos. Como durante los años 80 se habían popularizado bastante los lenguajes de programación orientados a objetos, principalmente C++, empezó a tomar cuerpo, empezó a tomar vida y empezó a tomar mucha popularidad, la idea fue llevar lo mismo al mundo de las bases de datos. ¿Por qué no tener una base de datos que implemente un modelo orientado a objetos? Si bien la idea a priori parecía bastante buena, fue un fracaso. ¿Por qué? Por varias razones. Una fue porque cuando surgieron ya las bases de datos relacionales dominaban el mercado, estaban recontraestablecidas, la usaba todo el mundo y las bases de datos orientadas a objetos realmente no aportaban nada nuevo. Recién surgían, estaban nobles, estaban verdes, todavía no estaban maduras y carecían de un montón de características que las otras ya sí tenían. Y después que además caían en ciertos problemas que se habían solucionado justamente con las bases de datos relacionales, que era por ejemplo que las bases de datos orientadas a objetos son navegacionales. Entonces si yo reestructuro mi base de datos orientada a objetos, puede que muchos de los programas que acceden a esos objetos dejen de funcionar. Y además los estándares, que hubo estándares de bases orientados a objetos, de lo que fue la ODMG, llegaron bastante tarde. Así que calculo que de los que están acá, ninguno o muy pocos han interactuado alguna vez con una base de datos orientados a objetos. Por lo que es lo que terminamos haciendo, porque sí tenemos una desigualdad y impedancia entre un lenguaje o un programa escrito en un lenguaje orientado a objetos y una base de datos relacional, son dos mundos diferentes para hacerlos convivir entre sí. Lo que por general se utiliza es un ORM o una capa de abstracción que me permita hacer que el mundo de la base de datos relacional parezca un mundo orientado a objetos. Si bien quizás no es la mejor solución del mundo, es la que más se terminó imponiendo en el tiempo, y las bases orientadas a objetos pasaron al olvido. Pero también durante los años 90 surgió otro problema. ¿Qué pasó? Ya las empresas grandes, las organizaciones grandes de aquel tiempo, que eran principalmente las que se dedicaban a la venta de mercaderías o la producción de productos, habían acumulado datos durante mucho tiempo. Las bases de datos distribuidas todavía no se habían popularizado, todavía no había desarrollos lo suficientemente estables, entonces habían aparecido empresas grandes con muchas bases de datos desperdigadas por muchos lugares y con la necesidad de combinar todos esos datos, de juntarlos para poder analizarlos, para poder realizar análisis y reportería de todos los datos de la organización. Y ahí aparece dos personas. Uno es Bill Inmond, que es justamente el padre de lo que se conoce hoy por él como Data Warehouse, que es más, es él el que lo bautizó. No tiene traducción Data Warehouse. Al principio en algún libro de la vieja época y traducido al español de España, aparecía como almacén de datos, pero la verdad que no tiene traducción, se utiliza de esa forma, Data Warehouse. Y lo que brinda es toda un enfoque de diseño de este almacén centralizado de todos los datos de la organización a partir de muchas fuentes de datos distintas y dispares. Y lo que él, digamos, encara es un diseño más bien de tipo top-down. Pero aparece otra persona que es Ralph Kimball, que es el otro padre de Data Warehouse, varios años después, y propone un enfoque completamente diferente. Es un enfoque que se conoce como button-up. Lo que propone en realidad es no construir todo el Data Warehouse de una, como proponía Inmond, que lo llamaba la única fuente de verdad y tenía que ser lo que se conoce como un Enterprise Data Warehouse, un Data Warehouse de toda la organización. Lo que propone Kimball es ir construyendo pedacitos, pedacitos sectoriales del Data Warehouse, que se conocen como Data Marts, y después combinar todos los Data Marts, todos esos pedacitos que se fueron construyendo, combinarlos y formar el Data Warehouse. Este enfoque fue bastante más difundido porque era bastante más viable de poder realizarlo y bastante más utilizado. Ahí hay un pequeño esquema de un Data Warehouse básicamente de cómo se construye, digamos, a partir de varias fuentes distintas, principalmente fuentes que son, o en esa época eran bases de datos relacionales. Se utiliza un ETL, se utiliza herramientas para extraer, realizar transformaciones y cargar los datos en el Data Warehouse. Muchas veces se utiliza un área intermedia que se conoce como el área de staging o el área de ensayo para extraer directamente los datos tal cual como vienen de las bases de datos relacionales y en el área de ensayo realizar las transformaciones necesarias porque las transformaciones muchas veces son muy costosas e interferirían sobre los sistemas que están operando sobre las bases de datos de las cuales extraemos los datos. A partir de eso vamos construyendo los Data Marts y esos se van combinando formando el Data Warehouse, que básicamente es un almacén centralizado donde están todos los datos de la organización. Pero con una diferencia que es que la estructura no está pensada como los sistemas normales de base de datos que están pensados justamente para insertar y modificar y consultar, sino están pensados simplemente para consultar. Está pensado para agregar información, agregar datos que nunca se modifican y luego consultarlos, consultarlos con consultas digamos que involucran gran cantidad de datos, consultas que resumen, que sumarizan los datos que están ahí almacenados. Hay dos definiciones de cada uno de los autores de Eamon y de Kimball, pero básicamente si bien las palabras son diferentes, las ideas que están atrás son bastante similares nada más que ambos tomaron un enfoque diferente. Y Kimball lo que proponía que es un enfoque bastante también popularizado para Data Warehouse, es si ahora adelante un modelo dimensional para estructurar los datos en el Data Warehouse. El modelo dimensional estructura la información y los datos que están almacenados de una forma diferente. Básicamente se piensa en dimensiones y en medidas. Uno cuando quiere consultar algo, cuando quiere analizar algo, lo que está buscando es analizar determinadas medidas. Esas medidas se organizan en lo que se conoce como una tabla de hechos en la que se va registrando históricamente y cronológicamente, que es los valores por los que va tomando esa medida, que puede ser por ejemplo la cantidad de ventas. Y las dimensiones son las que nos permiten agregar o desagregar datos de esa medida. Por ejemplo, analizar la cantidad de ventas por año, por mes, por trimestre, por sucursal, por tipo de producto que se vendió. Entonces la estructura básicamente es una tabla de hecho central con varias tablas de dimensiones que nos permiten estructurar y con distintos niveles de agregación. Por eso se lo conoce muchas veces como el esquema estrella, porque termina formando una especie de estrella. Y en esta época también se empieza a diferenciar justamente con el nacimiento del Data Workhouse, se empieza a diferenciar en lo que se llama OLTP y OLAP. Básicamente entre lo que es el procesamiento de transacciones que se realiza con bases de datos relacionales, en donde la organización guarda y procesa la información de las transacciones, por lo general ventas y demás. Y lo que es OLAP, que es el Procedamiento Analítico en Línea, que básicamente se realiza principalmente con Data Workhouse y lo que busca básicamente es realizar consultas y analizar la información que está almacenada. También se empieza a ver de la pirámide. En aquel tiempo muchas se hacía referencia a la pirámide de información, hoy se la conoce más como la pirámide de DIG. Porque se agregó el último nivel de sabiduría, que al principio no estaba, se llegaba hasta conocimiento. Esta pirámide muestra un poco la relación de cantidades y de nivel de agregación. Los datos son el menor nivel de agregación, hay mayor cantidad, y los posteriores son niveles de mayor agregación y de mayor complejidad. Básicamente cuando hablamos de datos, lo que estamos hablando es de la representación de un hecho. Cuando hablamos de información, ya hablamos de un conjunto de datos que están organizados y tienen un valor adicional. Cuando hablamos de conocimiento, ya estamos hablando sobre la comprensión, sobre el conjunto de información. Y además, poder convertir eso en algo útil. Y cuando hablamos de sabiduría, lo que estamos hablando básicamente es del juicio óptimo que se refleja. Básicamente lo que refleja es un profundo conocimiento sobre los hechos. También se empiezan a distinguir distintos tipos de analítica. En ese momento, principalmente la analítica que se hacía era de tipo descriptiva, y empiezan a aparecer otro tipo de analíticas. Se conoce como ética predictiva, diagnóstica predictiva y cognitiva. Hay un poco más de detalle. Cuando hablamos de descripción, básicamente lo que estamos analizando es lo que pasó. No sabemos por qué pasó, pero si analizamos por qué pasó, cuánto vendimos hasta ahora, cuáles fueron las ventas del mes pasado, de este mes, del año pasado. Eso lo hacemos principalmente con analítica, con reportería clásica. Ahora, por qué ocurrió es otra cosa. Ya es determinar cuáles son las causas de lo que está mostrando la analítica descriptiva. Ya ahí requiere conocimiento. La analítica predictiva básicamente nos permite determinar qué es lo que va a pasar. Si esto se comportó de esta forma hasta ahora, bueno, esto es lo que va a pasar si todo continúa más o menos como viene. La perspectiva básicamente nos permite intentar o determinar cuál sería el mejor camino a tomar para conseguir el mejor resultado, y la cognitiva, qué podemos mejorar o automatizar para sacar el mejor partido. Y también en los 90 surge o se populariza básicamente, o se empieza a popularizar lo que es la minería de datos. La minería de datos básicamente es el descubrimiento de patrones o de conocimiento de los datos. Ahí hay una pequeña comparativa con lo que es la analítica de datos y la minería de datos. Justamente la función principal es identificar esos patrones, identificar ese conocimiento y darle un uso. Hay mucha metodología para utilizar principalmente dentro de lo que es estadística, aprendizaje automático, descubrimiento de patrones, hay un montón de cosas. En cambio, la analítica justamente lo que nos permite hacer es construir modelos y visualizaciones. Pero ya en el 96, uno de los autores, le llamamos Fahyad, lo que nos contaba es que la minería de datos es solamente una etapa dentro de un proceso mucho más grande que se conoce como descubrimiento de conocimiento en bases de datos. Acá cuando a él se refirió a bases de datos, se refería a datos, no se refería a bases de datos en particular. Él lo llamó el KDD, el Knowledge Discovering Databases, pero sería Knowledge Discovering en datos básicamente. Y nos decían, mira, primero tenemos que conseguir los datos, después seleccionar los datos, procesarlos, transformarlos, y recién ahí, una vez que tenemos listo lo que antes se llamaba la vista minable, esa tabla a la cual le vamos a aplicar el algoritmo o el método de minería, recién ahí podemos hacer minería y obtener los patrones, evaluarlos, interpretarlos y utilizarlos. Y lo que nos decía era, fíjense que el esfuerzo justamente para crear, para transformar esos datos y llegar hasta la vista minable a la cual le vamos a recién aplicar los algoritmos o los métodos de minería, es más o menos como paleto, un 80-20. Es mucho más grande el esfuerzo que necesitamos para conseguir los datos y llevarlos al punto anterior a aplicarle el algoritmo de minería, que luego realizar la minería y evaluar los resultados que obtenimos y hacer una difusión. Acá está un poco por qué después termina surgiendo la ingeniería de datos. Algunas herramientas que hoy por hoy se utilizan para lo que es construcción de data warehouse, arriba son bases de datos para manejar grandes volúmenes de datos distribuidos y construcción de data warehouse como el Grimplume o Snowflake y los de abajo son herramientas para hacer la gtl, para hacer la extracción, la transformación y la carga de los datos provenientes de las distintas fuentes en los data warehouse como es Apache en iFi o Pentajobo. Luego vinieron los años 2000 y acá el problema fue básicamente Internet. Ya había surgido Internet, habían surgido grandes empresas de Internet como Google, Yahoo, Altavista y empezaron a tener montones de problemas para manejar los datos. Y empieza a surgir, no justamente en esa época, lo que posteriormente se fue a llamar Big Data. Nunca un término fue tan sobreutilizado como Big Data. Se lo utiliza para cualquier cosa, menos muchas veces para lo que la realmente es. Básicamente Big Data lo que implica es que tenemos un conjunto de datos que es tan grande y tan complejo que no podemos solucionarlo o que no podíamos solucionarlo con las herramientas que estábamos acostumbrados a manejar los datos hasta ese momento. No podemos manejarlo con una base data relacional, no podíamos manejarlo en mi seguida con un data warehouse. Entonces necesitábamos aplicaciones, herramientas no tradicionales para poder tratarlos. Principalmente se entiende el Big Data si lo vemos desde el punto de vista de lo que se conoce como las 3B, las 3B de Big Data. Hoy por hoy si buscan van a encontrar y les va a aparecer las 4B de Big Data, las 5B de Big Data, las 6B de Big Data, las 7B, las 24B de Big Data. Siempre le van agregando nuevas B cortas como si fuera una especie de moda. Le agregan valor, viscosidad, visualización, visibilidad, todas las B cortas que se le puede ocurrir. Pero inicialmente fueron estas las B. Las otras B o las B que le fueron agregando siempre estuvieron como un problema, pero se agravaron básicamente con estas 3B iniciales que es el volumen, la velocidad y la variedad. El volumen de los datos ya no podíamos manejarlos ni siquiera con un data warehouse. Ya las cosas no entraban en un único disco por más que me comprara el disco más grande que existiera. Los datos venían en una velocidad tan grande que no podía manejarlos con herramientas tradicionales y la variedad de los datos ya no eran datos simplemente estructurados que yo podía guardar en la base de datos, sino parecían otros tipos de datos que hasta ese momento se habían despreciado. O no teníamos herramientas adecuadas para poder trabajar con ellos. Bueno, más de lo mismo. Y si bien no hay un punto donde uno diga acá nació Big Data, yo siempre menciono estos tres papers porque fueron para mí fundacionales que publicó Google en su momento, dando tres soluciones a principalmente los problemas del volumen y de cómo procesar ese volumen de datos. En 2003 publicó de Google File System donde propone básicamente un sistema de archivos distribuidos justamente para albergar y para poder dar lugar a ese volumen enorme de información que teníamos. Después en 2004 publica MapReduce donde publica justamente una arquitectura para procesar ese gran volumen de datos que íbamos a almacenar en el Google File System. Y luego en 2006 publica Bigtable que lo que permite básicamente es procesar grandes volúmenes de datos en un formato estructurado, que hasta ahí no estaba bien claro cómo se iba a hacer. Google publicó sus papers y trabajó sus herramientas de puerta adentro. Para esa época este muchacho Donald Cotting trabajaba para Google, luego se marcha a trabajar a Yahoo. Yahoo obviamente tenía los mismos problemas que tenía Google y Cotting se pone a trabajar, ya había empezado a trabajar antes pero continúa trabajando ahora sí el tiempo completo en crear implementaciones para estas propuestas que había hecho Google porque Google no había liberado sus herramientas. Google tenía sus herramientas internas, publicó los papers y ahí quedó la cosa. Así es como nació el proyecto Hadoop que ahora es parte de Apache Software Foundation. Es más, el elefantito, después vamos a ver que Apache Hadoop tiene un elefantito amarillo, básicamente viene porque era el nombre de la mascotita del hijo de Donald Cotting que se llamaba básicamente. Entonces básicamente lo que propusieron es, che, tenemos que manejar el volumen, ¿cómo hacemos? Un sistema de archivos distribuidos. Así como tomaron las ideas de lo que había sido las bases de datos distribuidas, que todavía seguían sin popularizarse demasiado, pero ahora teníamos un sistema de archivos distribuidos donde yo podía alojar los datos, esos datos iban a parar a diferentes localidades, a diferentes ubicaciones, a diferentes nodos, incluso podíamos fragmentar ciertas partes en ciertos nodos replicados para dar data de disponibilidad y demás, pero yo tenía la visión de que era un solo sistema de archivos como si estuviera localmente mi máquina trabajando. Y así es como surge el concepto de Data Lake. Básicamente cuando hablamos de Data Lake lo que estamos hablando es eso, es de un sistema de archivos distribuido donde pongo mis datos principalmente en forma nativa, así en el mismo formato en el que fueron generados. Los depositamos ahí para que luego puedan ser procesados, analizados por quien lo necesite. Lo que pasa es que esa simpleza del Data Lake llevó varios problemas y se empezó a hablar de lo que se llamaba el data swamp, porque está bárbaro, yo podía solucionar el problema del volumen, pero no tenía metadatos, no tengo nada de lo que se llama gobernanza de datos, no sé qué datos están ahí, qué estructura tienen, dónde están, el volumen se empieza a ser enorme. Entonces después termina siendo una cínaga donde están todas las cosas tiradas y cuesta recuperarlas y uno no sabe dónde ir a buscarlas. Eso se le intenta dar solución, pero la solución viene principalmente más en esta época, ahora después vamos a ver, para tener un buen Data Lake. Y se incorpora el Data Lake dentro de lo que era la estructura del Data Workhouse. También se puede usar el Data Lake solo o como parte del Data Workhouse, como una fuente más del Data Workhouse, o incluso como el área de ensayo de un Data Workhouse. Pero principalmente en el Data Lake, los datos que se guardan van en un formato nativo, van en el mismo formato en el que fueron generados y se guardan. También esto se hace para atacar la velocidad, como llegan a tal velocidad de los datos, no hay mucho tiempo para darles otro formato y reformatearlos, estructurarlos y poder guardarlos. Y además, como muchas veces no se sabe de antemano qué uso se le va a dar, por ahí los estructuramos y los organizamos, gastamos tiempo innecesariamente porque después no sirven y lo que mejor sirve es guardarlo nativo y cuando lo voy a recuperar ahí darle formato. Por eso surge lo que empezamos a llamar Schema on Read y Schema on Write. El Schema on Write es básicamente lo que hacíamos con las bases de datos estructuradas o con muchas bases de datos. Cuando voy a guardar el dato, lo estructuro a una estructura que ya está predefinida y preconcebida y guardo los datos que quiero guardar en esa estructura ya preconcebida. Como ahora en el Data Lake yo guardaba los datos en forma nativa al intentar procesarlos, en ese momento le doy la estructura que necesito de acuerdo al análisis o al procesamiento que quiero realizar, pero no modifico la estructura original del dato. Siempre lo dejo un dato en el formato crudo que se generó y solo le doy el formato al momento de recuperarlo. Bueno, y esto es una estructura más o menos de cómo funciona el MapReduce con un ejemplo del famoso, es como el hola mundo del MapReduce es el word count, el contador de palabras. De cómo se leen los datos, se particionan, se hace un soufflé, se mezclan y se reducen. Pero no sé si vale la pena perder demasiado tiempo en esto, pero esto es básicamente lo que hace un MapReduce. Y también empiezan a aparecer, que ya habían aparecido a finales de los 90, pero para atacar también el problema de la velocidad, empiezan a aparecer los brokers de mensajería o los buzz de mensajería. Ahí hay que distinguir un poquito entre lo que es mensajes y procesamiento de flujos. Por general, las plataformas o las herramientas para procesar mensajes están más enfocadas en la entrega del mensaje, en si se entregó o no se entregó, si queda persistido o no queda persistido hasta que alguien lo consuma, si tiene hack knowledge de recibido o no tiene. En cambio, en el de procesamiento de flujos, la idea básicamente ahí es que todos esos mensajes que vienen forman un todo y yo necesito procesarlos y generar nuevos flujos a partir de ellos. Pero bueno, son otras de las herramientas que aparecen para atacar el problema de la velocidad en este caso y de la variedad. Y cuando hablábamos de variedad, básicamente lo clasificamos en tres partes. Lo que es datos estructurados, no estructurados o semi estructurados. Cuando hablamos de estructurados, hablamos de las bases datos tradicionales tipo data warehouse o bases datos relacionales, donde ya tienen una estructura fija y los datos sí o sí tienen que encajar dentro de esa estructura. Los no estructurados no tienen una estructura fija y siempre la misma, como es el texto libre, el video u otro tipo de dato que surja de analógico. Y los semi estructurados son aquellos que tienen cierta estructura no tan fija, más bien variable y que por lo general viene taggeado junto con el dato a qué atributo o a qué corresponde ese dato para poder luego al recuperarlo saber más o menos la estructura que tiene eso, como pueden ser los datos de formato JSON, XML y demás. Y aparecieron también en esa época para atacar también el problema de la variedad lo que son las bases datos que se encajillan dentro de lo que es no SQL. No SQL no significa que no tiene SQL, sino significa que no solo es SQL. Muchas de las bases datos que son SQL también soportan SQL. Básicamente lo que querían decir es que no son relacionales. Acá básicamente tenemos dos cosas con bases datos no SQL. El modelo de datos que presentan, que justamente no es relacional. Tenemos modelos clave valor, white column store o column family, orientadas a grafos, orientadas a documentos y además la alta disponibilidad y la escalivaridad horizontal, que era un tema que no tenían las bases datos relacionales por lo general. Acá hay varios actores y faltan algunos como por ejemplo las bases orientadas a series de tiempo, las bases orientadas a búsqueda como puede ser Elastic. Y surge también esta época con este autor que es Eric Bremer, un teorema que se llama el teorema CAP que está bastante sobrevaluado, pero más o menos lo que nos permite ver es que no podemos obtener todo. Las bases datos relacionales estaban muy concentradas en las consistencias, entonces después cuando queríamos darle alta disponibilidad y tolerancia al particionamiento de red era bastante complejo hacerlo. Lo que propone Bremer en su CAP teorema o en su teorema CAP es que no podemos obtener las tres cosas, lo cual no es tan cierto, pero algo de razón tiene. Si uno quiere puede elegir dos de esas, pero no las tres. Entonces si vos obtenes alta disponibilidad y tolerancia a particionamiento, perdés la consistencia, es lo que ofrecen mucho de las bases datos no SQL. Esto con el tiempo ha ido cambiando, no es tan así, pero sirve como herramienta. Hay un poquito más de lo mismo. Hay unas herramientas que surgieron desde lo que es el mundo de Big Data, como Apache Hadoop, Apache Hive, que permite construir data warehouse sobre plataformas distribuidas, Apache Spark, que es como la evolución de Apache Hadoop para procesamiento de grandes volúmenes de datos, Apache Kafka para ver un procesamiento de flujos en tiempo real, otros sistemas de archivos distribuidos como Mini-O, algunas herramientas para almacenar datos en nuestro Data Lake, algunos formatos serían principalmente y las herramientas para poder procesarlos como Parket, ORC, Abro. Abro también permite hacer serialización y des-serialización cuando uno transmite información de un punto a otro orientada a registros. Y algunas herramientas no SQL como Redis, que es una base de datos clas de valor, Neo4j, una base de datos orientada a grafos, Casandra, una base de datos orientada a columnas. Una diferencia grande en las bases orientadas a columnas con respecto a las relacionales, si bien la abstracción que me muestran son tablas, esas tablas no se pueden reunir con otras como el modelo relacional, pero sí soportan montones de columnas. Por ejemplo, una base de datos Casandra, una tabla, una base de datos Casandra puede tener aproximadamente dos mil millones de columnas tranquilamente. En cambio, una base de datos relacional tradicional, ponerle dos mil columnas sería una locura. Mongo, que es una base de datos orientada a documentos, Elastic también puede ser orientada a documentos o es un buscador, también permite hacer rápidas búsquedas. Bueno, y en la actualidad, no quiero ir mucho del tema, ya me estoy diciendo demasiado lento, perdón. Básicamente algunas cosas que se hablan hoy por hoy es dejar de utilizar este enfoque de OLTP por un lado o lab por el otro y tener herramientas que permitan un enfoque más híbrido, procesar al mismo tiempo carga transaccional y carga analítica sin tener que andar construciendo data huérgada o sin tener que andar haciendo un ETL. Esto va en camino, pero todavía estamos lejos. También se habla de new SQL, las bases de datos relacionales, tradicionales, no se quedaron quietas viendo cómo las herramientas no SQL le robaban el mercado y empezaron a adoptar parte de las soluciones que tenían las herramientas no SQL, tanto en el modelo de datos como la escalabilidad horizontal, la alta disponibilidad y demás. Lo mismo hicieron las herramientas no SQL, tampoco se quedaron quietas y también adoptaron cuestiones de las bases tradicionales, relacionales, como puede ser transacciones. Por ejemplo, Mongo 4 soporta transacciones. Mongo 5 también soporta no solo la gestión de documentos, sino también series de tiempo, con lo cual de a poco empieza a haber una especie de convergencia y las herramientas van captando características que tenían las otras y cada vez todo se parece más con todo. Y surge un nuevo concepto, que es el concepto del Lake House. En vez del Data Workhouse y en vez del Data Lake, el Data Lake House, que básicamente es justamente unificar todas esas dos plataformas en una única herramienta. O sea, dotar al Data Lake de gobernanza de datos y metadatos que me permitan llevar en el Data Lake mismo todo lo mismo que hacía fuera del Data Lake como en el Data Workhouse. Darle la posibilidad de tener consistencia, de tener transacciones, de tener evolución en el tiempo y demás. Esta es una de las propuestas que también que propone uno de los fabricantes, o uno de los que propone el Data Lake House, es la gente de Databricks, que tiene una herramienta llamada Delta Lake, que no solo es darle gobernanza de datos y metadatos, sino también manejar un esquema que fluya de calidad de datos, desde los datos puros, crudos, sin procesar, hasta datos cada vez más refinados y con mejor calidad para que luego puedan ser consumidos de una mejor manera. Bueno, se habla mucho de gobernanza de datos por todo lo que pasó con el Data Lake. Cuando hablamos de gobernanza de datos, estamos hablando justamente de que tener metadatos sobre esos datos, saber qué son, dónde están, dónde ir a buscar, los qué contienen, tener un buen modelo de esos datos, tener calidad sobre esos datos, tener seguridad sobre esos datos. Todo cosas que cuando teníamos, cuando solo usábamos base de datos relacionales, quizás lo teníamos, pero luego con las nuevas herramientas y la falta de madurez que tenían, se fue perdiendo un poco más el camino. Y surgen nuevos roles. Se empieza a hablar del data science, principalmente, y del data engineer, que son roles que a veces se confunden, pero son bastante disímiles. Uno está pensado para la construcción de los pipelines, para obtener los datos, procesarlos, estructurarlos y dejarlos listos para que los otros realicen análisis y construyan modelos de minería de datos o modelos de aprendizaje automático para aplicar o para obtener conocimiento. Y cada vez empiezan a surgir más roles. Se empieza a hablar del ingeniero de calidad de los datos, el modelador de los datos, que ese es un rol viejo, pero se termina reflotando. El ingeniero de inteligencia de negocio, el arquitecto de datos, cada vez se empiezan a aparecer nuevos roles y empieza a aparecer más especializaciones para dedicarse a ciertas tareas. Un poco también se superponen en algunos roles entre sí, pero la idea es como que cada vez más complejo el manejo de los datos y cada vez requiere más tareas y más profesionalismo, se empieza a realizar roles más especializados. Algunas herramientas de construcción de data lake house. Ahí tenemos algunas. Trino o presto también para hacer consultas distribuidas sobre nuestros datos. Delta EO, Apache Hudi o Dremio son herramientas propiamente pensadas para la construcción de data lakes sobre sistemas de archivos distribuidos como puede ser HDF o Mini-O o un S3 compatible. Bueno, y acá simplemente había puesto un estado del arte que sacaron los chicos de Lake FS mostrando la cantidad y la variedad de herramientas que hoy por hoy se utilizan en todo lo que tiene que ver con la ingesta, la transformación y el almacenamiento de los datos. No es exhaustivo, obviamente, pero muestra un poco el panorama de qué tan amplio está el abanico de herramientas que se utilizan o que las que uno tiene que lidiar en el camino. Y algunas cosas interesantes que por ahí están relacionadas o no están del todo relacionadas. Que Rescaté es un recorte, obviamente, todo es un recorte bastante subjetivo. Y ASQL es una herramienta que está promoviendo Amazon, así como tenemos infraestructura como código o infraestructura como configuración. Ahora Amazon lo que está proponiendo es infraestructura como ASQL. O sea que toda la infraestructura que vive la nube yo la vea como si fuera una gran base de datos y borrando filas o insertando filas en tablas lo que estoy haciendo es desplegando y eliminando objetos de la nube que estoy administrando. Bacapear mi base de datos sería bacapear el estado de mi nube y restaurar la base de datos sería restaurar el estado de la nube que estoy administrando. GraphQL y Language es el estándar que se viene. Recuerdo que el año que viene ya va a estar como parte del estándar de SQL. No solo va a ser un estándar para bases de datos relacionales, sino también para bases de datos orientadas al grafo. Son después de muchos años las que van a obtener un nuevo estándar. PartiQL también es un estándar que está empezando a promover o a agitar un poco la gente de Amazon, como las bases de datos orientadas a documentos como Mongo y Couch, Debea y demás. Uno tiene un estándar y cada uno maneja un lenguaje de consulta completamente diferente. Están intentando promover un estándar bastante SQL-like. Y cada SQL-DB, que es bastante nuevo, es de los creadores de Apache Kafka, que permite crear una base de datos sobre Kafka, en donde los datos no viven en archivos, sino los datos viven en los flujos y se calculan en tiempo real a partir de los datos que ingresan por cada uno de los flujos. Es un recorte totalmente subjetivo. Y nada más, perdón si le hice muy larga. No fue mi intención. La verdad que estuvo muy interesante, muchas cosas que yo no sabía, está buena. Hacemos preguntas, sería preguntas que quieras preguntar. Sí, sí, no hay ningún problema. Perdón si le hice muy larga. Me pongo el tema. No, no pasa nada. Buenísimo, Chela Charla, excelente. Te agradezco por la cantidad de temas que hay asociados. Sí, y es un recorte totalmente subjetivo, podría haber sido otro recorte completamente diferente. Está buena. ¿Alguna pregunta para Germán? Como quiera, no hay ningún drama. Ahí está, se prendió la cámara, está ejercitando a full. Sí, me diste tiempo, me diste tiempo para ponerme en forma. No, nada, ¿cómo es el futuro de las bases de datos, digamos, orientadas a blockchain, lo debo así por el espacimiento, para utilizarla como base a todos, relacionadas con comunes? La verdad es que conozco bastante poco de blockchain. Sé que se está utilizando bastante, conozco un poquito de oído. Lo que me parece que el problema que tiene blockchain es el procesamiento, justamente, de sí mismo. Requiere una cantidad de procesamiento, mucho, muchas de las herramientas muy modernas. O sea, antes se cuidaba mucho de lo que es el procesamiento, el uso de los recursos, la memoria, el disco y demás. Ha pasado con los lenguajes de programación históricamente y demás. Y ahora, con el correr del tiempo, un poco se desprecia y la cantidad de procesamiento que requieren es enorme. Yo no sé cuánto va a evolucionar en el tiempo de blockchain o hasta cuándo se van a bancar los recursos de nuestro planeta para procesar todo lo que hay que procesar. No sé si es viable. A futuro, la verdad que no sé. Pero sí es una herramienta fabulosa que está utilizando cada vez más. No sé, no sé hasta dónde va a ir eso. Gracias, quería... explicar. Sí. Germán, como tenía una consulta, a ver, tantas cosas interesantes acá. ¿Cuánto de esto se está tratando de usar en Inbap a aprovechar, digamos, el beneficio? Yo entiendo que a lo largo de los años han dado información en un montón de bases de datos, información de formato. Ahí no sé con los chicos de sistemas de información. Yo no... como soy parte de la especialidad de ingeniería de datos y, digamos, trabajo en proyectos, no sé qué están haciendo ellos, porque todas estas herramientas o muchas de estas herramientas son más para los chicos de sistemas de información que para muchos proyectos. Algunas de esas herramientas estamos intentando utilizar, más que la especialidad es nueva y bastante joven. Lo que pasa es que no todas aplican de la misma forma. Ahora sí, lo que estamos intentando hacer es en el proyecto de los radares meteorológicos, es utilizar herramientas de tipo ETL y orquestado de ejecución de tareas para obtener los datos radar, transformarlos, almacenarlos y catalogarlos. Es un proyecto que quizás... no sé si es uno de los primeros donde estamos intentando meter unas herramientas así. Yo hice algunas también, algunas investigaciones para lo que es la SST, pero algunas de estas herramientas están buenas, son muy buenas, pero de vuelta están pensadas muchas para tener un cluster de Facebook, de no sé, 10.000 servidores y tirarle cosas ahí y nosotros no tenemos las capacidades de cómputo para para tirar cosas allá arriba. Entonces a veces es muy difícil usarlas. Son de fuerza bruta muchas veces, no son tan tan refinadas. ¿Alguna pregunta más? Sí, al principio una... Perdón. No, yo quería agregar un comentario ahí a lo que decía Carlos en sí, yo trabajo ahí en sistemas de información y actualmente estamos trabajando en un proyecto de integración de toda la información que tenemos en la empresa en sí, asociada tanto a los proyectos como a los centros de costos en sí, para poder armar en realidad un data warehouse en sí. En principio lo que se va, se está tratando de implementar es utilizando MicroStrategy y después estamos viendo de implementar también una base de datos como Greenplamo en sí, es decir, una base de datos orientada a data warehouse. Bien, flor de desafío. Sí. Sí, sí, sí, sí. Lindo, lindo desafío. Lindo, lindo, toda la información que tenés en sí en la empresa, las distintas visiones que tenés de un mismo dato desde los distintos ángulos que tenemos en la empresa, es decir, centros de costos, unidades funcionales, proyectos, paquetes de trabajo. Y una desafío también ahí con... Muchas veces tenés el mismo dato dos veces como tenés distintas fuentes de datos, limpiar y unificar todo eso es un desafío bastante grande, ¿por qué? Sí, sí, sí, sí, bastante. También uno de los desafíos más grandes que nos encontramos, por ejemplo, es la decisión de qué herramientas utilizar. Porque la verdad, por ejemplo, tenés, con esto que nombrabas recién de las ETLs, tenés muchas herramientas, tanto open source como pagas. Pagas. Y presenciadas. Y la verdad que es también un desafío que es muy importante. Porque es un desafío que es muy importante. Es también un desafío empezar a encontrar eso, porque muchas realizan lo mismo, pero te encontrás que hay algo o una partecita que no te la cumple. Entonces, bueno, tenés que volver a cambiar de herramienta y empezar a de cero nuevamente. Sí, va, ahí... No sé si puedo aportar demasiado, pero... Lo que puedo aportar es que siempre tienen que elegir o traten de elegir la herramienta con la que se sientan cómodos y la que puedan hacerla evolucionar. Muchas de esas herramientas de ETL siempre prometen el oro y el moro, pero siempre se quedan cortas, siempre va a haber algo que no va a estar disponible. Y si vos la puedes evolucionar o extender, por lo general siempre es con programación, desarrollando componentes y demás, es salvable, digamos. El problema es cuando esas herramientas no se pueden extender o no se pueden evolucionar agregándole plugins o cosas por el estilo y no tienen la funcionaria que tenés y ya está. Ahí no tenés nada para hacer, digamos. La mayoría igual trae herramientas para evolucionarla o para agregarle extensiones y demás como para salvar lo que necesites que no tienen. A ti quería hacer una pregunta o un comentario. Sí, en realidad, más que nada, era una consulta, no necesariamente una pregunta específica. La consulta era esta. Viste que cuando el nivel de infraestructura empezó a mutar del hecho de tener un concentrador, empezar a hablar de escalabilidad horizontal, empezan a surgir tecnologías como Docker, Kubernetes y demás. Eso implica que sí o sí hace falta un cambio en los paradigmas de programación. ¿Vos pensás que ese mismo modelo de cambio de paradigma se va a dar para el modelado de datos en el futuro por la incorporación de estas tecnologías? Porque uno ve en Amazon o DigitalOcean y demás que dan los RDBMS, entonces, por ejemplo, ponen un MySQL cluster gigante de fondo y vos, programador, seguís modelando el dato como siempre usando mal el SQL. Eso es. Igual, o sea, hay un problema siempre que cuando se diseña bases de datos o se diseñan datos no se hace nunca bien, básicamente. El diseño de bases de datos o el diseño de estructuras de los datos que van a armazenadas tienen muchas etapas. Lo que es un diseño conceptual, un diseño lógico, un diseño físico. Y cuando aparece la parte distribuida, aparece una cuarta etapa, que justamente es el diseño distribuido. Está bien? Eso en realidad ya existe y los conceptos que se usan en estos almacenes distribuidos y demás son los mismos que se usan para bases de datos distribuidos de hace 30 años, que son justamente la fragmentación horizontal, la fragmentación vertical, la replicación. Son básicamente los mismos conceptos, pero aplicados en herramientas diferentes o con modelos diferentes. Eso ya existe y está. Si alguno quiere ahondar, puede leer el libro de Osu. Osu tiene un libro de bases de datos distribuidas que es fabuloso y que lo ha ido autorizando en el camino. Ya incluye todo lo que es Big Data y demás. Al principio el libro era muy viejo y no incluía nada, obviamente, de Big Data. Los primeros de mitad de los 90, creo, es por ahí más o menos la primera edición. La última edición creo que es la cuarta, la sexta, y ya incluye temas de Big Data y demás. Son los mismos conceptos, siempre son los mismos. Lo que pasa es que cuesta igual, no digo que sea fácil. Cuesta y a veces una mala edición hace que toda la performance vaya al demonio. Y lo otro que te quería preguntar era, no sé cómo estar ahora, la verdad hace bastante, no he trabajado con esas cosas, pero me acuerdo, para mí un caso icónico fue el de Mongo, de que cuando uno empezaba a desarrollar con Mongo, Mongo tenía distintas etapas de escalabilidad. Y en todo lo que es Cluster, hay un problema existencial, que es para que un Cluster escale, necesitas que tenga un quórum y para que tenga un quórum necesitas una cantidad de nodos mínimos, que lo hace bastante inviable llevar una poca chica. Entonces con Mongo pasaba de que vos arrancabas a desarrollar y ese desarrollo no era compatible con el Replica Set, y el Replica Set no era compatible con los Sharp. Y veo que eso pasa en otras bases de datos distribuidas. ¿Pensás que va a surgir alguna solución a futuro o el corto o mediano plazo? O quizás ya la hay. Sí, sí, sí, sí, sí, sí, la hay. Sí, sí, la hay, la hay, la hay. Yo nombré esa, Citus, se puede tranquilamente. Citus es una extensión para Postgre que te la convierte en una base de datos distribuida y con los mismos conceptos de siempre, de particionamiento y de replicación y funciona. No hay ningún problema. Hay que ver cuánto tiene. Es bastante transparente. Es bastante transparente, pero no para el diseñador, sí para el que la usa. No, claro, pero por ejemplo, con MySQL Cluster, cuando apareció, allá a lo lejos y hace tiempo, era 99% compatible, pero no 100%. Claro. Entonces a veces solucionan problemas, la definición no era transparente. Es que sí, sí, sí, siempre tendríamos problemas. O sea, los estándares, o sea, si bien están y están buenos, nunca se cumplen al 100% y nunca están 100% garantizados. Con Mongo lo que pasa también, o sea, son herramientas que muchas veces el problema que pasa es que nos hemos comprado nosotros los problemas que han tenido organizaciones grandes. Todos estos desarrollos muchos fueron para Facebook, para Google y nosotros nos lo hemos comprado muchas veces como si fueran nuestros y no son nuestros, porque nosotros no podemos montar un cluster de mil nodos. O sea, eso nos queda. Nunca lo vamos a ver quizás en la vida. Vamos a tener un nodo o dos con suerte y rezarle a Dios que no se nos caiga. Eso pasa. Con Mongo he tenido una experiencia una vez que montaban un nodo solo, que no en un cloud, sino on-premise, on-premise. Y no garantiza la consistencia, pero si vos tenés, si vos tenés mil nodos, qué te importa la consistencia o qué te importa si se cae un nodo, se cae en 100, si tenés mil. Ahora, si te cae un nodo y se te corrompe la base de datos enseguida y perdiste todos los datos y no tiene herramientas de recuperación. O sea, por eso te preguntaba por qué eso pasa un montón cuando recién arrancas de, por ejemplo, no sé, poner el clas... Esto lo vi mucho con Elastic, poner el cluster con dos nodos. Y por definición, un cluster con dos nodos no es un cluster. Vas a tener escenario split brain ante cualquier fallo. Y eso no está bien. Y muchas veces también lo que había visto era eso, que la escalabilidad no era lineal. Yo no podía arrancar con un nodo, agregar dos, después tres, después cuatro, porque cambiaban las, digamos, no era compatible en ese crecimiento hasta que no estabas en un cluster real con por lo menos cuatro miembros o más. Que por eso te preguntaba cómo veías ese problema en el futuro, si hay algo que ya lo solucionara o no está de pos, la verdad no lo conocía. Así tú. No, hay, hay, hay y hay otros que no. Pero bueno, es eso. O sea, se trata de conocer bastante la herramienta. Lo que pasa es que ahora suponemos. O mal suponemos, me parece a mí, a mí me incluyo, mal suponemos que como estamos en un cloud o lo que sea, o en una cloud privada o lo que sea, tiramos las imágenes ahí, los pod, los dockers, las imágenes, lo que sea y corren y nunca va a pasar nada. Pero es una mentira, digamos, estamos mintiendo y engañándonos a nosotros mismos. Yo todavía quiero conseguir el sticker de la nube. Son los padres que no pude conseguir. Bueno, muchas gracias. Gracias, Juan. Bueno, si hay alguna pregunta más. O si no, le invito a compartir, a abrir el micrófono y le damos un aplauso agradeciéndole a Germán por la charla que estuvo muy buena. Vamos. Muchas gracias. Gracias. Muy buenas. Gracias. Muchas gracias. Gracias, Germán. Buen fin de. Buenas. Chau. Chau, gracias. Chau, nos vemos.